# Anywhere {branch} is used, the branch name will be substituted.
# These comments will also be removed.
defaults:
  numbered: False
  maxdepth: 6
root: index
subtrees:

  - caption: Inference tutorials
    entries: 
    - file: notebooks/inference/opea_deployment_and_evaluation.ipynb
      title: ChatQnA vLLM deployment and performance evaluation
    - file: notebooks/inference/t2v_comfyui_radeon.ipynb
      title: Text-to-video generation with ComfyUI
    - file: notebooks/inference/deepseek_janus_cpu_gpu.ipynb
      title: DeepSeek Janus Pro on CPU or GPU
    - file: notebooks/inference/vllm_v1_DSR1.ipynb
      title: DeepSeek-R1 with vLLM V1  
    - file: notebooks/inference/build_airbnb_agent_mcp.ipynb
      title: AI agent with MCPs using vLLM and PydanticAI
    - file: notebooks/inference/1_inference_ver3_HF_transformers.ipynb
      title: Hugging Face Transformers
    - file: notebooks/inference/2_inference_ver3_HF_TGI.ipynb
      title: Hugging Face TGI
    - file: notebooks/inference/3_inference_ver3_HF_vllm.ipynb
      title: Deploying with vLLM
    - file: notebooks/inference/rapbot_vllm.ipynb 
      title: From chatbot to rap bot with vLLM
    - file: notebooks/inference/rag_ollama_llamaindex.ipynb
      title: RAG with LlamaIndex and Ollama
    - file: notebooks/inference/ocr_vllm.ipynb
      title: OCR with vision-language models with vLLM
    - file: notebooks/inference/voice_pipeline_rag_ollama.ipynb
      title: Building AI pipelines for voice assistants
    - file: notebooks/inference/speculative_decoding_deep_dive.ipynb
      title: Speculative decoding with vLLM 
    - file: notebooks/inference/llama-stack-rocm.ipynb
      title: Llama Stack
    - file: notebooks/inference/deepseekr1_sglang.ipynb
      title: DeepSeek-R1 with SGLang
    - file: notebooks/inference/SGlang_PD_Disagg_On_AMD_GPU.ipynb
      title: PD Disaggregation with SGLang

  - caption: Fine-tuning tutorials
    entries:
    - file: notebooks/fine_tune/fine_tuning_lora_qwen2vl.ipynb
      title: VLM with PEFT
    - file: notebooks/fine_tune/LoRA_Llama-3.2.ipynb
      title: LLM with LoRA
    - file: notebooks/fine_tune/QLoRA_Llama-3.1.ipynb
      title: LLM with QLoRA
    - file: notebooks/fine_tune/torchtune_llama3.ipynb
      title: Llama-3.1 8B with torchtune
    - file: notebooks/fine_tune/llama_factory_llama3.ipynb
      title: Llama-3.1 8B with Llama Factory
    - file: notebooks/fine_tune/unsloth_Llama3_1_8B_GRPO.ipynb
      title: GRPO with Unsloth

  - caption: Pretraining tutorials
    entries:
    - file: notebooks/pretrain/torch_fsdp.ipynb
      title: OLMo model with PyTorch FSDP
    - file: notebooks/pretrain/setup_tutorial.ipynb
      title: Training configuration with Megatron-LM
    - file: notebooks/pretrain/train_llama_mock_data.ipynb
      title: LLM with Megatron-LM
    - file: notebooks/pretrain/torchtitan_llama3.ipynb
      title: Llama-3.1 8B with torchtitan
    - file: notebooks/pretrain/ddim_pretrain.ipynb
      title: Custom diffusion model with PyTorch

  - caption: GPU development and optimization tutorials
    entries: 
    - file: notebooks/gpu_dev_optimize/aiter_mla_decode_kernel.ipynb
      title: MLA decoding kernel of AITER library
    - file: notebooks/gpu_dev_optimize/triton_kernel_dev.ipynb
      title: Kernel development and optimization with Triton
    - file: notebooks/gpu_dev_optimize/llama4_profiling_vllm.ipynb
      title: Profiling Llama-4 inference with vLLM
    - file: notebooks/gpu_dev_optimize/fp8_quantization_quark_vllm.ipynb
      title: FP8 quantization with AMD Quark for vLLM


  - caption: About
    entries:  
    - file: changelog.rst
      title: Changelog
    - file: notebooks/licensing.md
      title: Licensing and support information
