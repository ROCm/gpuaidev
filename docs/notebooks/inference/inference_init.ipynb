{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bdc7a27",
   "metadata": {},
   "source": [
    "# LLM Inference with AMD Instinct ™ MI300X Accelerators   \n",
    "add introduction\n",
    "\n",
    "## Prerequisites\n",
    "### 1.Hardware Requirements\n",
    "-AMD ROCm GPUs (e.g., MI210, MI300X).\n",
    "-Ensure your system meets the System Requirements, including ROCm 6.0+ and Ubuntu 22.04.\n",
    "### 2.Docker\n",
    "-Install Docker with GPU support\n",
    "-Ensure your user has appropriate permission to access to GPU\n",
    "\n",
    "```bash\n",
    "docker run --rm --device=/dev/kfd --device=/dev/dri rocm/pytorch:rocm6.1_ubuntu22.04_py3.10_pytorch_2.1.2 rocm-smi\n",
    "```\n",
    "### 3.Hugging Face API Access\n",
    "-Obtain an API token from Hugging Face for downloading models.\n",
    "-Ensure you have a Hugging Face API token with the necessary permissions and approval to access Meta’s LLaMA checkpoints.\n",
    "\n",
    "## Prepare Inference Environment\n",
    "### 1.Pull the Docker Image\n",
    "```bash\n",
    "# Host machine\n",
    "docker run -it --rm --device=/dev/kfd --device=/dev/dri --group-add video --shm-size 1G --security-opt seccomp=unconfined --security-opt apparmor=unconfined -v $(pwd):/workspace --env HUGGINGFACE_HUB_CACHE=/workspace rocm/pytorch:latesti\n",
    "\n",
    "# Inside the container \n",
    "cd /workspace \n",
    "export HF_TOKEN=\"Your hugging face token to access gated models\" \n",
    "pip install accelerate transformers \n",
    "```\n",
    "\n",
    "### 2.Install and Launch Jupyter\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "```bash\n",
    "pip install --upgrade pip setuptools wheel\n",
    "pip install jupyter\n",
    "```\n",
    "Start the Jupyter server:\n",
    "```bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "### 3.Run a Sample LLM\n",
    "Create a hf_transformer.py file inside the docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b9baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_transformers.py\n",
    "import transformers\n",
    "import torch  \n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "pipeline = transformers.pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "    device_map=\"auto\", \n",
    ") \n",
    "\n",
    "messages = [ \n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot in the online shopping mall!\"}, \n",
    "    {\"role\": \"user\", \"content\": \"How can I get a refund of this product?\"}, \n",
    "] \n",
    "\n",
    "outputs = pipeline( \n",
    "    messages,\n",
    "    max_new_tokens=10, \n",
    ") \n",
    "\n",
    "print(outputs[0][\"generated_text\"][-1]) \n",
    "\n",
    "# python hf_transformers.py \n",
    "# gives \n",
    "{'role': 'assistant', 'content': \"I'd be happy to help you with the refund\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c7a67",
   "metadata": {},
   "source": [
    "Inference on Hugging Face TGI \n",
    "\n",
    "Text Generation Inference from Hugging Face provides high-performance library optimized for serving large language models. With features like model sharding, multi-GPU inference, and low-latency decoding, TGI takes full advantage of AMD MI300X’s high compute density and memory bandwidth.  \n",
    "\n",
    " \n",
    "\n",
    "Running a LLM model on a local server  \n",
    "\n",
    "Deploying an LLM on a local server with Hugging Face TGI ensures optimized performance through features like multi-GPU support, and efficient batching. \n",
    "\n",
    "```bash\n",
    " docker run -it --rm --device=/dev/kfd --device=/dev/dri --group-add video --shm-size 1G --security-opt seccomp=unconfined --security-opt apparmor=unconfined -v $(pwd):/workspace --env HUGGINGFACE_HUB_CACHE=/workspace --ipc=host --net host --entrypoint /bin/bash ghcr.io/huggingface/text-generation-inference:latest-rocm \n",
    "\n",
    " # Inside the container \n",
    "cd /workspace \n",
    "export HF_TOKEN=\"Your hugging face token to access gated models\"  \n",
    "\n",
    "# Launch LLM backend server \n",
    "nohup text-generation-launcher --model-id meta-llama/Meta-Llama-3.1-8B-Instruct --num-shard 1 --cuda-graphs 1 --max-batch-prefill-tokens 131072 --max-batch-total-tokens 139264 --dtype float16 --port 88 &\n",
    "\n",
    "# Check server status, when \"Connected\" is printed out then the LLM backend server is ready\n",
    "tail nohup.out \n",
    "# after a few seconds,  \n",
    "router/src/server.rs:2015: Invalid hostname, defaulting to 0.0.0.0 \n",
    "2025-01-08T03:25:41.801732Z  INFO text_generation_router::server: router/src/server.rs:2402: Connected   \n",
    " ```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
