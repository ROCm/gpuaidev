{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bdc7a27",
   "metadata": {},
   "source": [
    "# Inference on Hugging Face Transformers   \n",
    "Transformers library from Hugging Face provides framework for running inference on pre-trained models. By leveraging AMD's advanced architecture, the library can deliver faster inference times and better efficiency for tasks such as text generation, classification, and translation. Support for GPU acceleration ensures seamless integration with AMD MI300X, enabling high-performance AI applications. \n",
    "\n",
    "## Prerequisites\n",
    "### 1.Hardware Requirements\n",
    "-AMD ROCm GPUs (e.g., MI210, MI300X).\n",
    "-Ensure your system meets the System Requirements, including ROCm 6.0+ and Ubuntu 22.04.\n",
    "### 2.Docker\n",
    "-Install Docker with GPU support\n",
    "-Ensure your user has appropriate permission to access to GPU\n",
    "\n",
    "```bash\n",
    "docker run --rm --device=/dev/kfd --device=/dev/dri rocm/pytorch:rocm6.1_ubuntu22.04_py3.10_pytorch_2.1.2 rocm-smi\n",
    "```\n",
    "### 3.Hugging Face API Access\n",
    "-Obtain an API token from Hugging Face for downloading models.\n",
    "-Ensure you have a Hugging Face API token with the necessary permissions and approval to access Meta’s LLaMA checkpoints.\n",
    "\n",
    "## Prepare Inference Environment\n",
    "### 1.Pull the Docker Image\n",
    "```bash\n",
    "# Host machine\n",
    "docker run -it --rm --device=/dev/kfd --device=/dev/dri --group-add video --shm-size 1G --security-opt seccomp=unconfined --security-opt apparmor=unconfined -v $(pwd):/workspace --env HUGGINGFACE_HUB_CACHE=/workspace rocm/pytorch:latest\n",
    "\n",
    "# Inside the container \n",
    "cd /workspace \n",
    "export HF_TOKEN=\"Your hugging face token to access gated models\" \n",
    "pip install accelerate transformers \n",
    "```\n",
    "\n",
    "### 2.Install and Launch Jupyter\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "```bash\n",
    "pip install --upgrade pip setuptools wheel\n",
    "pip install jupyter\n",
    "```\n",
    "Start the Jupyter server:\n",
    "```bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "### 3.Run a Sample LLM\n",
    "Create a hf_transformer.py file inside the docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b9baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_transformers.py\n",
    "import transformers\n",
    "import torch  \n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "pipeline = transformers.pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "    device_map=\"auto\", \n",
    ") \n",
    "\n",
    "messages = [ \n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot in the online shopping mall!\"}, \n",
    "    {\"role\": \"user\", \"content\": \"How can I get a refund of this product?\"}, \n",
    "] \n",
    "\n",
    "outputs = pipeline( \n",
    "    messages,\n",
    "    max_new_tokens=10, \n",
    ") \n",
    "\n",
    "print(outputs[0][\"generated_text\"][-1]) \n",
    "\n",
    "# python hf_transformers.py \n",
    "# gives \n",
    "{'role': 'assistant', 'content': \"I'd be happy to help you with the refund\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c7a67",
   "metadata": {},
   "source": [
    "# Inference on Hugging Face TGI \n",
    "Text Generation Inference from Hugging Face provides high-performance library optimized for serving large language models. With features like model sharding, multi-GPU inference, and low-latency decoding, TGI takes full advantage of AMD MI300X’s high compute density and memory bandwidth.   \n",
    "\n",
    "## Prerequisites\n",
    "### 1.Hardware Requirements\n",
    "-AMD ROCm GPUs (e.g., MI210, MI300X).\n",
    "-Ensure your system meets the System Requirements, including ROCm 6.0+ and Ubuntu 22.04.\n",
    "### 2.Docker\n",
    "-Install Docker with GPU support\n",
    "-Ensure your user has appropriate permission to access to GPU\n",
    "\n",
    "```bash\n",
    "docker run --rm --device=/dev/kfd --device=/dev/dri rocm/pytorch:rocm6.1_ubuntu22.04_py3.10_pytorch_2.1.2 rocm-smi\n",
    "```\n",
    "### 3.Hugging Face API Access\n",
    "-Obtain an API token from Hugging Face for downloading models.\n",
    "-Ensure you have a Hugging Face API token with the necessary permissions and approval to access Meta’s LLaMA checkpoints.\n",
    "\n",
    "## Prepare Inference Environment\n",
    "### 1.Pull the Docker Image\n",
    "```bash\n",
    "#Host machine\n",
    "docker run -it --rm --device=/dev/kfd --device=/dev/dri --group-add video --shm-size 1G --security-opt seccomp=unconfined --security-opt apparmor=unconfined -v $(pwd):/workspace --env HUGGINGFACE_HUB_CACHE=/workspace --ipc=host --net host --entrypoint /bin/bash ghcr.io/huggingface/text-generation-inference:latest-rocm \n",
    "\n",
    "# Inside the container\n",
    "cd /workspace \n",
    "export HF_TOKEN=\"Your hugging face token to access gated models\" \n",
    "```\n",
    "\n",
    "### 2.Install and Launch Jupyter\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "```bash\n",
    "pip install --upgrade pip setuptools wheel\n",
    "pip install jupyter\n",
    "```\n",
    "Start the Jupyter server:\n",
    "```bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "### 3.Running LLM model on a local server\n",
    "```bash\n",
    "# Launch LLM backend server \n",
    "\n",
    "nohup text-generation-launcher --model-id meta-llama/Meta-Llama-3.1-8B-Instruct --num-shard 1 --cuda-graphs 1 --max-batch-prefill-tokens 131072 --max-batch-total-tokens 139264 --dtype float16 --port 88 & \n",
    "\n",
    "# Check server status, when \"Connected\" is printed out then the LLM backend server is ready \n",
    "tail nohup.out \n",
    "\n",
    "# after a few seconds,\n",
    "router/src/server.rs:2015: Invalid hostname, defaulting to 0.0.0.0 \n",
    "2025-01-08T03:25:41.801732Z  INFO text_generation_router::server: router/src/server.rs:2402: Connected \n",
    "```\n",
    "\n",
    "### 4.Acessing the LLM server from the client\n",
    "```bash\n",
    "curl localhost:88/v1/chat/completions \\\n",
    "    -X POST \\ \n",
    "    -d '{ \n",
    "  \"model\": \"tgi\", \n",
    "  \"messages\": [ \n",
    "    { \n",
    "      \"role\": \"system\", \n",
    "      \"content\": \"You are a chatbot in the online shopping mall!\" \n",
    "    }, \n",
    "    { \n",
    "      \"role\": \"user\", \n",
    "      \"content\": \"How can I get a refund of this product?\" \n",
    "    } \n",
    "  ], \n",
    "  \"stream\": false, \n",
    "  \"max_tokens\": 10 \n",
    "}' \\ \n",
    "   -H 'Content-Type: application/json' \n",
    "\n",
    "# gives \n",
    "\n",
    "{\"object\":\"chat.completion\",\"id\":\"\",\"created\":1736307160,\"model\":\"meta-llama/Llama-3.1-8B-Instruct\",\"system_fingerprint\":\"2.4.2-dev0-sha-9f5c9a5-rocm\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"You can initiate the refund process by logging into your\"},\"logprobs\":null,\"finish_reason\":\"length\"}],\"usage\":{\"prompt_tokens\":56,\"completion_tokens\":10,\"total_tokens\":66}} \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352b8b6",
   "metadata": {},
   "source": [
    "# Inference on Hugging Face vLLM\n",
    "vLLM provides specialized inference engine that prioritizes fast decoding for large language models, fully leveraging the power of AMD MI300X. It employs techniques like continuous batching and efficient memory management to maximize throughput. vLLM is well-suited for scenarios requiring real-time inference and high concurrency with large-scale LLMs.     \n",
    "\n",
    "## Prerequisites\n",
    "### 1.Hardware Requirements\n",
    "-AMD ROCm GPUs (e.g., MI210, MI300X).\n",
    "-Ensure your system meets the System Requirements, including ROCm 6.0+ and Ubuntu 22.04.\n",
    "### 2.Docker\n",
    "-Install Docker with GPU support\n",
    "-Ensure your user has appropriate permission to access to GPU\n",
    "\n",
    "```bash\n",
    "docker run --rm --device=/dev/kfd --device=/dev/dri rocm/pytorch:rocm6.1_ubuntu22.04_py3.10_pytorch_2.1.2 rocm-smi\n",
    "```\n",
    "### 3.Hugging Face API Access\n",
    "-Obtain an API token from Hugging Face for downloading models.\n",
    "-Ensure you have a Hugging Face API token with the necessary permissions and approval to access Meta’s LLaMA checkpoints.\n",
    "\n",
    "## Prepare Inference Environment\n",
    "### 1.Pull the Docker Image\n",
    "```bash\n",
    "# Host machine \n",
    "docker run -it --rm --device=/dev/kfd --device=/dev/dri --group-add video --shm-size 1G --security-opt seccomp=unconfined --security-opt apparmor=unconfined -v $(pwd):/workspace --env HUGGINGFACE_HUB_CACHE=/workspace --ipc=host --net host --entrypoint /bin/bash rocm/vllm:rocm6.2_mi300_ubuntu20.04_py3.9_vllm_0.6.4  \n",
    "\n",
    "# Inside the container \n",
    "cd /workspace \n",
    "export HF_TOKEN=\"Your hugging face token to access gated models\"  \n",
    "```\n",
    "\n",
    "### 2.Install and Launch Jupyter\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "```bash\n",
    "pip install --upgrade pip setuptools wheel\n",
    "pip install jupyter\n",
    "```\n",
    "Start the Jupyter server:\n",
    "```bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "### 3.Running LLM model on a local server\n",
    "```bash\n",
    "# Launch LLM backend server\n",
    "req=128 \n",
    "max_num_batched_tokens=131072 \n",
    "model=meta-llama/Meta-Llama-3.1-8B-Instruct \n",
    "nohup python -m vllm.entrypoints.openai.api_server \\ \n",
    "       --model $model \\ \n",
    "       --gpu-memory-utilization 0.9 \\ \n",
    "       --swap-space 16 \\ \n",
    "       --disable-log-requests \\ \n",
    "       --dtype float16 \\ \n",
    "       --max-model-len $max_num_batched_tokens \\ \n",
    "       --tensor-parallel-size 1 \\ \n",
    "       --host 0.0.0.0 \\ \n",
    "       --port 88 \\ \n",
    "       --num-scheduler-steps 10 \\ \n",
    "       --enable-chunked-prefill False \\ \n",
    "       --max-num-seqs $req \\ \n",
    "       --max-num-batched-tokens $max_num_batched_tokens \\ \n",
    "       --max-model-len $max_num_batched_tokens \\ \n",
    "       --distributed-executor-backend \"mp\" & \n",
    "\n",
    "# Check server status, when \"Connected\" is printed out then the LLM backend server is ready \n",
    "tail nohup.out \n",
    "# after a few seconds,  \n",
    "INFO:     Application startup complete. \n",
    "\n",
    "INFO:     Uvicorn running on socket ('0.0.0.0', 88) (Press CTRL+C to quit) \n",
    "```\n",
    "\n",
    "### 4.Acessing the LLM server from the client\n",
    "```bash\n",
    "# Accessing the LLM server from the client \n",
    "\n",
    "curl localhost:88/v1/chat/completions \\ \n",
    "    -X POST \\ \n",
    "    -d '{ \n",
    "  \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "  \"messages\": [ \n",
    "    { \n",
    "      \"role\": \"system\", \n",
    "      \"content\": \"You are a chatbot in the online shopping mall!\" \n",
    "    }, \n",
    "    { \n",
    "      \"role\": \"user\", \n",
    "      \"content\": \"How can I get a refund of this product?\" \n",
    "    } \n",
    "  ], \n",
    "  \"stream\": false, \n",
    "  \"max_tokens\": 10 \n",
    "}' \\ \n",
    "    -H 'Content-Type: application/json' \n",
    "\n",
    "# gives \n",
    "{\"id\":\"chat-31ea193149064b0cb0401311c4cff2a4\",\"object\":\"chat.completion\",\"created\":1736307940,\"model\":\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"I'd be happy to help you with the refund\",\"tool_calls\":[]},\"logprobs\":null,\"finish_reason\":\"length\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":56,\"total_tokens\":66,\"completion_tokens\":10},\"prompt_logprobs\":null}  \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
