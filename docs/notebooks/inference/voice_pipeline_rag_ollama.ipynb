{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building AI pipelines for voice assistants using ROCm, LlamaIndex, and RAG\n",
    "\n",
    "The following notebook demonstrates how to use AMD GPUs with [LlamaIndex](https://docs.llamaindex.ai/en/stable/) and Retrieval-Augmented Generation (RAG). It takes an input audio recording, transcribes it to text, sends the transcribed text to the RAG model, and generates a response in text format, which is then converted to speech and saved as an audio file.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup.\n",
    "\n",
    "### Operating system\n",
    "\n",
    "* **Ubuntu 22.04**: Ensure your system is running Ubuntu version 22.04.\n",
    "\n",
    "### Hardware\n",
    "\n",
    "* **AMD GPUs**: This tutorial was tested on an AMD Instinct™ MI300X and an AMD Radeon™ W7900. Ensure you are using an AMD GPU with ROCm support and that your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "### Software\n",
    "\n",
    "This tutorial was tested on both AMD Radeon and AMD Instinct GPUs using the following setup:\n",
    "- ROCm 6.2.0\n",
    "- Python 3.10\n",
    "- PyTorch 2.3.0\n",
    "\n",
    "### Objectives\n",
    "\n",
    "After completing this tutorial, you should understand the following concepts:\n",
    "\n",
    "* Multi-model pipeline\n",
    "* LlamaIndex with ROCm on AMD GPUs\n",
    "\n",
    "## Prepare the inference environment\n",
    "\n",
    "To set up the inference environment, follow these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a conda environment:\n",
    "   ```bash\n",
    "   conda create -n rocm python=3.10\n",
    "   ```\n",
    "\n",
    "1. Activate the environment:\n",
    "   ```bash\n",
    "   conda activate rocm\n",
    "   ```\n",
    "\n",
    "1. Install the PyTorch for ROCm software:\n",
    "   ```bash\n",
    "   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.2\n",
    "   ```\n",
    "\n",
    "1. Install Ollama (if not previously installed). This step requires `curl`:\n",
    "   ```bash\n",
    "   sudo apt install curl -y\n",
    "   curl -fsSL https://ollama.com/install.sh | sh\n",
    "   ```\n",
    "\n",
    "1. Launch the Ollama server if it isn't already running:\n",
    "   ```bash\n",
    "   ollama serve &\n",
    "   ```\n",
    "\n",
    "1. Pull `llama3` with Ollama:\n",
    "   ```bash\n",
    "   ollama pull llama3\n",
    "   ```\n",
    "\n",
    "1. Install the example dependencies:\n",
    "   ```bash\n",
    "   pip install llama-index llama-index-llms-ollama llama-index-embeddings-ollama llama-index-embeddings-huggingface openai-whisper transformers ChatTTS\n",
    "   ```\n",
    "\n",
    "1. Include an audio file (for example, `summarize_question.wav`). Place it in the current working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and launch Jupyter\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "```bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Then start the Jupyter server:\n",
    "\n",
    "```bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the packages\n",
    "\n",
    "Import the following packages:\n",
    "\n",
    "* **os**: Operating-system-dependent functionality.\n",
    "* **whisper**: A speech-recognition library.\n",
    "* **torch**: A PyTorch library for tensor computations and deep learning.\n",
    "* **llama_index.core**: Core functionality for the Llama Index.\n",
    "* **llama_index.embeddings.huggingface**: Support for embedding HuggingFace.\n",
    "* **llama_index.llms.ollama**: Functionality for the Ollama language model.\n",
    "* **ChatTTS**: A text-to-speech conversion library.\n",
    "* **torchaudio**: An audio-processing library.\n",
    "* **IPython.display**: For displaying audio in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Imports for Speech to Text\n",
    "import whisper\n",
    "import torch\n",
    "\n",
    "# Imports for RAG Model\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Imports for Text to Speech\n",
    "import ChatTTS\n",
    "import torchaudio\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "1. Optionally, set the environment variables to enable experimental features in PyTorch ROCm.\n",
    "2. Verify the PyTorch version and GPU availability.\n",
    "3. Select the computation device:\n",
    "   - Use the GPU if available and print its properties.\n",
    "   - Fall back to the CPU otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment variable for experimental features (optional)\n",
    "os.environ['TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL'] = '1'\n",
    "os.environ['HIP_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Check GPU availability and properties\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU (no GPU detected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe speech to text\n",
    "\n",
    "The following section performs speech-to-text transcription using the Whisper model.\n",
    "\n",
    "First, download the sample audio file for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L https://raw.githubusercontent.com/ROCm/gpuaidev/main/docs/notebooks/assets/summarize_question.wav -o summarize_question.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the audio file and transcribe the speech content into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_FILE = \"summarize_question.wav\"\n",
    "Audio(AUDIO_FILE, rate=24_000, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transcribe the speech content into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech-to-Text with Whisper\n",
    "try:\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(AUDIO_FILE)\n",
    "    input_text = result[\"text\"]\n",
    "    print(f\"Transcribed text: {input_text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in speech-to-text: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the RAG model\n",
    "\n",
    "To use a RAG model, provide the context that you'd like the LLM to use for the queries. This example is configured to use the documents in the `data` folder. If you don't have any documents yet, you can add your own or download the one below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "\n",
    "# Check if the data directory exists, and create it if it doesn't\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "    print(f\"Data directory '{DATA_DIR}' created. Please add a file of your choosing or use the cell below to download sample text.\")\n",
    "    exit(1)\n",
    "else:\n",
    "    # Check if data directory is empty\n",
    "    if not os.listdir(DATA_DIR):\n",
    "        print(f\"Data directory '{DATA_DIR}' is empty. Please add a file of your choosing or use the cell below to download sample text.\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the `data` directory is empty, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIONAL - Run this cell if your data directory is empty\n",
    "!mkdir -p data && curl -L https://www.gutenberg.org/cache/epub/11/pg11.txt -o data/pg11.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the data file now exists in the `data` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the files in your data directory\n",
    "print(\"Files in data directory:\", os.listdir(\"data\"))\n",
    "documents = SimpleDirectoryReader(DATA_DIR).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the embedding model, use \"bge-base\" from `HuggingFaceEmbedding`. Confirm that the Ollama server is running because it supplies Llama-3 for the LLM.  \n",
    "\n",
    "Next, create a `VectorStoreIndex` from the loaded documents and initialize a query engine with the index. Then issue your query using the text output from the Whisper model. Print the response so you can compare it against the audio output in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding and LLM models\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "try:\n",
    "    Settings.llm = Ollama(model=\"llama3\", request_timeout=360.0)\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Ollama server: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Build and query the vector index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine(streaming=True, response_mode=\"compact\", similarity_top_k=3)\n",
    "response = query_engine.query(input_text)\n",
    "\n",
    "# Function to convert StreamingResponse to string\n",
    "def streaming_response_to_string(streaming_response):\n",
    "    text = \"\"\n",
    "    for chunk in streaming_response.response_gen:\n",
    "        if isinstance(chunk, dict) and \"text\" in chunk:\n",
    "            text += chunk[\"text\"]\n",
    "        else:\n",
    "            text += str(chunk)\n",
    "    return text\n",
    "\n",
    "# Convert response to string\n",
    "response_text = streaming_response_to_string(response)\n",
    "print(f\"Generated response: {response_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform text-to-speech conversion\n",
    "\n",
    "The following example performs text-to-speech conversion using the ChatTTS library and saves the output audio to a file.\n",
    "\n",
    "This example uses the following constants:\n",
    "* `OUTPUT_AUDIO_FILE` (`str`): The name of the output audio file.\n",
    "* `SAMPLE_RATE` (`int`): The sample rate for the output audio file.\n",
    "\n",
    "It provides the following functionality:\n",
    "* Initializes a `ChatTTS.Chat` object.\n",
    "* Loads the chat model without compilation for faster loading. (Set `compile=True` for better performance.)\n",
    "* Converts the response text from the previous step to speech.\n",
    "* Saves the generated audio to the specified output file using torchaudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_AUDIO_FILE = \"voice_pipeline_response.wav\"\n",
    "SAMPLE_RATE = 24000\n",
    "\n",
    "# Text cleanup function for TTS\n",
    "def sanitize_input(text):\n",
    "    sanitized_text = text.replace('-', '')  # Remove hyphens\n",
    "    sanitized_text = sanitized_text.replace('(', '').replace(')', '')  # Remove parentheses\n",
    "    return sanitized_text.strip()\n",
    "\n",
    "# Text-to-Speech processing\n",
    "try:\n",
    "    sanitized_response = re.sub(r\"[^a-zA-Z0-9.,?! ]\", \"\", response_text)  # Remove special characters\n",
    "    print(f\"Sanitized response for TTS: {sanitized_response}\")\n",
    "    sanitized_response = [sanitized_response]\n",
    "\n",
    "    chat = ChatTTS.Chat()\n",
    "    chat.load(compile=False) # Set to True for better performance\n",
    "\n",
    "    params_infer_code = ChatTTS.Chat.InferCodeParams(\n",
    "        spk_emb = chat.sample_random_speaker(),\n",
    "    )\n",
    "\n",
    "    wavs = chat.infer(\n",
    "        sanitized_response,\n",
    "        params_infer_code=params_infer_code,\n",
    "    )\n",
    "    try:\n",
    "        torchaudio.save(OUTPUT_AUDIO_FILE, torch.from_numpy(wavs[0]).unsqueeze(0), SAMPLE_RATE)\n",
    "    except:\n",
    "        torchaudio.save(OUTPUT_AUDIO_FILE, torch.from_numpy(wavs[0]), SAMPLE_RATE)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in text-to-speech: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "finally:\n",
    "    if 'chat' in locals():\n",
    "        chat.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the following cell to hear the generated speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(wavs[0], rate=24_000, autoplay=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feb24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
