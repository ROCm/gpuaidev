{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building AI Pipelines for Voice Assistants Using ROCm, LlamaIndex, and RAG on AMD GPUs\n",
    "\n",
    "The following notebook demonstrates how to use AMD GPUs with LlamaIndex and Retrieval-Augmented Generation (RAG). It takes an input audio recording, transcribes it to text, uses the transcribed text as input to the RAG model, generates a response in text format, which is then converted to speech and saved as an audio file.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup.\n",
    "\n",
    "### Operating System\n",
    "\n",
    "* Ubuntu 22.04: Ensure your system is running Ubuntu version 22.04.\n",
    "\n",
    "### Hardware\n",
    "\n",
    "* AMD GPUs: This tutorial was tested on an AMD Instinct&trade; MI300X GPU and AMD Radeon&trade; W7900 GPU. Ensure you are using an AMD GPU compatible with ROCm support and that your system meets the official requirements.\n",
    "\n",
    "### Software\n",
    "\n",
    "This tutorial was tested on both AMD Radeon and AMD Instinct GPUs using the following setup:\n",
    "- ROCm 6.2.0\n",
    "- Python 3.10\n",
    "- Pytorch 2.3.0\n",
    "\n",
    "### Learnings\n",
    "\n",
    "By the end of this tutorial, you should feel comfortable with the following:\n",
    "\n",
    "* Multi-model pipeline\n",
    "* LlamaIndex with ROCm on AMD GPUs\n",
    "\n",
    "## Prepare the inference environment\n",
    "\n",
    "To set up the inference environment, use the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a conda environment:\n",
    "   ```bash\n",
    "   conda create -n rocm python=3.10\n",
    "   ```\n",
    "\n",
    "1. Activate the environment:\n",
    "   ```bash\n",
    "   conda activate rocm\n",
    "   ```\n",
    "\n",
    "1. Install ROCm Software:\n",
    "   ```bash\n",
    "   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.2\n",
    "   ```\n",
    "\n",
    "1. Install Ollama (if not previously installed, requires curl):\n",
    "   ```bash\n",
    "   sudo apt install curl -y\n",
    "   curl -fsSL https://ollama.com/install.sh | sh\n",
    "   ```\n",
    "\n",
    "1. Launch the Ollama Server (if not already running):\n",
    "   ```bash\n",
    "   ollama serve &\n",
    "   ```\n",
    "\n",
    "1. Pull `llama3` with Ollama:\n",
    "   ```bash\n",
    "   ollama pull llama3\n",
    "   ```\n",
    "\n",
    "1. Install example dependencies:\n",
    "   ```bash\n",
    "   pip install llama-index llama-index-llms-ollama llama-index-embeddings-ollama llama-index-embeddings-huggingface openai-whisper transformers ChatTTS\n",
    "   ```\n",
    "\n",
    "1. Include an audio file (e.g., `summarize_question.wav`). Place it in the current working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Install and launch Jupyter\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "```bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Then start the Jupyter server:\n",
    "\n",
    "```bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages\n",
    "\n",
    "* **os**: Provides a way of using operating system dependent functionality.\n",
    "* **whisper**: Library for speech recognition.\n",
    "* **torch**: PyTorch library for tensor computations and deep learning.\n",
    "* **llama_index.core**: Provides core functionalities for the Llama Index.\n",
    "* **llama_index.embeddings.huggingface**: Provides HuggingFace embedding functionalities.\n",
    "* **llama_index.llms.ollama**: Provides functionalities for the Ollama language model.\n",
    "* **ChatTTS**: Library for text to speech conversion.\n",
    "* **torchaudio**: Library for audio processing.\n",
    "* **IPython.display**: Provides functionalities for displaying audio in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Imports for Speech to Text\n",
    "import whisper\n",
    "import torch\n",
    "\n",
    "# Imports for RAG Model\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Imports for Text to Speech\n",
    "import ChatTTS\n",
    "import torchaudio\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup our environment\n",
    "\n",
    "1. Optionally set environment variables for enabling experimental features in PyTorch ROCm.\n",
    "2. Verify PyTorch version and GPU availability.\n",
    "3. Select the computation device:\n",
    "   - Use GPU if available and print its properties.\n",
    "   - Fall back to CPU otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment variable for experimental features (optional)\n",
    "os.environ['TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL'] = '1'\n",
    "os.environ['HIP_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Check GPU availability and properties\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU (no GPU detected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech to Text\n",
    "\n",
    "The following performs speech-to-text transcription using the Whisper model.\n",
    "\n",
    "It loads an audio file and transcribes the speech content into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_FILE = \"./summarize_question.wav\"\n",
    "Audio(AUDIO_FILE, rate=24_000, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech-to-Text with Whisper\n",
    "try:\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(AUDIO_FILE)\n",
    "    input_text = result[\"text\"]\n",
    "    print(f\"Transcribed text: {input_text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in speech-to-text: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Model\n",
    "\n",
    "For a RAG model, you'll want to provide the context that you'd like the LLM to use for your queries. This is example is setup such that we'll use all the documents in the data folder. If none exist, you can add your own or download the one below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "\n",
    "# Check if the data directory exists, and create it if it doesn't\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "    print(f\"Data directory '{DATA_DIR}' created. Please add a file of your choosing or use the cell below to download sample text.\")\n",
    "    exit(1)\n",
    "else:\n",
    "    # Check if data directory is empty\n",
    "    if not os.listdir(DATA_DIR):\n",
    "        print(f\"Data directory '{DATA_DIR}' is empty. Please add a file of your choosing or use the cell below to download sample text.\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIONAL - Run this cell if your data directory is empty\n",
    "!wget -P data https://www.gutenberg.org/cache/epub/11/pg11.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the files in your data directory\n",
    "print(\"Files in data directory:\", os.listdir(\"data\"))\n",
    "documents = SimpleDirectoryReader(DATA_DIR).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the embedding model, we're using 'bge-base' from HuggingFaceEmbedding. We also want to confirm that the Ollama server is running as it will provide llama3 for our LLM.  \n",
    "\n",
    "Then, we create a VectorStoreIndex from the loaded documents and initialize a query engine with the index. Then we issue our query using the text output from the Whisper model and print the response so we can compare with the audio output in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding and LLM models\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "try:\n",
    "    Settings.llm = Ollama(model=\"llama3\", request_timeout=360.0)\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Ollama server: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Build and query the vector index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine(streaming=True, response_mode=\"compact\", similarity_top_k=3)\n",
    "response = query_engine.query(input_text)\n",
    "\n",
    "# Function to convert StreamingResponse to string\n",
    "def streaming_response_to_string(streaming_response):\n",
    "    text = \"\"\n",
    "    for chunk in streaming_response.response_gen:\n",
    "        if isinstance(chunk, dict) and \"text\" in chunk:\n",
    "            text += chunk[\"text\"]\n",
    "        else:\n",
    "            text += str(chunk)\n",
    "    return text\n",
    "\n",
    "# Convert response to string\n",
    "response_text = streaming_response_to_string(response)\n",
    "print(f\"Generated response: {response_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to Speech\n",
    "\n",
    "The following performs text-to-speech conversion using the ChatTTS library and saves the output audio to a file.\n",
    "\n",
    "Constants:\n",
    "* OUTPUT_AUDIO_FILE (str): The name of the output audio file.\n",
    "* SAMPLE_RATE (int): The sample rate for the output audio file.\n",
    "\n",
    "Functionality:\n",
    "* Initializes a ChatTTS.Chat object.\n",
    "* Loads the chat model without compilation for faster loading (set compile=True for better performance).\n",
    "* Converts the response text from the previous step to speech.\n",
    "* Saves the generated audio to the specified output file using torchaudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_AUDIO_FILE = \"voice_pipeline_response.wav\"\n",
    "SAMPLE_RATE = 24000\n",
    "\n",
    "# Text cleanup function for TTS\n",
    "def sanitize_input(text):\n",
    "    sanitized_text = text.replace('-', '')  # Remove hyphens\n",
    "    sanitized_text = sanitized_text.replace('(', '').replace(')', '')  # Remove parentheses\n",
    "    return sanitized_text.strip()\n",
    "\n",
    "# Text-to-Speech processing\n",
    "try:\n",
    "    sanitized_response = re.sub(r\"[^a-zA-Z0-9.,?! ]\", \"\", response_text)  # Remove special characters\n",
    "    print(f\"Sanitized response for TTS: {sanitized_response}\")\n",
    "    sanitized_response = [sanitized_response]\n",
    "\n",
    "    chat = ChatTTS.Chat()\n",
    "    chat.load(compile=False) # Set to True for better performance\n",
    "\n",
    "    params_infer_code = ChatTTS.Chat.InferCodeParams(\n",
    "        spk_emb = chat.sample_random_speaker(),\n",
    "    )\n",
    "\n",
    "    wavs = chat.infer(\n",
    "        sanitized_response,\n",
    "        params_infer_code=params_infer_code,\n",
    "    )\n",
    "    try:\n",
    "        torchaudio.save(OUTPUT_AUDIO_FILE, torch.from_numpy(wavs[0]).unsqueeze(0), SAMPLE_RATE)\n",
    "    except:\n",
    "        torchaudio.save(OUTPUT_AUDIO_FILE, torch.from_numpy(wavs[0]), SAMPLE_RATE)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in text-to-speech: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "finally:\n",
    "    if 'chat' in locals():\n",
    "        chat.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the following cell to hear the generated speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(wavs[0], rate=24_000, autoplay=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feb24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
