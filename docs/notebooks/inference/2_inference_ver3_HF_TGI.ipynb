{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on Hugging Face TGI\n",
    "Hugging Face Text Generation Inference (TGI) is a high-performance, low-latency solution for serving advanced language models in production. It streamlines the process of text generation, enabling developers to deploy and scale language models for tasks like conversational AI, and content creation.\n",
    "\n",
    "In this tutorial, we’ll demonstrate how to configure and run TGI using AMD Radeon™ and Instinct™ GPUs, leveraging the ROCm software stack for accelerated performance. You’ll learn how to set up your environment, containerize your workflow, and test your inference server by sending customized queries. \n",
    "\n",
    "### Prepare Inference Environment\n",
    "#### 1. Launch the Docker Container\n",
    "Run the following command in your terminal to pull the prebuilt Docker image containing all necessary dependencies and launch the Docker container with proper configuration:\n",
    "```bash\n",
    "(shell)docker run -it --rm --device=/dev/kfd --device=/dev/dri --group-add video --shm-size 1G --security-opt seccomp=unconfined --security-opt apparmor=unconfined -v $(pwd):/workspace --env HUGGINGFACE_HUB_CACHE=/workspace --ipc=host --net host --entrypoint /bin/bash ghcr.io/huggingface/text-generation-inference:latest-rocm\n",
    "--> if docker is launched it will look like root@xxx:\n",
    "```\n",
    "```bash\n",
    "(docker)cd && cd /workspace\n",
    "```\n",
    "**Note:**Mounts the current host directory ($(pwd)) to **/workspace** in the container, allowing files to be shared between the host and the container.\n",
    "\n",
    "### 2. Install and Launch Jupyter\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "```bash\n",
    "\n",
    "(docker)pip install --upgrade pip setuptools wheel\n",
    "(docker)pip install jupyter\n",
    "(docker)jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root\n",
    "```\n",
    "**Note:**Save the token or URL provided in the terminal output to access the notebook from your host machine.\n",
    "\n",
    "### 3. Provide Your Hugging Face Token\n",
    "You will need a Hugging Face API token to access meta-llama/Llama-3.1-8B-Instruct. Tokens typically start with \"hf_\". Generate your token at Hugging Face Tokens and request access for [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct).\n",
    "\n",
    "Run the following interactive block in your Jupyter notebook to set up the token:\n",
    "**Note:** Please uncheck the \"Add token as Git credential?\" option.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "# Prompt the user to log in\n",
    "status = notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that your token was captured correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Validate the token\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying LLM using Hugging Face TGI \n",
    "Start deploying LLM(meta-llama/Llama-3.1-8B-Instruct) using Hugging Face TGI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!HIP_VISIBLE_DEVICES=4 \\\n",
    "text-generation-launcher \\\n",
    "    --model-id meta-llama/Llama-3.1-8B-Instruct \\\n",
    "    --num-shard 1 \\\n",
    "    --cuda-graphs 1 \\\n",
    "    --max-batch-prefill-tokens 131072 \\\n",
    "    --max-batch-total-tokens 139264 \\\n",
    "    --dtype float16 \\\n",
    "    --port 8000 \\\n",
    "    --trust-remote-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In a multi-GPU environment, it is recommended to set **HIP_VISIBLE_DEVICES=x** to deploy the LLM on the user’s preferred GPU.\n",
    "\n",
    "The LLM running in the Docker container acts as a server. To test it, open a new notebook and send a query to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:8000/generate\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"inputs\": \"System: You are an expert in the field of AI. Make sure to provide an explanation in few sentences.\\nUser: Explain the concept of AI.\\nAssistant:\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"do_sample\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Remember to match the docker --port **8000** and http://localhost:**8000**. If the port is used by other application you can modify the number. \n",
    "\n",
    "If the connection is successful the output will be:\n",
    "```bash\n",
    "{\"generated_text\":\" AI, or Artificial Intelligence, refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, problem-solving, ...}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
