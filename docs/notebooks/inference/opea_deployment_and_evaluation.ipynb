{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatQnA vLLM deployment and performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ChatQnA is a Retrieval-Augmented Generation (RAG) system that combines document retrieval with LLM inference. This tutorial provides a comprehensive guide for deploying ChatQnA using vLLM on AMD GPUs with ROCm support, as well as evaluating pipeline performance.\n",
    "\n",
    "## Key features\n",
    "\n",
    "Here are the benefits of using ChatQnA:\n",
    "\n",
    "- **vLLM integration**: LLM serving with optimized inference on AMD Instinct™ GPUs\n",
    "- **AMD GPU support**: ROCm-based GPU acceleration\n",
    "- **Vector search**: Redis-based document retrieval\n",
    "- **RAG pipeline**: Complete question-answering system\n",
    "- **Performance monitoring**: Built-in metrics and evaluation tools\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial includes the following sections:\n",
    "\n",
    "1. [Prerequisites](#prerequisites)\n",
    "2. [Prepare the environment](#prepare-the-environment)\n",
    "3. [System architecture](#system-architecture)\n",
    "4. [Deployment guide](#deployment-guide)\n",
    "5. [Performance evaluation](#performance-evaluation)\n",
    "6. [Common issues and solutions](#common-issues-and-solutions)\n",
    "7. [Advanced configuration](#advanced-configuration)\n",
    "8. [Troubleshooting](#troubleshooting)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup. \n",
    "\n",
    "### Operating system\n",
    "\n",
    "* **Ubuntu 22.04/24.04**: Ensure your system is running Ubuntu version 22.04 or 24.04.\n",
    "\n",
    "### Hardware\n",
    "\n",
    "* **AMD Instinct™ GPUs**: This tutorial was tested on an AMD Instinct MI300X GPU. Ensure you are using an AMD Instinct GPU or compatible hardware with ROCm support and that your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "### Software\n",
    "\n",
    "* **ROCm 6.3 or 6.4**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html). \n",
    "\n",
    "### Hugging Face API access\n",
    "\n",
    "* Obtain an API token from [Hugging Face](https://huggingface.co).\n",
    "\n",
    "## Prepare the environment\n",
    "\n",
    "This section creates a virtual environment and then starts the Jupyter server. \n",
    "\n",
    "### Set up a virtual environment\n",
    "\n",
    "Start by creating a virtual environment:\n",
    "\n",
    "``` bash\n",
    "python3 -m venv venv\n",
    "source venv/bin/activate \n",
    "```\n",
    "\n",
    "### Install and launch Jupyter\n",
    "\n",
    "Install Jupyter using the following command:\n",
    "\n",
    "```bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Then start the Jupyter server:\n",
    "\n",
    "```bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System architecture\n",
    "\n",
    "This section describes the ChatQnA architecture, including the services and data flow.\n",
    "\n",
    "### Service components\n",
    "\n",
    "The following diagram shows the complete ChatQnA system architecture.\n",
    "\n",
    "**Architecture Overview:**\n",
    "```\n",
    "┌───────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                               EXTERNAL ACCESS                                     │\n",
    "│                                                                                   │\n",
    "│   ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────────────────┐   │\n",
    "│   │   Web browser   │    │   API clients   │    │      Monitoring tools       │   │\n",
    "│   │                 │    │                 │    │    (Grafana, Prometheus)    │   │\n",
    "│   └─────────────────┘    └─────────────────┘    └─────────────────────────────┘   │\n",
    "│           │                       │                           │                   │\n",
    "│           │                       │                           │                   │\n",
    "│           ▼                       ▼                           ▼                   │\n",
    "│   ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────────────────┐   │\n",
    "│   │   Nginx Proxy   │    │   Backend API   │    │        Redis Insight        │   │\n",
    "│   │   (Port 8081)   │    │   (Port 8890)   │    │         (Port 8002)         │   │\n",
    "│   └─────────────────┘    └─────────────────┘    └─────────────────────────────┘   │\n",
    "│           │                       │                           │                   │\n",
    "│           │                       │                           │                   │\n",
    "│           ▼                       ▼                           ▼                   │\n",
    "│   ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────────────────┐   │\n",
    "│   │   Frontend UI   │    │     Backend     │    │   Redis Vector database     │   │\n",
    "│   │   (Port 5174)   │    │     server      │    │         (Port 6380)         │   │\n",
    "│   │   (React App)   │    │    (FastAPI)    │    │      (Vector storage)       │   │\n",
    "│   └─────────────────┘    └─────────────────┘    └─────────────────────────────┘   │\n",
    "│                                   │                           │                   │\n",
    "│                                   │                           │                   │\n",
    "│                                   ▼                           ▼                   │\n",
    "│  ┌─────────────────────────────────────────────────────────────────────────────┐  │\n",
    "│  │                             RAG PIPELINE                                    │  │\n",
    "│  │                                                                             │  │\n",
    "│  │  ┌───────────────────┐ ┌─────────────────────┐ ┌─────────────────────────┐  │  │\n",
    "│  │  │ Retriever service │ │TEI embedding service│ │  TEI reranking service  │  │  │\n",
    "│  │  │                   │ │                     │ │                         │  │  │\n",
    "│  │  │   (Port 7001)     │ │    (Port 18091)     │ │      (Port 18809)       │  │  │\n",
    "│  │  │                   │ │                     │ │                         │  │  │\n",
    "│  │  │ • Vector search   │ │ • Text embedding    │ │ • Document reranking    │  │  │\n",
    "│  │  │ • Similarity      │ │ • BGE model         │ │ • Relevance scoring     │  │  │\n",
    "│  │  │   matching        │ │ • CPU inference     │ │ • CPU inference         │  │  │\n",
    "│  │  └───────────────────┘ └─────────────────────┘ └─────────────────────────┘  │  │\n",
    "│  │            │                      │                         │               │  │\n",
    "│  │            │                      │                         │               │  │\n",
    "│  │            ▼                      ▼                         ▼               │  │\n",
    "│  │  ┌───────────────────────────────────────────────────────────────────────┐  │  │\n",
    "│  │  │                           vLLM service                                │  │  │\n",
    "│  │  │                           (Port 18009)                                │  │  │\n",
    "│  │  │                                                                       │  │  │\n",
    "│  │  │                  • High-performance LLM inference                     │  │  │\n",
    "│  │  │                  • AMD GPU acceleration (ROCm)                        │  │  │\n",
    "│  │  │                  • Qwen2.5-7B-Instruct model                          │  │  │\n",
    "│  │  │                  • Optimized for throughput and latency               │  │  │\n",
    "│  │  │                  • Tensor parallel support                            │  │  │\n",
    "│  │  └───────────────────────────────────────────────────────────────────────┘  │  │\n",
    "│  └─────────────────────────────────────────────────────────────────────────────┘  │\n",
    "│                                      │                                            │\n",
    "│                                      │                                            │\n",
    "│                                      ▼                                            │\n",
    "│  ┌─────────────────────────────────────────────────────────────────────────────┐  │\n",
    "│  │                            DATA PIPELINE                                    │  │\n",
    "│  │                                                                             │  │\n",
    "│  │  ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────────────┐  │  │\n",
    "│  │  │   Dataprep      │    │   Model cache   │    │   Document storage      │  │  │\n",
    "│  │  │   service       │    │   (./data)      │    │   (Redis Vector DB)     │  │  │\n",
    "│  │  │   (Port 18104)  │    │                 │    │                         │  │  │\n",
    "│  │  │                 │    │ • Downloaded    │    │ • Vector embeddings     │  │  │\n",
    "│  │  │ • Document      │    │   models        │    │ • Metadata index        │  │  │\n",
    "│  │  │   processing    │    │ • Model weights │    │ • Full-text search      │  │  │\n",
    "│  │  │ • Text          │    │ • Cache storage │    │ • Similarity search     │  │  │\n",
    "│  │  │   extraction    │    │ • Shared volume │    │ • Redis stack           │  │  │\n",
    "│  │  └─────────────────┘    └─────────────────┘    └─────────────────────────┘  │  │\n",
    "│  └─────────────────────────────────────────────────────────────────────────────┘  │\n",
    "└───────────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Additional services:**\n",
    "- **Dataprep service** (Port `18104`): Document processing and ingestion\n",
    "- **Redis Insight** (Port `8002`): Database monitoring interface\n",
    "- **Model cache** (`./data`): Shared volume for model storage\n",
    "\n",
    "### Data flow\n",
    "\n",
    "The pipeline for a new query follows these steps:\n",
    "\n",
    "1. **User input**: A question is submitted using the frontend.\n",
    "2. **Embedding**: The question is converted to a vector using the TEI service.\n",
    "3. **Retrieval**: Similar documents are retrieved from the Redis vector database.\n",
    "4. **Reranking**: The retrieved documents are reranked for relevance.\n",
    "5. **LLM inference**: vLLM generates an answer using the retrieved context.\n",
    "6. **Response**: The answer is returned to the user through the frontend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment guide\n",
    "\n",
    "To deploy ChatQnA, follow these steps:\n",
    "\n",
    "### Step 1: Pull the source code from GitHub\n",
    "\n",
    "First, clone the Open Platform for Enterprise AI (OPEA) GenAIExamples repository, which contains the ChatQnA implementation and other AI examples needed for your deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home directory\n",
    "import os\n",
    "HOME_DIR = os.getcwd()\n",
    "# Open Platform for Enterprise AI (OPEA)\n",
    "!git clone https://github.com/opea-project/GenAIExamples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, clone the LaunchPad repository that provides one-click deployment scripts and configuration files specifically designed for ChatQnA use cases on AMD GPU environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One click deployment scripts for the use case\n",
    "!git clone https://github.com/Yu-amd/LaunchPad.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finelly, clone the GenAIEval evaluation repository, which contains the benchmarking tools you'll use to evaluate the ChatQnA system performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline performance evaluation harness\n",
    "!git clone https://github.com/opea-project/GenAIEval.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LaunchPad project uses the same hierarchy as the OPEA project. Copy the LaunchPad scripts and YAML files from each directory to the corresponding directory in the OPEA folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy necessary scripts and configuration files to the OPEA directory\n",
    "!cp {HOME_DIR}/LaunchPad/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/*.sh {HOME_DIR}/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/\n",
    "!cp {HOME_DIR}/LaunchPad/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/*.yaml {HOME_DIR}/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/\n",
    "!cp {HOME_DIR}/LaunchPad/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/.env {HOME_DIR}/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/\n",
    "!cp -r {HOME_DIR}/LaunchPad/GenAIEval/evals/benchmarks/* {HOME_DIR}/GenAIEval/evals/benchmark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Environment setup\n",
    "\n",
    "Now navigate to the OPEA deployment directory where all the configuration files and scripts are located.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the OPEA deployment directory\n",
    "%cd {HOME_DIR}/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to configure environment variable management. First, install the `python-dotenv` package which lets you load environment variables from an `.env` file. Then import the necessary modules and load the environment variables from the file that contains your configuration settings.\n",
    "\n",
    "You also need to configure your Hugging Face API token, which is required to download the AI models used by the ChatQnA system. Replace `your_token_here` with the actual Hugging Face token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and load .env\n",
    "!pip install python-dotenv\n",
    "# Configure Hugging Face token\n",
    "!sed -i 's/CHATQNA_HUGGINGFACEHUB_API_TOKEN=\"\"/CHATQNA_HUGGINGFACEHUB_API_TOKEN=\"YOUR_ACTUAL_TOKEN_HERE\"/' .env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set all the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()  # Loads variables from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set up the vLLM environment using the provided script. This configures all the necessary components for high-performance LLM inference on AMD GPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup vLLM environment\n",
    "!./run_chatqna.sh setup-vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Deploy the workload\n",
    "\n",
    "With the environment configured, you can now start the vLLM services. This launches all the necessary containers and services for the ChatQnA system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start vLLM services\n",
    "import subprocess\n",
    "subprocess.run([\"./run_chatqna.sh\", \"start-vllm\"], check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the status of all the running services to ensure they started correctly and are functioning properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check service status\n",
    "!./run_chatqna.sh status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor the vLLM service logs for 60 seconds to verify the service starts correctly. Review the logs for any initialization issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check chatqna-vllm-service status\n",
    "!timeout 200 docker logs -f chatqna-vllm-service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Verify the deployment\n",
    "\n",
    "Verify that all Docker containers are running properly by checking their status and port mappings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check running containers\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next command sends a simple test message to the backend API to verify the ChatQnA service is working properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test backend API\n",
    "!curl -X POST http://localhost:8890/v1/chatqna \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"messages\": \"Hello, how are you?\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Upload documents\n",
    "\n",
    "Create a sample document and upload it to the system. This demonstrates how to feed documents into the ChatQnA system for retrieval and question answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text file\n",
    "!echo \"Your document content here\" > document.txt\n",
    "\n",
    "# Upload the file\n",
    "!curl -X POST http://localhost:18104/v1/dataprep/ingest \\\n",
    "  -H \"Content-Type: multipart/form-data\" \\\n",
    "  -F \"files=@document.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now verify that the document was successfully uploaded and indexed by checking the contents of the Redis vector database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the upload worked\n",
    "# Check if the document was indexed\n",
    "!curl -X POST http://localhost:18104/v1/dataprep/get \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"index_name\": \"rag-redis\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also upload multiple documents at once. Here's how to create and upload several documents simultaneously to build up your knowledge base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple documents\n",
    "# Create multiple files\n",
    "!echo \"Document 1 content\" > doc1.txt\n",
    "!echo \"Document 2 content\" > doc2.txt\n",
    "\n",
    "# Upload multiple files\n",
    "!curl -X POST http://localhost:18104/v1/dataprep/ingest \\\n",
    "  -H \"Content-Type: multipart/form-data\" \\\n",
    "  -F \"files=@doc1.txt\" \\\n",
    "  -F \"files=@doc2.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation\n",
    "\n",
    "Performance evaluation helps you understand the following metrics:\n",
    "\n",
    "- **Throughput**: Requests per second\n",
    "- **Latency**: Response time\n",
    "- **Accuracy**: Answer quality\n",
    "- **Resource usage**: CPU, GPU, and memory utilization\n",
    "\n",
    "### Step 1: Set up the evaluation environment\n",
    "\n",
    "Navigate to the GenAIEval directory to set up and run your performance evaluation tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to evaluation directory\n",
    "%cd {HOME_DIR}/GenAIEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the required dependencies for the evaluation tools and set up the GenAIEval package in development mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install evaluation dependencies\n",
    "!pip install -r requirements.txt\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run the basic evaluation\n",
    "\n",
    "Now navigate back to the ChatQnA deployment directory and run the performance evaluation tests on your deployed system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate back to GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/\n",
    "%cd {HOME_DIR}/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the vLLM evaluation script, which tests the performance of your ChatQnA system, measuring metrics like throughput, latency, and response quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run vLLM evaluation\n",
    "!./run_chatqna.sh vllm-eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Performance metrics\n",
    "\n",
    "This section performs additional throughput and latency testing.\n",
    "\n",
    "#### Throughput testing\n",
    "\n",
    "Install Apache Bench (ab), which performs load testing and measures the throughput of the ChatQnA API under various conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependency\n",
    "!apt install -y apache2-utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a test file with a complex question to evaluate how well the system handles detailed, multi-part queries and generates comprehensive responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complex test file\n",
    "!echo '{\"messages\": \"Can you provide a detailed explanation of how neural networks work, including the concepts of forward propagation, backpropagation, and gradient descent? Also explain how these concepts relate to deep learning and why they are important for modern AI systems.\"}' > test_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use Apache Bench to run a load test simulating 100 concurrent requests with 10 simultaneous connections. This measures the system's throughput and performance under stress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test concurrent requests\n",
    "!ab -n 100 -c 10 -p test_data.json -T application/json \\\n",
    "  http://localhost:8890/v1/chatqna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latency testing\n",
    "\n",
    "Create a detailed timing format file for `curl` to help measure various latency metrics including DNS lookup, connection time, and total response time for precise performance analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create curl-format.txt with the following content:\n",
    "curl_format_content = \"\"\"     time_namelookup:  %{time_namelookup}\n",
    "        time_connect:  %{time_connect}\n",
    "     time_appconnect:  %{time_appconnect}\n",
    "    time_pretransfer:  %{time_pretransfer}\n",
    "       time_redirect:  %{time_redirect}\n",
    "  time_starttransfer:  %{time_starttransfer}\n",
    "                     ----------\n",
    "          time_total:  %{time_total}\n",
    "          http_code:  %{http_code}\n",
    "       size_download:  %{size_download}\n",
    "      speed_download:  %{speed_download}\"\"\"\n",
    "\n",
    "with open('curl-format.txt', 'w') as f:\n",
    "    f.write(curl_format_content)\n",
    "\n",
    "print(\"curl-format.txt has been created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use `curl` with your detailed timing format to measure the precise response times for a single request. This provides granular insights into each step of the request processing pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure response times\n",
    "!curl -w \"@curl-format.txt\" -X POST http://localhost:8890/v1/chatqna \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"messages\": \"What is machine learning?\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluation of results\n",
    "\n",
    "The evaluation results include the following:\n",
    "\n",
    "- **Response time**: Average, median, and 95th percentile\n",
    "- **Throughput**: Requests per second\n",
    "- **Accuracy**: Answer quality metrics\n",
    "- **Resource usage**: CPU, GPU, and memory consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common issues and solutions\n",
    "\n",
    "The performance results you collected could potentially indicate certain performance issues with the ChatQnA system.\n",
    "\n",
    "### GPU memory errors\n",
    "\n",
    "**Symptoms**: Out-of-memory or similar errors.\n",
    "\n",
    "**Solution**: Reduce the batch size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce batch size in vLLM configuration\n",
    "# Edit compose_vllm.yaml, modify vLLM service command:\n",
    "--max-model-len 2048 --tensor-parallel-size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Service startup failures\n",
    "\n",
    "**Symptoms**: Services fail to start or remain in the `starting` state.\n",
    "\n",
    "**Solution**: Check the logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check logs for specific errors\n",
    "!docker compose -f compose_vllm.yaml logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart inactive services by passing `restart-vllm` to the `run_chatqna.sh` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart services\n",
    "!./run_chatqna.sh restart-vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redis index issues\n",
    "\n",
    "**Symptoms**: The retrieval service fails to find documents.\n",
    "\n",
    "**Solution**: Fix the Redis index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./fix_redis_index.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then recreate the Redis index manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!docker exec chatqna-redis-vector-db redis-cli FT.CREATE rag-redis ON HASH PREFIX 1 doc: SCHEMA content TEXT WEIGHT 1.0 distance NUMERIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model download failures\n",
    "\n",
    "**Symptoms**: Services fail to download models.\n",
    "\n",
    "**Solution**: Verify the Hugging Face token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check HF token\n",
    "!echo $CHATQNA_HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your Hugging Face token manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set token manually\n",
    "!export CHATQNA_HUGGINGFACEHUB_API_TOKEN=\"your_token_here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced configuration\n",
    "\n",
    "This section covers advanced scenarios, such as how to use different models.\n",
    "\n",
    "### Custom model configuration\n",
    "\n",
    "Edit the `set_env_vllm.sh` file to use different models.\n",
    "\n",
    "Run this command to change the LLM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CHATQNA_LLM_MODEL_ID=\"Qwen/Qwen2.5-14B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this command to change the embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CHATQNA_EMBEDDING_MODEL_ID=\"BAAI/bge-large-en-v1.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this command to change the reranking model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CHATQNA_RERANK_MODEL_ID=\"BAAI/bge-reranker-large\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "Use this section as a reference if the system isn't working as expected.\n",
    "\n",
    "### Diagnostic commands\n",
    "\n",
    "Run this command to check the system resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./detect_issues.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a quick test for the whole system using this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./quick_test_chatqna.sh eval-only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this command to check the service health:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f compose_vllm.yaml ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View all logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f compose_vllm.yaml logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review specific service logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f compose_vllm.yaml logs -f chatqna-vllm-service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f compose_vllm.yaml logs | grep -i error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU memory management\n",
    "\n",
    "Determine GPU memory usage and clear memory if required.\n",
    "\n",
    "#### Check GPU memory status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the current GPU memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current GPU memory usage\n",
    "# Expected output shows VRAM% and GPU% usage\n",
    "# If VRAM% is high (>80%) but GPU% is low, memory may be fragmented\n",
    "!rocm-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear GPU memory (if necessary)\n",
    "\n",
    "If you encounter GPU memory issues or high VRAM usage with low GPU utilization, try the commands in the following sections:\n",
    "\n",
    "**Option 1: Kill GPU processes**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find any processes that are using the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo fuser -v /dev/kfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kill the GPU-related processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!sudo pkill -f \"python|vllm|docker\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Restart GPU services**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart the `amdgpu` and related services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo systemctl restart amdgpu\n",
    "!sudo systemctl restart kfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 3: System reboot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the other methods don't work, reboot the system. This is the most reliable way of dealing with GPU memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: If you're on a remote server, wait approximately 30 seconds to 1 minute\n",
    "# before attempting to SSH back into the server\n",
    "!sudo reboot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clearing GPU memory, verify memory is available again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check GPU memory is now available\n",
    "# Expected: VRAM% should be low (<20%) and GPU% should be 0%\n",
    "!rocm-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial provides a comprehensive guide for deploying ChatQnA with vLLM on AMD GPUs and performing detailed performance evaluation. The ChatQnA system offers:\n",
    "\n",
    "- **High performance**: vLLM-optimized inference\n",
    "- **Scalability**: Docker-based microservices architecture\n",
    "- **Monitoring**: Built-in performance metrics\n",
    "- **Flexibility**: Configurable models and parameters\n",
    "\n",
    "For additional support or advanced configurations, see the [project documentation](https://github.com/opea-project/GenAIExamples/tree/main/ChatQnA) or create an issue in the repository.\n",
    "\n",
    "### Next steps\n",
    "\n",
    "1. **Customize models**: Experiment with different LLM and embedding models.\n",
    "2. **Scale deployment**: Add multiple GPU nodes for higher throughput.\n",
    "3. **Optimize performance**: Fine-tune vLLM parameters for your specific use case.\n",
    "4. **Monitor production**: Set up comprehensive monitoring for production deployments.\n",
    "\n",
    "**Note**: This tutorial assumes you have the necessary permissions and that all required software is installed. For production deployments, consider additional security measures and monitoring solutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
