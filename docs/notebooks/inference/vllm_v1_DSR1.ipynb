{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a53d8536",
   "metadata": {},
   "source": [
    "# Deploy DeepSeek-R1 with the vLLM V1 engine and build an AI-powered office assistant\n",
    "\n",
    "Imagine starting up DeepSeek-R1, a 60-billion-plus parameter reasoning machine, on an AMD Instinct™ MI300X GPU and watching it produce over 200 tokens per second. Now imagine doing this with a single command line and no driver issues.\n",
    "\n",
    "Starting with vLLM v0.9.2, the V1 engine gives you low latency and out-of-the-box throughput.\n",
    "\n",
    "In this hands-on deep dive, you will:\n",
    "\n",
    "- Compare V1 vs V0 performance with live benchmarks.\n",
    "- Deploy DeepSeek-R1 in production-ready mode with quantized safetensors.\n",
    "- Build a native Microsoft Word and Microsoft Excel AI sidekick using OpenAI-compatible endpoints.\n",
    "\n",
    "**V1 versus V0 challenge**\n",
    "\n",
    "You'll run a multi-round conversation system to conduct side-by-side live benchmarks and record real-time latency and throughputs. The results will confirm the V1 Engine’s scheduler and paged-attention kernels deliver higher queries-per-second (QPS) and a lower Time-to-First-Token (TTFT).\n",
    "\n",
    "**Deploy DeepSeek-R1 in production**\n",
    "\n",
    "From quantized safetensors to OpenAI-compatible `/v1/chat/completions` endpoints, DeepSeek-R1 features auto-scaling, batching, and KV-cache offloading tuned for an MI300X with 128 GB HBM3.\n",
    "\n",
    "**Build an AI-powered office assistant**\n",
    "\n",
    "Use the deployed DeepSeek-R1 backend to enable a native Microsoft Word and Excel copilot that sits inside your desktop apps, instantly condensing multi-page documents and turning hours of paperwork into minutes of conversation.\n",
    "\n",
    "It's time to launch the Instinct MI300X. Now that you know what’s coming up, launch the notebook and get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b9b0a0",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup. \n",
    "\n",
    "### Operating system\n",
    "\n",
    "* **Ubuntu 22.04/24.04**: Ensure your system is running Ubuntu 22.04 or 24.04.\n",
    "\n",
    "### Hardware\n",
    "\n",
    "* **AMD Instinct™ GPUs**: This tutorial was tested on a single node with eight AMD Instinct MI300X GPUs. Ensure you are using an AMD Instinct GPU or compatible hardware with ROCm support and that your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "### Software\n",
    "\n",
    "* **ROCm 6.3 or 6.4**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html). After installation, confirm your setup using:\n",
    "\n",
    "    ``` bash\n",
    "    amd-smi\n",
    "    ```\n",
    "\n",
    "    This command lists your AMD GPUs with relevant details.\n",
    "    \n",
    "    **Note**: For ROCm 6.4 and earlier, use the `rocm-smi` command instead.\n",
    "\n",
    "* **Docker**: Ensure Docker is installed and configured correctly. Follow the Docker installation guide for your operating system.\n",
    "\n",
    "   **Note**: Ensure the Docker permissions are correctly configured. To configure permissions to allow non-root access, run the following commands:\n",
    "\n",
    "   ``` bash\n",
    "   sudo usermod -aG docker $USER\n",
    "   newgrp docker\n",
    "   ```\n",
    "\n",
    "   Verify Docker is working correctly:\n",
    "\n",
    "   ``` bash\n",
    "   docker run hello-world\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c91c98",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "Follow these steps to set up the development environment.\n",
    "\n",
    "### Step 1: Pull the ROCm vLLM image\n",
    "\n",
    "Open a new terminal window on your server and run the following command:\n",
    "\n",
    "``` bash\n",
    "docker pull rocm/vllm-dev:nightly_0624_rc2_0624_rc2_20250620\n",
    "```\n",
    "\n",
    "### Step 2: Launch the Docker container\n",
    "\n",
    "Use this command to run the Docker container.\n",
    "\n",
    "``` bash\n",
    "docker run -it --rm \\\n",
    "  --cap-add=SYS_PTRACE \\\n",
    "  -e SHELL=/bin/bash \\\n",
    "  --network=host \\\n",
    "  --security-opt seccomp=unconfined \\\n",
    "  --device=/dev/kfd \\\n",
    "  --device=/dev/dri \\\n",
    "  -v $(pwd):/workspace \\\n",
    "  --group-add video \\\n",
    "  --ipc=host \\\n",
    "  --name vllm_V1_demo_01 \\\n",
    "  -w /workspace \\\n",
    "  rocm/vllm-dev:nightly_0624_rc2_0624_rc2_20250620\n",
    "```\n",
    "\n",
    "### Step 3: Install and launch Jupyter Notebooks\n",
    "\n",
    "The previous step launched an interactive Docker shell. Inside the Docker container, install JupyterLab using the following command:\n",
    "\n",
    "``` bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Start the Jupyter server:\n",
    "\n",
    "``` bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the command above. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`.\n",
    "\n",
    "\n",
    "The remainder of this tutorial is intended to be run inside a Jupyter notebook. Upload this file to the JupyterLab instance you just launched and follow the steps below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cc9d23",
   "metadata": {},
   "source": [
    "## Demo 1: Multi-turn question-answering with the DeepSeek-R1 V1 and V0 engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485cffa1",
   "metadata": {},
   "source": [
    "DeepSeek-R1 provides a powerful toolkit for assessing model performance. It focuses on multi-turn question answering, simulating realistic multi-user conversation scenarios, where new users continuously join while existing users exit. The `multi-round-qa.py` script launches numerous parallel user sessions, each engaging in multiple rounds of dialogue with the model. This setup provides a clear view of serving latency and overall throughput.\n",
    "\n",
    "### Understanding the vLLM performance testing toolkit\n",
    "\n",
    "The high-level flow of the script is illustrated below: \n",
    "\n",
    "![vllm qa](../assets/multi-round-qa-vllm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4451adbf",
   "metadata": {},
   "source": [
    "The script replays a realistic multi-turn QA pattern:\n",
    "\n",
    "- Each user holds a stateful chat session (with the conversation history carried forward).\n",
    "\n",
    "- Every turn, the user asks a follow-up question with a length that grows with the accumulated context.\n",
    "\n",
    "- The script stresses the engine on latency (TTFT, ITL) and throughput (effective QPS), while the KV cache is being reused, evicted, or off-loaded.\n",
    "\n",
    "Start testing it now by working through the following steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2d8617",
   "metadata": {},
   "source": [
    "### Step 1: Clean the cache space\n",
    "\n",
    "Start by cleaning the pip installation cache space to avoid potential compilation issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349c95ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip cache purge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da2b91b",
   "metadata": {},
   "source": [
    "### Step 2: Download the repository\n",
    "\n",
    "Run these commands to download the repository and install the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da0918",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /app \n",
    "git clone https://github.com/vllm-project/production-stack.git\n",
    "cd /app/production-stack/benchmarks/multi-round-qa\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d1f587",
   "metadata": {},
   "source": [
    "### Step 3: Prepare the Hugging Face models\n",
    "\n",
    "Follow these steps to install the Hugging Face CLI and download the models.\n",
    "\n",
    "First install the Hugging Face CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31445c71",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b070dfd",
   "metadata": {},
   "source": [
    "\n",
    "Then download the DeepSeek-R1 model by running the following cell:\n",
    "\n",
    "**Note**: This is a very large model, so the download process could take a few minutes. If the connection is interrupted, rerun this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01137ed3-3264-4090-86e2-3b87c9a6b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_DIR=/workspace/models/DeepSeek-R1   # single, consistent path\n",
    "mkdir -p \"$MODEL_DIR\"\n",
    "\n",
    "huggingface-cli download \\\n",
    "    deepseek-ai/DeepSeek-R1 \\\n",
    "    --local-dir \"$MODEL_DIR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb5486",
   "metadata": {},
   "source": [
    "### Step 4: Run the DeepSeek-V1 engine\n",
    "\n",
    "Start a new terminal to run the DeepSeek-V1 engine. To start a new terminal, click on the **+** symbol next to the active notebook tab. The following animation demonstrates how to open a new terminal in the Jupyter environment and customize your view.\n",
    "\n",
    "![Jupyter terminal](../assets/jupyter_terminal.gif)\n",
    "\n",
    "Copy and paste the following command to the terminal window:\n",
    "\n",
    "``` bash\n",
    "SAFETENSORS_FAST_GPU=1 \\\n",
    "VLLM_ROCM_USE_AITER=1 \\\n",
    "VLLM_USE_V1=1 \\\n",
    "vllm serve /workspace/models/DeepSeek-R1 \\\n",
    "  -tp 8 \\\n",
    "  --max-model-len 32768 \\\n",
    "  --block-size 1 \\\n",
    "  --max_seq_len_to_capture 32768 \\\n",
    "  --no-enable-prefix-caching \\\n",
    "  --max-num-batched-tokens 32768 \\\n",
    "  --gpu-memory-utilization 0.95 \\\n",
    "  --trust-remote-code \\\n",
    "  --port 8000\n",
    "```\n",
    "\n",
    "After successfully running the command above in your terminal, the output should look like the example below. After you confirm the server is ready, return to the Jupyter notebook and continue executing the cells.\n",
    "\n",
    "![vllm server ready](../assets/vllm_server_ready.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f3981",
   "metadata": {},
   "source": [
    "#### Running the multi-round benchmark\n",
    "\n",
    "Initiate the multi-round question-answering benchmark by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83753c7c-9ca7-4808-b28e-b41ae3ae2b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /app/production-stack/benchmarks/multi-round-qa/multi-round-qa.py \\\n",
    "    --num-users 10 \\\n",
    "    --num-rounds 5 \\\n",
    "    --qps 0.5 \\\n",
    "    --shared-system-prompt 1000 \\\n",
    "    --user-history-prompt 2000 \\\n",
    "    --answer-len 100 \\\n",
    "    --model /workspace/models/DeepSeek-R1 \\\n",
    "    --base-url http://localhost:8000/v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd6752-b8df-4538-89a2-685ce5a14d92",
   "metadata": {},
   "source": [
    "The script above should produce output similar to the image below.\n",
    "\n",
    "![qa out v1](../assets/v1_qa_output.png)\n",
    "\n",
    "**Note**: The live demo will continue running until you manually stop it, so you can observe it for as long as you like.  \n",
    "\n",
    "To start testing the V0 engine, click **Interrupt the kernel**, as shown in the image below, to stop the current process, then continue running the remaining cells.\n",
    "\n",
    "**Important**: ⚠️ The script must be manually stopped before moving to the next step.\n",
    "\n",
    "![jypyter kernel interrupt](../assets/jypter_kernel_stop.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da4f20a",
   "metadata": {},
   "source": [
    "### Step 5: Run the DeepSeek-V0 engine:  \n",
    "\n",
    "In the same terminal you used to run the `vllm serve` command in the previous section, press **Ctrl-C** twice to stop the V1 engine. Then run the V0 engine using the following command.\n",
    "\n",
    "Copy and paste the following command to the terminal window:\n",
    "\n",
    "``` bash\n",
    "SAFETENSORS_FAST_GPU=1 \\\n",
    "VLLM_ROCM_USE_AITER=1 \\\n",
    "VLLM_USE_V1=0 \\\n",
    "vllm serve /workspace/models/DeepSeek-R1 \\\n",
    "  -tp 8 \\\n",
    "  --max-model-len 32768 \\\n",
    "  --block-size 1 \\\n",
    "  --max-seq-len-to-capture 32768 \\\n",
    "  --no-enable-prefix-caching \\\n",
    "  --max-num-batched-tokens 32768 \\\n",
    "  --gpu-memory-utilization 0.95 \\\n",
    "  --trust-remote-code \\\n",
    "  --port 8000\n",
    "```\n",
    "\n",
    "After successfully running the command above in your terminal, the output should look like the example below. After you confirm the server is ready, return to the Jupyter notebook and continue executing the cells.\n",
    "\n",
    "![vllm server read](../assets/vllm_server_ready.png)\n",
    "\n",
    "#### Running the multi-round benchmark\n",
    "\n",
    "Initiate the multi-round question-answering benchmark by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87346bcb-5043-4049-85e9-5a3d0d7e59fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /app/production-stack/benchmarks/multi-round-qa/multi-round-qa.py \\\n",
    "    --num-users 10 \\\n",
    "    --num-rounds 5 \\\n",
    "    --qps 0.5 \\\n",
    "    --shared-system-prompt 1000 \\\n",
    "    --user-history-prompt 2000 \\\n",
    "    --answer-len 100 \\\n",
    "    --model /workspace/models/DeepSeek-R1 \\\n",
    "    --base-url http://localhost:8000/v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d69f6-b8ac-4fcd-b4f3-a105a04887bf",
   "metadata": {},
   "source": [
    "The script above should produce output similar to the image below.\n",
    "\n",
    "![v0 qa out](../assets/v0_qa_out.png)\n",
    "\n",
    "**Note**: The live demo will continue running until you manually stop it, so you can observe it for as long as you like.  \n",
    "\n",
    "To proceed to the second demo, click **Interrupt the kernel**, as shown in the image below, to stop the current process, then continue running the remaining cells.\n",
    "\n",
    "**Important**: ⚠️ The script must be manually stopped before moving to the next step.\n",
    "\n",
    "![jypyter kernel interrupt](../assets/jypter_kernel_stop.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f3200d",
   "metadata": {},
   "source": [
    "## Demo 2: AI-powered office assistant\n",
    "\n",
    "In this demo, you'll launch an AI-powered office assistant. Drag-and-drop `.docx` or `.txt` files, and the containerized tool instantly returns concise, high-quality summaries. No setup is required beyond `docker run`.\n",
    "\n",
    "**Important**: ⚠️ You must terminate both `vllm server` commands in the terminal and click the **Interrupt the kernel** button in the previous cell before proceeding to the next step.\n",
    "\n",
    "Follow these steps to run the AI office assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d857cd52-6019-4150-8334-ca615708b3f0",
   "metadata": {},
   "source": [
    "Serve the DeepSeek-R1 model in online serving mode. Start a new terminal and copy and paste the following command to the terminal window:\n",
    "\n",
    "``` bash\n",
    "SAFETENSORS_FAST_GPU=1 \\\n",
    "VLLM_ROCM_USE_AITER=1 \\\n",
    "VLLM_USE_V1=1 \\\n",
    "vllm serve /workspace/models/DeepSeek-R1 \\\n",
    "  -tp 8 \\\n",
    "  --max-model-len 32768 \\\n",
    "  --block-size 1 \\\n",
    "  --max-seq-len-to-capture 32768 \\\n",
    "  --no-enable-prefix-caching \\\n",
    "  --max-num-batched-tokens 32768 \\\n",
    "  --gpu-memory-utilization 0.95 \\\n",
    "  --trust-remote-code \\\n",
    "  --port 8000\n",
    "```\n",
    "\n",
    "After successfully running the command above, return to the Jupyter notebook and continue executing the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f27620-d064-4957-86fb-17251564491f",
   "metadata": {},
   "source": [
    "Install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea6418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install the required libraries.\n",
    "%pip install python-docx openai ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745c672d",
   "metadata": {},
   "source": [
    "Now import the libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdec04b-9af3-46f1-b023-50378dcca45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import io\n",
    "import os\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "import docx\n",
    "import ipywidgets as widgets\n",
    "from openai import OpenAI\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from io import BytesIO\n",
    "import docx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee63efc8",
   "metadata": {},
   "source": [
    "Define your local vLLM model endpoint in the OpenAI-compatible API format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8baf79e-cd2c-4865-a7e8-c4bdbda004f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the vllm configurations for the openai endpoint\n",
    "\n",
    "vllm_base = os.getenv(\"VLLM_ENDPOINT\", \"http://localhost:8000/v1\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=vllm_base,\n",
    "    api_key=\"dummy\",   # or api_key=\"\"  – any non-None string works\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93195400-4769-4887-adc8-69b9a56d65cb",
   "metadata": {},
   "source": [
    "\n",
    "Define a helper function for reading docs in the `.txt` and `.docx` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa05d825-dae4-4866-9fff-d9d085fe98bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import docx\n",
    "import textwrap\n",
    "\n",
    "def read_document(path: Path) -> str:\n",
    "    path = Path(path)\n",
    "    if path.suffix.lower() == \".docx\":\n",
    "        doc = docx.Document(path)\n",
    "        return \"\\n\".join(para.text for para in doc.paragraphs)\n",
    "    elif path.suffix.lower() == \".txt\":\n",
    "        return path.read_text(encoding=\"utf-8\")\n",
    "    else:\n",
    "        raise ValueError(\"Only .docx and .txt are supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f1c2f5",
   "metadata": {},
   "source": [
    "Set an appropiate system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361de447-de3d-41ac-8dcc-d73630800fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are a concise assistant. \"\n",
    "    \"Read the following text and provide a short summary \"\n",
    "    \"(3-5 sentences, 80-120 words).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521eff44",
   "metadata": {},
   "source": [
    "Define functions to summarize long texts by invoking the model in chunks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba10837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize_chunk(text: str) -> str:\n",
    "    \"\"\"Summarize one chunk with DeepSeek-R1 via vLLM.\"\"\"\n",
    "    prompt = f\"{SYSTEM_PROMPT}\\n\\n{text}\"\n",
    "    response = client.completions.create(\n",
    "        model=\"/workspace/models/DeepSeek-R1\",  # must match vllm model arg\n",
    "        prompt=prompt,\n",
    "        max_tokens=120,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        stop=[\"\\n\\n\"]\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "def summarize_long(text: str, max_chunk: int = 3000) -> str:\n",
    "    \"\"\"Chunk + summarize with vLLM.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = [\" \".join(words[i:i+max_chunk]) for i in range(0, len(words), max_chunk)]\n",
    "    summaries = [summarize_chunk(c) for c in chunks]\n",
    "    return \" \".join(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909cbc07-fb44-404f-987e-901ccfd6ca69",
   "metadata": {},
   "source": [
    "Use some sample text to verify the function is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4528fc40-4813-4e22-80e8-2a6dcf22a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"AMD CDNA architecture is the dedicated compute architecture underlying AMD Instinct GPUs and APUs. It features advanced packaging that unifies AMD chiplet technologies and High Bandwidth Memory (HBM), a high throughput Infinity Architecture fabric, and offers advanced Matrix Core Technology that supports a comprehensive set of AI and HPC data formats—designed to reduce data movement overhead and enhance power efficiency. AMD CDNA 4 offers enhanced Matrix Core Technologies that double the computational throughput for low precision Matrix data types compared to the previous Gen architecture. AMD CDNA 4 brings improved instruction-level parallelism, expands shared LDS resources with double the bandwidth, and includes support for a broad range of precisions that now include FP4 and FP6, along with sparse matrix data (i.e. sparsity) support. AMD demonstrated end-to-end, open-standards rack-scale AI infrastructure—already rolling out with AMD Instinct MI350 Series accelerators, 5th Gen AMD EPYC processors and AMD Pensando Pollara NICs in hyperscaler deployments such as Oracle Cloud Infrastructure (OCI) and set for broad availability in 2H 2025. AMD also previewed its next generation AI rack called Helios. It will be built on the next-generation AMD Instinct MI400 Series GPUs, which compared to the previous generation are expected to deliver up to 10x more performance running inference on Mixture of Experts models.\"\n",
    "print(\"Test summary:\", summarize_long(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71201827-ecd4-4f68-9a7d-f02d1e6ce85f",
   "metadata": {},
   "source": [
    "\n",
    "Run the cells below, then click the **Upload** button to upload your own documents.\n",
    "\n",
    "![Upload button](../assets/upload_text_button.png)\n",
    "\n",
    "**Note**: The only accepted formats are `.docx` or `.txt`, and the recommended text length is less than 500 words for quick response time. Files outside these limits or formats might raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edb0b3b-03f3-43e6-b42a-b775c30f184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create file upload widget\n",
    "uploader = widgets.FileUpload(\n",
    "    accept='.txt,.docx',  # Accepted file types\n",
    "    multiple=False  # Single file upload\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def handle_upload(change):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        \n",
    "        if not uploader.value:\n",
    "            print(\"No file uploaded\")\n",
    "            return\n",
    "            \n",
    "        # Universal approach for all ipywidgets versions\n",
    "        if hasattr(uploader.value, 'keys'):\n",
    "            # Dictionary format (newer versions)\n",
    "            filename = list(uploader.value.keys())[0]\n",
    "            file_data = uploader.value[filename]\n",
    "            content = file_data['content']\n",
    "        elif isinstance(uploader.value, tuple) and len(uploader.value) > 0:\n",
    "            # Tuple format (older versions)\n",
    "            filename = uploader.value[0]['name']\n",
    "            content = uploader.value[0]['content']\n",
    "        else:\n",
    "            print(\"Unsupported file format\")\n",
    "            return\n",
    "        \n",
    "        # Convert content to bytes if needed\n",
    "        if isinstance(content, memoryview):\n",
    "            content = content.tobytes()\n",
    "            \n",
    "        # Process based on file type\n",
    "        if filename.endswith('.txt'):\n",
    "            # Decode text file\n",
    "            try:\n",
    "                text = content.decode('utf-8')\n",
    "            except UnicodeDecodeError:\n",
    "                # Try alternative encodings if UTF-8 fails\n",
    "                try:\n",
    "                    text = content.decode('latin-1')\n",
    "                except:\n",
    "                    print(\"Failed to decode text file\")\n",
    "                    return\n",
    "            \n",
    "        elif filename.endswith('.docx'):\n",
    "            # Process DOCX file\n",
    "            try:\n",
    "                doc = docx.Document(BytesIO(content))\n",
    "                text = '\\n'.join([para.text for para in doc.paragraphs])\n",
    "            except:\n",
    "                print(\"Failed to process DOCX file\")\n",
    "                return\n",
    "            \n",
    "        else:\n",
    "            print(\"Unsupported file type\")\n",
    "            return\n",
    "        \n",
    "        # Clean and store text\n",
    "        global test\n",
    "        test = \" \".join(text.split())  # Normalize whitespace\n",
    "        \n",
    "        # Show success message and preview\n",
    "        print(f\"✅ File '{filename}' processed successfully!\")\n",
    "        print(\"\\nText preview (first 100 characters):\")\n",
    "        print(test[:100] + (\"...\" if len(test) > 100 else \"\"))\n",
    "        print(\"\\nFull text stored in variable 'test'\")\n",
    "\n",
    "# Setup interaction\n",
    "uploader.observe(handle_upload, names='value')\n",
    "\n",
    "# Display widgets\n",
    "print(\"Upload a .txt or .docx file:\")\n",
    "display(uploader)\n",
    "display(output_area)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2187ae00-8459-4b7b-a889-002827bb74eb",
   "metadata": {},
   "source": [
    "Upon successful execution, you’ll see the following confirmation:\n",
    "\n",
    "![success upload](../assets/file_upload_success.png)\n",
    "\n",
    "Continue running the following cells after the file is uploaded. Review the contents of the file you just uploaded to ensure they match your expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8373fef-eb17-4133-a868-da72caeb879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[:200])  # View first 200 characters\n",
    "len(test)          # Check text length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07af0c19-1685-4871-9f79-b8a32fac15be",
   "metadata": {},
   "source": [
    "Generate a summary of the provided text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544be702-e102-409e-a2b3-32d48a4dafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary:\", summarize_long(test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "076fd96e-26ee-4349-9365-af327c1d704c",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully built a powerful AI assistant!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
