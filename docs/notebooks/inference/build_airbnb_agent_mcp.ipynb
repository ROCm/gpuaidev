{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI agent with MCPs using vLLM and PydanticAI\n",
        "\n",
        "This tutorial leverages AMD GPUs and **Model Context Protocol (MCP)**, an open standard for exposing LLM tools using an API, to deploy powerful language models like Qwen3. The key components for this tutorial are:\n",
        "\n",
        "- üñ•Ô∏è **vLLM** for GPU-optimized inference\n",
        "- üõ†Ô∏è **PydanticAI** for agent and tool management\n",
        "- üîå **MCP Servers** for prebuilt tool integration\n",
        "\n",
        "You'll learn how to set up your environment, deploy large language models like Qwen3, connect them to real-world tools using MCP, and build a conversational agent capable of reasoning and taking actions.\n",
        "\n",
        "By the end of this workshop, you‚Äôll have built an AI-powered Airbnb assistant agent that can find a place to stay based on preferences like location, budget, and travel dates.\n",
        "\n",
        "This tutorial includes the following sections:\n",
        "\n",
        "- [1. Launching the vLLM server](#step1)\n",
        "- [2. Installing dependencies](#step2)\n",
        "- [3. Create a simple instance of a PydanticAI agent](#step3)\n",
        "- [4. Write a date/time tool for your agent](#step4)\n",
        "- [5. Replace the date/time tool with an MCP server](#step5)\n",
        "- [6. Turn your agent into an Airbnb finder](#step6)\n",
        "- [7. Challenge to expand the agent](#step7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "This tutorial was developed and tested using the following setup. \n",
        "\n",
        "### Operating system\n",
        "\n",
        "* **Ubuntu 22.04/24.04**: Ensure your system is running Ubuntu version 22.04 or 24.04.\n",
        "\n",
        "### Hardware\n",
        "\n",
        "* **AMD Instinct‚Ñ¢ GPUs**: This tutorial was tested on an AMD Instinct MI300X GPU. Ensure you are using an AMD Instinct GPU or compatible hardware with ROCm support and that your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
        "\n",
        "### Software\n",
        "\n",
        "* **ROCm 6.3 or 6.4**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html). After installation, confirm your setup using:\n",
        "\n",
        "    ``` bash\n",
        "    amd-smi\n",
        "    ```\n",
        "\n",
        "    This command lists your AMD GPUs with relevant details.\n",
        "    \n",
        "    **Note**: For ROCm 6.4 and earlier, use the `rocm-smi` command instead.\n",
        "\n",
        "* **Docker**: Ensure Docker is installed and configured correctly. Follow the Docker installation guide for your operating system.\n",
        "\n",
        "   **Note**: Ensure the Docker permissions are correctly configured. To configure permissions to allow non-root access, run the following commands:\n",
        "\n",
        "   ``` bash\n",
        "   sudo usermod -aG docker $USER\n",
        "   newgrp docker\n",
        "   ```\n",
        "\n",
        "   Verify Docker is working correctly:\n",
        "\n",
        "   ``` bash\n",
        "   docker run hello-world\n",
        "   ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the training environment\n",
        "\n",
        "Follow these steps to configure your tutorial environment:\n",
        "\n",
        "### 1. Pull the Docker image\n",
        "\n",
        "Ensure your system meets the [system requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
        "\n",
        "Pull the Docker image required for this tutorial:\n",
        "\n",
        "``` bash\n",
        "docker pull rocm/vllm:latest\n",
        "```\n",
        "\n",
        "### 2. Launch the Docker container\n",
        "\n",
        "Launch the Docker container and map the necessary directories. \n",
        "\n",
        "``` bash\n",
        "docker run -it --rm \\\n",
        "  --network=host \\\n",
        "  --device=/dev/kfd \\\n",
        "  --device=/dev/dri \\\n",
        "  --group-add=video \\\n",
        "  --ipc=host \\\n",
        "  --cap-add=SYS_PTRACE \\\n",
        "  --security-opt seccomp=unconfined \\\n",
        "  --shm-size 8G \\\n",
        "  -v $(pwd):/workspace \\\n",
        "  -w /workspace/notebooks \\\n",
        "  rocm/vllm:latest\n",
        "```\n",
        "\n",
        "**Note**: This command mounts the current directory to the `/workspace` directory in the container. Ensure the notebook file is either copied to this directory before running the Docker command or uploaded into the Jupyter Notebook environment after it starts. Save the token or URL provided in the terminal output to access the notebook from your web browser. You can download this notebook from the [AI Developer Hub GitHub repository](https://github.com/ROCm/gpuaidev).\n",
        "\n",
        "### 3. Install and launch Jupyter\n",
        "\n",
        "Inside the Docker container, install Jupyter using the following command:\n",
        "\n",
        "``` bash\n",
        "pip install jupyter\n",
        "```\n",
        "\n",
        "Start the Jupyter server:\n",
        "\n",
        "``` bash\n",
        "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
        "```\n",
        "\n",
        "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"step1\"></a>\n",
        "\n",
        "## Step 1: Launching the vLLM server\n",
        "\n",
        "In this workshop, you'll use [vLLM](https://github.com/vllm-project/vllm) as your inference serving engine. vLLM provides many benefits, such as fast model execution, an extensive list of supported models, and ease of use. Best of all, it's open source. \n",
        "\n",
        "### Deploy the Qwen3-30B-A3B model with vLLM\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "It's time to start your vLLM server and create an endpoint for your LLM. First open a terminal using your Jupyter server. Then run the following command in this terminal to start the vLLM server:\n",
        "\n",
        "```bash\n",
        "VLLM_USE_TRITON_FLASH_ATTN=0 \\\n",
        "vllm serve Qwen/Qwen3-30B-A3B \\\n",
        "    --served-model-name Qwen3-30B-A3B \\\n",
        "    --api-key abc-123 \\\n",
        "    --port 8000 \\\n",
        "    --enable-auto-tool-choice \\\n",
        "    --tool-call-parser hermes \\\n",
        "    --trust-remote-code\n",
        "```\n",
        "\n",
        "Open another terminal and monitor the GPU utilization by running this command.\n",
        "\n",
        "**Note**: For ROCm 6.4 and earlier, use the `rocm-smi` command instead.\n",
        "\n",
        "```bash\n",
        "watch amd-smi\n",
        "```\n",
        "\n",
        "After a successful launch, your server should be accepting incoming traffic through an OpenAI-compatible API. Now set some environment variables for your server to use throughout this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "BASE_URL = f\"http://localhost:8000/v1\"\n",
        "\n",
        "os.environ[\"BASE_URL\"]    = BASE_URL\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"abc-123\"   \n",
        "\n",
        "print(\"Config set:\", BASE_URL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify the model is available at the `BASE_URL` you just set by running the following command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!curl http://localhost:8000/v1/models -H \"Authorization: Bearer $OPENAI_API_KEY\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Congratulations, you just launched a powerful server that can serve any incoming request, allowing you to build amazing applications.\n",
        "\n",
        "<a id=\"step2\"></a>\n",
        "\n",
        "## Step 2: Installing dependencies\n",
        "\n",
        "Install the PydanticAI dependencies using this command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!pip install -q pydantic_ai openai     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<a id=\"step3\"></a>\n",
        "\n",
        "## Step 3: Create a simple instance of a PydanticAI agent\n",
        "\n",
        "Start by creating a custom OpenAI-compatible endpoint for your agent. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from pydantic_ai.models.openai import OpenAIModel\n",
        "from pydantic_ai.providers.openai import OpenAIProvider\n",
        "\n",
        "provider = OpenAIProvider(\n",
        "    base_url=os.environ[\"BASE_URL\"],\n",
        "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        ")\n",
        "\n",
        "agent_model = OpenAIModel(\"Qwen3-30B-A3B\", provider=provider)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now create an instance of the `Agent` class from `pydantic_ai`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from pydantic_ai import Agent\n",
        "\n",
        "agent = Agent(\n",
        "    model=agent_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's time to test the agent. The `pydantic_ai` framework provides multiple ways to run an `Agent` instance. To learn more, see the [PydanticAI site](https://ai.pydantic.dev/agents/#running-agents).\n",
        "\n",
        "In this workshop, you are running the agent in `async` mode. Define a helper function that allows you to quickly test your agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from pydantic_ai.mcp import MCPServerStdio\n",
        "async def run_async(prompt: str) -> str:\n",
        "    async with agent.run_mcp_servers():\n",
        "        result = await agent.run(prompt)\n",
        "        return result.output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the agent by calling this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "await run_async(\"What is the capital of France?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that you have the basics of an agent instance, connect it to the model you are serving with vLLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"step4\"></a>\n",
        "\n",
        "## Step 4: Write a date/time tool for your agent\n",
        "\n",
        "LLMs rely on their training data to respond to your prompts, so the agent you just defined would fail to answer a factual question that falls outside of its training knowledge. You can show this with an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "await run_async(\"What‚Äôs the date today?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's no surprise that the model failed to answer this question. Now it's time to power-up your LLM by providing the agent with a function that can get the current date. The process by which an LLM triggers a function call is commonly referred to as \"Tool Calling\" or \"Function Calling\". In this workshop, you're going to take advantage of the `pydantic_ai` agent `Tool` package to provide the agent with appropriate tools. First, define a custom tool within this framework using the code sample below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from pydantic_ai import Tool          \n",
        "@Tool\n",
        "def get_current_date() -> str:\n",
        "    \"\"\"Return the current date/time as an ISO-formatted string.\"\"\"\n",
        "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, provide this tool to your agent by adding the function signature of the tool to the `Agent` constructor. This notifies the LLM that the new tool exists. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "agent = Agent(\n",
        "    model=agent_model,\n",
        "    tools=[get_current_date],\n",
        "    system_prompt = (\n",
        "        \"You have access to:\\n\"\n",
        "        \"   1. get_current_time(params: dict)\\n\"\n",
        "        \"Use this tool for date/time questions.\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now test the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "await run_async(\"What‚Äôs the date today?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Congratulations on building an agent with access to real-time data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "<a id=\"step5\"></a>\n",
        "\n",
        "## Step 5: Replace the date/time tool with an MCP server\n",
        "\n",
        "Now that you learned how to create a custom tool and let the agent access it, you can enhance this step using the [Model Context Protocol](https://modelcontextprotocol.io/introduction) (MCP). You can replace the custom tool with a simple MCP server that serves the agent and provides similar information.\n",
        "\n",
        "Why should you use MCP? MCP servers provide:\n",
        "- ‚úÖ Standardized API interfaces\n",
        "- üîÑ Reusability across projects\n",
        "- üì¶ Prebuilt functionality\n",
        "\n",
        "To replace your custom time tool with an official MCP time server, follow these steps:\n",
        "\n",
        "### Installing an MCP time server\n",
        "\n",
        "Start by installing the MCP server:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!pip install -q mcp-server-time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now define the `time_server`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from pydantic_ai.mcp import MCPServerStdio\n",
        "\n",
        "time_server = MCPServerStdio(\n",
        "    \"python\",\n",
        "    args=[\n",
        "        \"-m\", \"mcp_server_time\",\n",
        "        \"--local-timezone=America/New_York\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, modify your agent by removing the previously defined tool and adding the MCP server instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "agent = Agent(\n",
        "    model=agent_model,\n",
        "    toolsets=[time_server],\n",
        "    system_prompt = (\n",
        "        \"You are a helpful agent and you have access to this tool:\\n\"\n",
        "        \"   get_current_time(params: dict)\\n\"\n",
        "        \"When the user asks for the current date or time, call get_current_time.\\n\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "You can now see whether the agent can use MCP to provide the correct time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "await run_async(\"What‚Äôs the date today?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "You have now used an official MCP server to power up your agent. In the next section, you'll learn how to turn your ideas into real working projects by using the hundreds of available free or paid MCP servers.\n",
        "\n",
        "\n",
        "<a id=\"step6\"></a>\n",
        "\n",
        "## Step 6: Turn your agent into an Airbnb finder\n",
        "\n",
        "As you discovered in the last section, MCP servers are very easy to use. They provide a standard way of providing LLMs with the tools they need. There are already thousands of MCP servers available for you to use. Consult one of the following MCP trackers as a reference to find out about the available servers:\n",
        "- [https://github.com/modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers)\n",
        "- [https://mcp.so/](https://mcp.so/)\n",
        "\n",
        "You will use npx to launch your next server. Use the following commands to install the required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Install Node.js 20 via NodeSource\n",
        "!curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -\n",
        "!apt install -y nodejs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify the `npm` and `npx` installations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!node -v && npm -v && npx --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "In this part of the workshop, you're going to build an agent to help browse available Airbnbs listings to book. You can build on top of what you've done already and add an open-source Airbnb MCP server to your agent. Start by defining the Airbnb server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "airbnb_server = MCPServerStdio(\n",
        "    \"npx\", args=[\"-y\", \"@openbnb/mcp-server-airbnb\", \"--ignore-robots-txt\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now update your agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"\n",
        "You have access to three tools:\n",
        "1. get_current_time(params: dict)\n",
        "2. airbnb_search(params: dict)\n",
        "3. airbnb_listing_details(params: dict)\n",
        "When the user asks for listings, first call get_current_time, then airbnb_search, etc.\n",
        "\"\"\"\n",
        "\n",
        "agent = Agent(\n",
        "    model=agent_model,\n",
        "    toolsets=[time_server, airbnb_server],\n",
        "    system_prompt=system_prompt,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, see if your agent can browse through the Airbnb listings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "await run_async(\"Find a place to stay in Vancouver for next Sunday for 3 nights for 2 adults?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "<a id=\"step7\"></a>\n",
        "\n",
        "## Step 7: Challenge to expand the agent\n",
        "\n",
        "For an additional challenge, add weather integration using the MCP weather server of your choice:\n",
        "1. Launch the MCP weather server\n",
        "2. Add it to the list of agent tools\n",
        "3. Ask the agent to suggest the best travel dates based on the weather"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
