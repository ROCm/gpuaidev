{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on Hugging Face Transformers\n",
    "Hugging Face Transformers is a popular open-source library that provides an easy-to-use interface for working with state-of-the-art language models, such as BERT, GPT, and Llama variants. These models can be fine-tuned or used off-the-shelf for tasks like text generation, question answering, and sentiment analysis.   \n",
    "\n",
    "In this tutorial, we’ll demonstrate how to run inference on Hugging Face Transformers models using AMD Instinct™ GPUs. We will cover configuring ROCm for GPU support, installing the necessary libraries, and running a LLM(meta-llama/Meta-Llama-3.1-8B-Instruct) in a containerized environment. \n",
    "\n",
    "### Prepare Inference Environment\n",
    "#### 1. Launch the Docker Container\n",
    "Run the following command in your terminal to pull the prebuilt Docker image containing all necessary dependencies and launch the Docker container with proper configuration:\n",
    "```bash\n",
    "(shell)docker run -it --rm -p 8888:8888 --device=/dev/kfd --device=/dev/dri --group-add video --shm-size 1G --security-opt seccomp=unconfined --security-opt apparmor=unconfined -v $(pwd):/workspace --env HUGGINGFACE_HUB_CACHE=/workspace rocm/pytorch:latest\n",
    "--> if docker is launched it will look like root@xxx:\n",
    "```\n",
    "```bash\n",
    "(docker)cd && cd /workspace\n",
    "```\n",
    "**Note:**Mounts the current host directory ($(pwd)) to **/workspace** in the container, allowing files to be shared between the host and the container.\n",
    "\n",
    "### 2. Install and Launch Jupyter\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "```bash\n",
    "\n",
    "(docker)pip install --upgrade pip setuptools wheel\n",
    "(docker)pip install jupyter\n",
    "(docker)jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root\n",
    "```\n",
    "**Note:**Save the token or URL provided in the terminal output to access the notebook from your host machine.\n",
    "\n",
    "### 3. Install Required Libraries\n",
    "Install the libraries needed for this tutorial. Run the following commands inside the Jupyter notebook running within the Docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install accelerate transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip list | grep transformer\n",
    "!pip list | grep accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Provide Your Hugging Face Token\n",
    "You will need a Hugging Face API token to access meta-llama/Llama-3.1-8B-Instruct. Tokens typically start with \"hf_\". Generate your token at Hugging Face Tokens and request access for [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct).\n",
    "\n",
    "Run the following interactive block in your Jupyter notebook to set up the token:\n",
    "**Note:** Please uncheck the \"Add token as Git credential?\" option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "# Prompt the user to log in\n",
    "status = notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that your token was captured correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Validate the token\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LLM Inference using Hugging Face Transformers\n",
    "Inside the docker container, run the following codes using jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "query = \"Explain the concept of AI.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert in the field of AI. Make sure to provide an explanation in few sentences.\"},\n",
    "    {\"role\": \"user\", \"content\": query},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=512,\n",
    "    top_p = 0.7,     \n",
    "    temperature=0.2,               \n",
    ")\n",
    "\n",
    "response = outputs[0][\"generated_text\"][-1]['content']\n",
    "print('-------------------------------')\n",
    "print('Query:\\n', query)\n",
    "print('-------------------------------')\n",
    "print('Response:\\n', response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successful execution, output will look like:\n",
    "```bash\n",
    "Query:\n",
    " Explain the concept of AI.\n",
    "-------------------------------\n",
    "Response:\n",
    " Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, problem-solving, decision-making, and perception. These systems use algorithms and data to simulate human-like behavior, enabling them to adapt to new situations and improve their performance over time. AI can be categorized into two main types: Narrow or Weak AI, which is designed to perform a specific task, and General or Strong AI, which aims to replicate human intelligence and reasoning across a wide range of tasks.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
