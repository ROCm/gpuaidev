{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running inference with Hugging Face Transformers\n",
    "\n",
    "This tutorial explores how to leverage Hugging Face Transformers on AMD hardware. Learn how to install and configure ROCm for AMD Instinct™ GPUs and launch your favorite models. By following these steps, you’ll be able to run advanced LLMs in a ROCm-accelerated environment, capitalizing on AMD’s GPU performance for innovative natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup. \n",
    "\n",
    "### Operating system\n",
    "\n",
    "* **Ubuntu 22.04**: Ensure your system is running Ubuntu version 22.04.\n",
    "\n",
    "### Hardware\n",
    "\n",
    "* **AMD Instinct GPUs**: This tutorial was tested on an AMD Instinct MI300X GPU. Ensure you are using an AMD Instinct GPU or compatible hardware with ROCm support and that your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "### Software\n",
    "\n",
    "* **ROCm 6.2 or 6.3**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html). After installation, confirm your setup using:\n",
    "\n",
    "    ``` bash\n",
    "    amd-smi\n",
    "    ```\n",
    "\n",
    "    This command lists your AMD GPUs with relevant details.\n",
    "    \n",
    "    **Note**: For ROCm 6.4 and earlier, use the `rocm-smi` command instead.\n",
    "\n",
    "* **Docker**: Ensure Docker is installed and configured correctly. Follow the Docker installation guide for your operating system.\n",
    "\n",
    "   **Note**: Ensure the Docker permissions are correctly configured. To configure permissions to allow non-root access, run the following commands:\n",
    "\n",
    "   ``` bash\n",
    "   sudo usermod -aG docker $USER\n",
    "   newgrp docker\n",
    "   ```\n",
    "\n",
    "   Verify Docker is working correctly:\n",
    "\n",
    "   ``` bash\n",
    "   docker run hello-world\n",
    "   ```\n",
    "\n",
    "### Hugging Face API access\n",
    "\n",
    "* Obtain an API token from [Hugging Face](https://huggingface.co) for downloading models.\n",
    "* Ensure the Hugging Face API token has the necessary permissions and approval to access [Meta's Llama checkpoints](https://huggingface.co/meta-llama/Llama-3.1-8B)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Inference on Hugging Face Transformers\n",
    "\n",
    "Hugging Face Transformers is a popular open-source library that provides an easy-to-use interface for working with widely used language models, such as BERT, GPT, and the Llama variants. These models can be fine-tuned or used off-the-shelf for tasks like text generation, question answering, and sentiment analysis.   \n",
    "\n",
    "This tutorial demonstrates how to run inference on Hugging Face Transformers models using AMD Instinct GPUs. It covers configuring ROCm for GPU support, installing the necessary libraries, and running an LLM (meta-llama/Meta-Llama-3.1-8B-Instruct) in a containerized environment. \n",
    "\n",
    "## Prepare the inference environment\n",
    "\n",
    "To set up the inference environment, follow these steps.\n",
    "\n",
    "### 1. Launch the Docker container\n",
    "Run the following command in your terminal to pull the prebuilt Docker image containing all necessary dependencies and launch the Docker container with the proper configuration:\n",
    "\n",
    "``` bash\n",
    "\n",
    "docker run -it --rm \\\n",
    "  --network=host \\\n",
    "  --device=/dev/kfd \\\n",
    "  --device=/dev/dri \\\n",
    "  --group-add=video \\\n",
    "  --ipc=host \\\n",
    "  --cap-add=SYS_PTRACE \\\n",
    "  --security-opt seccomp=unconfined \\\n",
    "  --shm-size 8G \\\n",
    "  --hostname=ROCm-FT \\\n",
    "  --env HUGGINGFACE_HUB_CACHE=/workspace \\\n",
    "  -v $(pwd):/workspace \\\n",
    "  -w /workspace/notebooks \\\n",
    "  rocm/pytorch:rocm6.3.1_ubuntu22.04_py3.10_pytorch\n",
    "```\n",
    "\n",
    "**Note**: This command mounts the current directory to the `/workspace` directory in the container. Ensure the notebook file is either copied to this directory before running the Docker command or uploaded into the Jupyter Notebook environment after it starts. Save the token or URL provided in the terminal output to access the notebook from your web browser. You can download this notebook from the [AI Developer Hub GitHub repository](https://github.com/ROCm/gpuaidev).\n",
    "\n",
    "### 2. Install and launch Jupyter\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "``` bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Then start the Jupyter server:\n",
    "\n",
    "``` bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`.\n",
    "\n",
    "### 3. Install the required libraries\n",
    "\n",
    "Install the libraries needed for this tutorial. Run the following commands inside the Jupyter notebook running within the Docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install accelerate transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip list | grep transformer\n",
    "!pip list | grep accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Provide your Hugging Face token\n",
    "\n",
    "You'll require a Hugging Face API token to access meta-llama/Llama-3.1-8B-Instruct. Generate your token at Hugging Face Tokens and request access for [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct). Tokens typically start with \"hf_\".\n",
    "\n",
    "Run the following interactive block in your Jupyter notebook to set up the token:\n",
    "\n",
    "**Note**: Uncheck the \"Add token as Git credential?\" option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "# Prompt the user to log in\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that your token was accepted correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Validate the token\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LLM inference using Hugging Face Transformers\n",
    "\n",
    "Inside the Docker container, run the following code sample using Jupyter Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "query = \"Explain the concept of AI.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert in the field of AI. Make sure to provide an explanation in few sentences.\"},\n",
    "    {\"role\": \"user\", \"content\": query},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=512,\n",
    "    top_p = 0.7,     \n",
    "    temperature=0.2,               \n",
    ")\n",
    "\n",
    "response = outputs[0][\"generated_text\"][-1]['content']\n",
    "print('-------------------------------')\n",
    "print('Query:\\n', query)\n",
    "print('-------------------------------')\n",
    "print('Response:\\n', response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a successful run, the output will look like this:\n",
    "\n",
    "\n",
    "Query:\n",
    "```\n",
    " Explain the concept of AI.\n",
    "```\n",
    "Response:\n",
    "```\n",
    " Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, problem-solving, decision-making, and perception. These systems use algorithms and data to simulate human-like behavior, enabling them to adapt to new situations and improve their performance over time. AI can be categorized into two main types: Narrow or Weak AI, which is designed to perform a specific task, and General or Strong AI, which aims to replicate human intelligence and reasoning across a wide range of tasks.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
