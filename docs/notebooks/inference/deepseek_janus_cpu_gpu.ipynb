{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Janus Pro DeepSeek model on AMD hardware\n",
    "\n",
    "This tutorial demonstrates how to perform multimodal inference with the Janus Pro autoregressive framework, which is part of the [Janus-Series](https://github.com/deepseek-ai/Janus#-janus-series-unified-multimodal-understanding-and-generation-models) from DeepSeek AI. You'll run the model on high-performance AMD hardware, including EPYC™ CPUs and Instinct™ GPUs.\n",
    "\n",
    "The term multimodal means that the model can understand and process information from multiple sources simultaneously, such as text and images. By unifying these different data types, known as modalities, Janus enables sophisticated understanding and generation tasks.\n",
    "\n",
    "The tutorial also explains how to leverage the [AMD ZenDNN](https://www.amd.com/en/developer/zendnn.html) plugin (also known as zentorch) for PyTorch when executing the model on a CPU to accelerate inferencing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Use the following setup to run this tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardware\n",
    "\n",
    "For this tutorial, you'll need a system with an AMD Instinct GPU. To run the model on the CPU and use AMD ZenDNN, you need an AMD EPYC CPU. \n",
    "\n",
    "This tutorial was tested on the following hardware:\n",
    "* AMD Instinct MI100\n",
    "* AMD Instinct MI210\n",
    "* AMD Instinct MI300X\n",
    "* 4th generation AMD EPYC (Genoa)\n",
    "* 5th generation AMD EPYC (Turin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software\n",
    "\n",
    "* **Ubuntu 22.04**: Ensure your system is running Ubuntu 22.04 or later.\n",
    "* **ROCm 6.3**: This is only required for GPU execution. Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html).\n",
    "* **PyTorch 2.6** (or later)\n",
    "* **zentorch 5.0.2** (or later)\n",
    "* Clone the official [Janus-Pro DeepSeek repository](https://github.com/deepseek-ai/Janus).\n",
    "  \n",
    "**Note**: This tutorial was tested with `torch2.7.1+rocm6.3`, `torch2.6.0+cpu`, and `zentorch-5.0.2`.\n",
    "\n",
    "### Install and launch Jupyter Notebooks\n",
    "If Jupyter is not already installed on your system, install it and launch JupyterLab using the following commands:\n",
    "\n",
    "```\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "To start the Jupyter server, run the following command:\n",
    "\n",
    "```\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`.\n",
    "\n",
    "After the command executes, the terminal output displays a URL and token. Copy and paste this URL into your web browser on the host machine to access JupyterLab. After launching JupyterLab, upload this notebook to the environment and continue to follow the steps in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the environment and install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands install all dependencies required to successfully run this tutorial, along with the `janus` module from the DeepSeek AI GitHub repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision --index-url https://download.pytorch.org/whl/rocm6.3\n",
    "!pip install transformers ipywidgets\n",
    "!pip install git+https://github.com/deepseek-ai/Janus.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run a quick check for the PyTorch environment. Validate that the GPU hardware is accessible using the ROCm backend. If not, then execute the model on the CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "print(\"--- GPU Verification ---\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ PyTorch has access to the GPU.\")\n",
    "    print(f\"ROCm Version (detected by PyTorch): {torch.version.hip}\")\n",
    "    print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU installed on the system: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"❌ PyTorch CANNOT access the GPU. Please check your ROCm installation and drivers or proceed to continue with executing on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the required Python libraries required for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the following variables for the upcoming inference process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 1\n",
    "warmup = 0\n",
    "max_new_tokens = 512\n",
    "dtype = \"bfloat16\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline summary\n",
    "\n",
    "- Choose the hardware backend: CPU or GPU\n",
    "- Load the Janus-Pro model and processor.\n",
    "- Define the image and question for multimodal understanding.\n",
    "- Preprocess the text and image inputs.\n",
    "- Generate image embeddings for the model.\n",
    "- Leverage zentorch.\n",
    "- Run warmup iterations to stabilize performance.\n",
    "- Perform timed inference to measure latency.\n",
    "- Compute and display the average generation time.\n",
    "- Decode and display the AI-generated response.\n",
    "\n",
    "## Choose the hardware backend: CPU or GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates two possible hardware backends where you can execute your AI workload: the CPU or GPU. To deploy your workload on a GPU, set `device = \"cuda\"`. Otherwise, to deploy on the CPU, set `device = \"cpu\"`. The CPU supports multiple software backends, for example, `zentorch`, the Intel® Extension for PyTorch (`ipex`), and the default PyTorch CPU backend (also known as `inductor`). You can also run the tutorial in native mode (for instance, eager-mode as opposed to graph-mode).\n",
    "\n",
    "**Note**: zentorch plugin version 5.0.2 requires the CPU-only version of PyTorch 2.6. This is a PyTorch limitation. As a workaround, remove any previous PyTorch installations and install the CPU version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" # Change to \"cpu\" to execute on CPU\n",
    "backend = \"zentorch\" # Available CPU backends: zentorch, inductor, ipex, or native [Eager Mode]\n",
    "\n",
    "if device == \"cpu\" and backend == \"zentorch\":\n",
    "    !pip uninstall -y torch torchvision torchaudio pytorch-triton-rocm\n",
    "    !pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cpu\n",
    "    !pip install zentorch #--no-cache-dir\n",
    "    import torch \n",
    "    import zentorch\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"Zentorch Version: {zentorch.__version__}\")\n",
    "\n",
    "amp_enabled = True if dtype != \"float32\" else False\n",
    "amp_dtype = getattr(torch, dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Model initialization and setup\n",
    "\n",
    "Begin by specifying the model path and initializing the necessary components for processing images and text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify the path to the model  \n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"  \n",
    "\n",
    "# Load the multimodal chat processor and tokenizer  \n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)  \n",
    "tokenizer = vl_chat_processor.tokenizer  \n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(  \n",
    "    model_path, trust_remote_code=True  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the model to use `BFloat16` precision and move it to the CPU for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl_gpt = vl_gpt.to(amp_dtype).to(device).eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the image and user input\n",
    "\n",
    "Define an image and a text-based query to analyze its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"../assets/deepseek_janus_demo_small.jpg\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code snippet below to check your image. This also confirms that the image path is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now prepare the input payload for the vision-language model. This is achieved by constructing a `conversation` list that adheres to the model's required chat template. The user message is a dictionary containing the role, the textual question embedded with an `<image_placeholder>` token, and the corresponding image object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is happening in this image?\"  \n",
    "\n",
    "conversation = [  \n",
    "    {  \n",
    "        \"role\": \"<|User|>\",  \n",
    "        \"content\": f\"<image_placeholder>\\n{question}\",  \n",
    "        \"images\": [image],  \n",
    "    },   \n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Preprocess the image and text inputs\n",
    "\n",
    "Load the image and convert the conversation data into a suitable format for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and prepare inputs  \n",
    "pil_images = load_pil_images(conversation)  \n",
    "\n",
    "# Process conversation and images into model-compatible input format  \n",
    "prepare_inputs = vl_chat_processor(  \n",
    "    conversations=conversation, images=pil_images, force_batchify=True  \n",
    ").to(vl_gpt.device)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate image embeddings\n",
    "\n",
    "Before running inference, process the image to obtain its embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Leverage the AMD ZenDNN plugin for PyTorch (zentorch) \n",
    "\n",
    "AMD has registered zentorch as a custom backend to `torch.compile`. This backend integrates ZenDNN optimizations after AOT Autograd through a function called `optimize()`. This function operates on the FX-based graph at the Aten IR layer to produce an optimized FX-based graph as the output. For more information about the plugin and its operations, see the [ZenDNN user guide](https://docs.amd.com/r/en-US/57300-ZenDNN-user-guide/ZenDNN). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cpu\":\n",
    "    if(backend == \"zentorch\"):\n",
    "        print(\"Backend: ZenTorch\")\n",
    "        import zentorch\n",
    "        torch._dynamo.reset()\n",
    "        vl_gpt.language_model.forward = torch.compile(vl_gpt.language_model.forward, backend=\"zentorch\")  \n",
    "    \n",
    "    elif(backend == \"inductor\"):\n",
    "        print(\"Backend: Inductor\")\n",
    "        torch._dynamo.reset()\n",
    "        vl_gpt.language_model.forward = torch.compile(vl_gpt.language_model.forward)  \n",
    "    \n",
    "    else:\n",
    "        print(\"Running in Eager mode\")\n",
    "else:\n",
    "    print(\"We are executing on GPU therefore we won't be leveraging any CPU-acceleration software like Zentorch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Profiler\n",
    "\n",
    "The PyTorch Profiler helps verify the operations (ops) of `torch.compile` and assess its effectiveness in optimizing the model. It provides insights into the model's performance by tracking execution times and pinpointing areas where optimizations can be made, ensuring that `torch.compile` is working as expected.\n",
    "\n",
    "This part can be skipped if your focus is on performance checks rather than detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start profiling\n",
    "# from torch.profiler import profile, record_function, ProfilerActivity\n",
    "# def trace_handler(prof):\n",
    "#     # Print profiling information after each step\n",
    "#     print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "\n",
    "# with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU],record_shapes=False,schedule=torch.profiler.schedule(wait=1, warmup=1, active=1),on_trace_ready=trace_handler,) as prof:\n",
    "#     for i in range(3):\n",
    "#     # Run the model to get the response\n",
    "#         outputs = vl_gpt.language_model.generate(\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#             attention_mask=prepare_inputs.attention_mask,\n",
    "#             pad_token_id=tokenizer.eos_token_id,\n",
    "#             bos_token_id=tokenizer.bos_token_id,\n",
    "#             eos_token_id=tokenizer.eos_token_id,\n",
    "#             max_new_tokens=max_new_tokens,\n",
    "#             do_sample=False,\n",
    "#             use_cache=True,\n",
    "#         )\n",
    "#         prof.step()\n",
    "\n",
    "# # To check the DataType\n",
    "# for name, param in vl_gpt.named_parameters():\n",
    "#     print(f\"Parameter: {name}, Shape: {param.shape}, Data Type: {param.dtype}\")\n",
    "#     print(f\"First few values: {param.flatten()[:5]}\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Warm-up inference (stabilization runs)\n",
    "\n",
    "To ensure stable performance, run a few inference cycles without measuring the time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(warmup):  \n",
    "    # Generate a response without timing for warmup  \n",
    "    outputs = vl_gpt.language_model.generate(  \n",
    "        inputs_embeds=inputs_embeds,  \n",
    "        attention_mask=prepare_inputs.attention_mask,  \n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        bos_token_id=tokenizer.bos_token_id,  \n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        min_new_tokens = max_new_tokens,  \n",
    "        max_new_tokens = max_new_tokens,  \n",
    "        do_sample=False,  \n",
    "        use_cache=True,  \n",
    "    )  \n",
    "    print(f\"WARMUP:{i+1} COMPLETED!\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Timed inference execution\n",
    "\n",
    "Now run the actual inference while measuring the latency for performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = 0.0  \n",
    "\n",
    "for i in range(iteration):  \n",
    "    tic = time.time()  # Start time  \n",
    "\n",
    "    # Generate response from the model  \n",
    "    outputs = vl_gpt.language_model.generate(  \n",
    "        inputs_embeds=inputs_embeds,  \n",
    "        attention_mask=prepare_inputs.attention_mask,  \n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        bos_token_id=tokenizer.bos_token_id,  \n",
    "        eos_token_id=tokenizer.eos_token_id,  \n",
    "        min_new_tokens = max_new_tokens,\n",
    "        max_new_tokens = max_new_tokens,  \n",
    "        do_sample=False,  \n",
    "        use_cache=True,  \n",
    "    )  \n",
    "\n",
    "    toc = time.time()  # End time  \n",
    "    delta = toc - tic  # Compute time taken  \n",
    "    total_time = total_time + delta  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Compute and display latency\n",
    "\n",
    "Next, calculate the average latency and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = total_time / iteration\n",
    "print( \n",
    "    f\"e2e_latency (TTFT + Generation Time) for step: {total_time:.6f} sec\", \n",
    "    flush=True, \n",
    ")\n",
    "\n",
    "tps_per_step = (max_new_tokens / total_time)\n",
    "print( \n",
    "    f\"Throughput: {tps_per_step:.6f} tokens/sec\", \n",
    "    flush=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Decode and display the output\n",
    "\n",
    "Finally, decode the generated token sequence into human-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = tokenizer.decode(outputs[0].to(device).tolist(), skip_special_tokens=True)  \n",
    "print(f\"{prepare_inputs['sft_format'][0]}\", answer)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline summary\n",
    "\n",
    "- Choose the hardware backend: CPU or GPU  \n",
    "- Load the Janus-Pro model and processor.  \n",
    "- Define the image and question for multimodal understanding.  \n",
    "- Preprocess the text and image inputs.  \n",
    "- Generate image embeddings for the model.  \n",
    "- Leverage zentorch.  \n",
    "- Run warmup iterations to stabilize performance.  \n",
    "- Perform timed inference to measure latency.  \n",
    "- Compute and display the average generation time.  \n",
    "- Decode and display the AI-generated response.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds_test0]",
   "language": "python",
   "name": "conda-env-ds_test0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
