{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Janus-Pro DeepSeek Model on AMD üß™\n",
    "\n",
    "This tutorial demonstrates how to perform multimodal inference with **Janus-Pro**, a new autoregressive framework, which is part of the [Janus-Series](https://github.com/deepseek-ai/Janus#-janus-series-unified-multimodal-understanding-and-generation-models) from Deepseek AI. We will run the model on high-performance AMD hardware, including EPYC‚Ñ¢ CPUs and Instinct‚Ñ¢ GPUs.\n",
    "\n",
    "\"Multimodal\" simply means the model can understand and process information from multiple sources at once, such as both text and images. By unifying these different data types, known as \"modalities,\" Janus enables sophisticated understanding and generation tasks.\n",
    "\n",
    "When executing the model on a CPU, we also show you the ability to leverage **AMD ZenDNN** or more precisely the **zentorch** plugin, to PyTorch to accelerate CPU-inferencing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites üß™\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardware requirements\n",
    "For this tutorial, you will need a system featuring an AMD Instinct GPU.  If you intend to also run the model on CPU and use AMD ZenDNN, you will need an AMD EPYC CPU. \n",
    "\n",
    "This tutorial was tested on:\n",
    "* AMD Instinct MI100\n",
    "* AMD Instinct MI210\n",
    "* AMD Instinct MI300X\n",
    "* 4th Gen AMD EPYC (Genoa)\n",
    "* 5th Gen AMD EPYC (Turin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software Requirements\n",
    "* Ubuntu 22.04: Ensure your system is running Ubuntu version 22.04 or later\n",
    "* For GPU executions, you will need ROCm 6.3 installed on your system. \n",
    "* PyTorch 2.6 or later\n",
    "* ZenTorch 5.0.2 or later\n",
    "* The official [Janus-Pro DeepSeek repository](https://github.com/deepseek-ai/Janus) cloned\n",
    "  \n",
    "**Note**: This tutorial was tested with `torch2.7.1+rocm6.3` and `torch2.6.0+cpu`and `zentorch-5.0.2`\n",
    "\n",
    "### Install and launch Jupyter Notebooks\n",
    "If Jupyter is not already installed on your system, install it and launch JupyterLab using the following commands:\n",
    "\n",
    "```\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Then to start the jupyter server run the following command:\n",
    "\n",
    "```\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port 8888 is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`.\n",
    "\n",
    "After the command executes, the terminal output displays a URL and token. Copy and paste this URL into your web browser on the host machine to access JupyterLab. After launching JupyterLab, upload this notebook to the environment and continue to follow the steps in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0Ô∏è‚É£ Prepare Environment: Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands will install all the required dependencies to ensure you can run this tutorial successfully. We will also install `janus` from Deepseek AI's Github repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision --index-url https://download.pytorch.org/whl/rocm6.3\n",
    "!pip install transformers ipywidgets\n",
    "!pip install git+https://github.com/deepseek-ai/Janus.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a quick sanity check for the PyTorch environment. We will validate taht the GPU hardware is accessible via the ROCm backend. If not, we will then execute the model on CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "print(\"--- GPU Verification ---\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"‚úÖ PyTorch has access to the GPU.\")\n",
    "    print(f\"ROCm Version (detected by PyTorch): {torch.version.hip}\")\n",
    "    print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU installed on the system: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ùå PyTorch CANNOT access the GPU. Please check your ROCm installation and drivers or proceed to continue with executing on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with importing the required Python libraries required for our tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from janus.models import MultiModalityCausalLM, VLChatProcessor\n",
    "from janus.utils.io import load_pil_images\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the following variables for use in the upcoming inference process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 1\n",
    "warmup = 0\n",
    "max_new_tokens = 512\n",
    "dtype = \"bfloat16\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0Ô∏è‚É£Choose Hardware backend - CPU or GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we show two possible hardware backends where you can execute your AI workload - namely CPU or GPU. If you want to deploy your workload on GPU, set `device = \"cuda\"`, else if you want to deploy on CPU, set `device = \"cpu\"`. The CPU supports multiple software backends, for example `zentorch`, the Intel¬Æ Extension for PyTorch (`ipex`), the default PyTorch CPU-backend, also known as `inductor` or you could execute in native mode (i.e eager-mode as opposed to graph-mode) \n",
    "\n",
    "Note: At the time of publishing this tutorial, the zentorch Plugin version 5.0.2 requires the CPU-only version of PyTorch 2.6. This is a limitation of PyTorch. We therefore remove any previous installation of torch and install the CPU-version of torch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" # Change to \"cpu\" to execute on CPU\n",
    "backend = \"zentorch\" # Available CPU backends: zentorch, inductor, ipex, or native [Eager Mode]\n",
    "\n",
    "if device == \"cpu\" and backend == \"zentorch\":\n",
    "    !pip uninstall -y torch torchvision torchaudio pytorch-triton-rocm\n",
    "    !pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cpu\n",
    "    !pip install zentorch #--no-cache-dir\n",
    "    import torch \n",
    "    import zentorch\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"Zentorch Version: {zentorch.__version__}\")\n",
    "\n",
    "amp_enabled = True if dtype != \"float32\" else False\n",
    "amp_dtype = getattr(torch, dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Model Initialization and Setup\n",
    "\n",
    "We begin by specifying the model path and initializing the necessary components for processing images and text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify the path to the model  \n",
    "model_path = \"deepseek-ai/Janus-Pro-7B\"  \n",
    "\n",
    "# Load the multimodal chat processor and tokenizer  \n",
    "vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)  \n",
    "tokenizer = vl_chat_processor.tokenizer  \n",
    "\n",
    "vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(  \n",
    "    model_path, trust_remote_code=True  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the model to use BFloat16 precision and move it to CPU for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl_gpt = vl_gpt.to(amp_dtype).to(device).eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Define Image and User Input\n",
    "We define an image and a text-based query to analyze its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"../assets/deepseek_janus_demo_small.jpg\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the code snippet below to check our image. That will also confirm that the image path is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now prepare the input payload for the vision-language model. This is achieved by constructing a `conversation` list that adheres to the model's required chat template. The user's message is a dictionary containing the role, the textual question embedded with an `<image_placeholder>` token, and the corresponding image object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is happening in this image?\"  \n",
    "\n",
    "conversation = [  \n",
    "    {  \n",
    "        \"role\": \"<|User|>\",  \n",
    "        \"content\": f\"<image_placeholder>\\n{question}\",  \n",
    "        \"images\": [image],  \n",
    "    },   \n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Preprocess Inputs (Image + Text)\n",
    "We load the image and convert the conversation data into a format suitable for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and prepare inputs  \n",
    "pil_images = load_pil_images(conversation)  \n",
    "\n",
    "# Process conversation and images into model-compatible input format  \n",
    "prepare_inputs = vl_chat_processor(  \n",
    "    conversations=conversation, images=pil_images, force_batchify=True  \n",
    ").to(vl_gpt.device)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Generate Image Embeddings\n",
    "Before running inference, we process the image to obtain its embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Leveraging AMD ZenDNN Plugin for PyTorch (`zentorch`) \n",
    "We have registered a custom backend to `torch.compile` called zentorch. This backend integrates ZenDNN optimizations after AOTAutograd through a function called `optimize()`. This function operates on the FX based graph at the ATEN IR to produce an optimized FX based graph as the output. Please checkout the [ZenDNN User Guide](https://docs.amd.com/r/en-US/57300-ZenDNN-user-guide/ZenDNN) for more information on the operation mechanism of the plugin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cpu\":\n",
    "    if(backend == \"zentorch\"):\n",
    "        print(\"Backend: ZenTorch\")\n",
    "        import zentorch\n",
    "        torch._dynamo.reset()\n",
    "        vl_gpt.language_model.forward = torch.compile(vl_gpt.language_model.forward, backend=\"zentorch\")  \n",
    "    \n",
    "    elif(backend == \"inductor\"):\n",
    "        print(\"Backend: Inductor\")\n",
    "        torch._dynamo.reset()\n",
    "        vl_gpt.language_model.forward = torch.compile(vl_gpt.language_model.forward)  \n",
    "    \n",
    "    else:\n",
    "        print(\"Running in Eager mode\")\n",
    "else:\n",
    "    print(\"We are executing on GPU therefore we won't be leveraging any CPU-acceleration software like Zentorch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Profiler\n",
    "The **Profiler** helps verify the operations (ops) and assess the effectiveness of **torch.compile** in optimizing the model. It provides insights into the performance of the model by tracking execution times and pinpointing areas where optimizations can be made, ensuring that torch.compile is working as expected.\n",
    "\n",
    "This part can be skipped if the focus is on performance checks rather than detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start profiling\n",
    "# from torch.profiler import profile, record_function, ProfilerActivity\n",
    "# def trace_handler(prof):\n",
    "#     # Print profiling information after each step\n",
    "#     print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=-1))\n",
    "\n",
    "# with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU],record_shapes=False,schedule=torch.profiler.schedule(wait=1, warmup=1, active=1),on_trace_ready=trace_handler,) as prof:\n",
    "#     for i in range(3):\n",
    "#     # Run the model to get the response\n",
    "#         outputs = vl_gpt.language_model.generate(\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#             attention_mask=prepare_inputs.attention_mask,\n",
    "#             pad_token_id=tokenizer.eos_token_id,\n",
    "#             bos_token_id=tokenizer.bos_token_id,\n",
    "#             eos_token_id=tokenizer.eos_token_id,\n",
    "#             max_new_tokens=max_new_tokens,\n",
    "#             do_sample=False,\n",
    "#             use_cache=True,\n",
    "#         )\n",
    "#         prof.step()\n",
    "\n",
    "# # To check the DataType\n",
    "# for name, param in vl_gpt.named_parameters():\n",
    "#     print(f\"Parameter: {name}, Shape: {param.shape}, Data Type: {param.dtype}\")\n",
    "#     print(f\"First few values: {param.flatten()[:5]}\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Warmup Inference (Stabilization Runs)\n",
    "To ensure stable performance, we run a few inference cycles without measuring time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(warmup):  \n",
    "    # Generate a response without timing for warmup  \n",
    "    outputs = vl_gpt.language_model.generate(  \n",
    "        inputs_embeds=inputs_embeds,  \n",
    "        attention_mask=prepare_inputs.attention_mask,  \n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        bos_token_id=tokenizer.bos_token_id,  \n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        min_new_tokens = max_new_tokens,  \n",
    "        max_new_tokens = max_new_tokens,  \n",
    "        do_sample=False,  \n",
    "        use_cache=True,  \n",
    "    )  \n",
    "    print(f\"WARMUP:{i+1} COMPLETED!\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Timed Inference Execution\n",
    "We now run actual inference while measuring latency for performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = 0.0  \n",
    "\n",
    "for i in range(iteration):  \n",
    "    tic = time.time()  # Start time  \n",
    "\n",
    "    # Generate response from the model  \n",
    "    outputs = vl_gpt.language_model.generate(  \n",
    "        inputs_embeds=inputs_embeds,  \n",
    "        attention_mask=prepare_inputs.attention_mask,  \n",
    "        pad_token_id=tokenizer.eos_token_id,  \n",
    "        bos_token_id=tokenizer.bos_token_id,  \n",
    "        eos_token_id=tokenizer.eos_token_id,  \n",
    "        min_new_tokens = max_new_tokens,\n",
    "        max_new_tokens = max_new_tokens,  \n",
    "        do_sample=False,  \n",
    "        use_cache=True,  \n",
    "    )  \n",
    "\n",
    "    toc = time.time()  # End time  \n",
    "    delta = toc - tic  # Compute time taken  \n",
    "    total_time = total_time + delta  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Compute and Display Latency\n",
    "We calculate the average latency and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = total_time / iteration\n",
    "print( \n",
    "    f\"e2e_latency (TTFT + Generation Time) for step: {total_time:.6f} sec\", \n",
    "    flush=True, \n",
    ")\n",
    "\n",
    "tps_per_step = (max_new_tokens / total_time)\n",
    "print( \n",
    "    f\"Throughput: {tps_per_step:.6f} tokens/sec\", \n",
    "    flush=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Decode and Display the Model‚Äôs Output\n",
    "Finally, we decode the generated token sequence into human-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = tokenizer.decode(outputs[0].to(device).tolist(), skip_special_tokens=True)  \n",
    "print(f\"{prepare_inputs['sft_format'][0]}\", answer)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ú® Summary of the Pipeline\n",
    "‚úÖ Step 0: Choose your hardware backend - CPU or GPU\n",
    "‚úÖ Step 1: Load Janus-Pro model and processor.  \n",
    "‚úÖ Step 2: Define image and question for multimodal understanding.  \n",
    "‚úÖ Step 3: Preprocess text + image inputs.  \n",
    "‚úÖ Step 4: Generate image embeddings for the model.\n",
    "‚úÖ Step 5: Leveraging ZenTorch.  \n",
    "‚úÖ Step 6: Run warmup iterations to stabilize performance.  \n",
    "‚úÖ Step 7: Perform timed inference to measure latency.  \n",
    "‚úÖ Step 8: Compute and display average generation time.  \n",
    "‚úÖ Step 9: Decode and display the AI-generated response.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds_test0]",
   "language": "python",
   "name": "conda-env-ds_test0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
