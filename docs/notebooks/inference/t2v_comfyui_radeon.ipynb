{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c54efd",
   "metadata": {},
   "source": [
    "# Text-to-video generation with ComfyUI and an AMD Radeon GPU\n",
    "\n",
    "With the advancement of Artificial Intelligence Generated Content (AIGC) technology, the fields of text-to-video and image-to-video generation have garnered widespread attention from visual designers, art enthusiasts, and media creators.\n",
    "\n",
    "[ComfyUI](https://github.com/comfyanonymous/ComfyUI) is a ‌node-based graphical interface‌ designed for diffusion models, enabling users to visually construct AI image/video generation workflows through modular operations. Its modular node design, efficiency, compatibility, and workflow advantage make it a perfect choice for media creators to ‌boost productivity.\n",
    "\n",
    "This tutorial guides you through setting up and running ComfyUI on AMD Radeon™ GPUs using ROCm™ software. Learn how to configure your environment, install the ComfyUI tool, and generate video from text on AMD consumer GPUs.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup.\n",
    "\n",
    "### Operating system\n",
    "\n",
    "- **Ubuntu 22.04/24.04**: Ensure your system is running Ubuntu 22.04 or 24.04.\n",
    "\n",
    "### Hardware\n",
    "\n",
    "- **AMD Radeon GPUs**: This tutorial was tested on an AMD Radeon RX 7900 XTX GPU. Ensure you are using an AMD Radeon GPU or compatible hardware with ROCm support and that your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "### Software\n",
    "\n",
    "- **ROCm 6.3**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html). After installation, confirm your setup using:\n",
    "\n",
    "    ```bash\n",
    "    rocm-smi\n",
    "    ```\n",
    "\n",
    "    This command lists your AMD GPUs with relevant details, similar to the image below:\n",
    "\n",
    "    ![rocm-smi output](../assets/comfyui-rocmsmi.png)\n",
    "\n",
    "## ComfyUI setup\n",
    "\n",
    "To set up the ComfyUI inference environment, follow the steps below.\n",
    "\n",
    "### Prepare the inference environment\n",
    "Create and activate a virtual environment using `conda` or `venv`.\n",
    "\n",
    "```bash\n",
    "conda create -n comfyui_env_test python=3.12\n",
    "conda activate comfyui_env_test\n",
    "```\n",
    "\n",
    "### PyTorch installation\n",
    "\n",
    "Install the PyTorch ROCm wheels in the virtual environment. See [Install PyTorch for Radeon GPUs](https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-pytorch.html) for installation instructions.\n",
    "\n",
    "\n",
    "### Install and launch Jupyter\n",
    "\n",
    "Inside the Python virtual environment, install Jupyter using the following command:\n",
    "\n",
    "```bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Then start the Jupyter server:\n",
    "\n",
    "```bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`.\n",
    "\n",
    "### Verify the PyTorch installation\n",
    "\n",
    "Verify that PyTorch is correctly installed.\n",
    "\n",
    "**Step 1**: Verify PyTorch is installed and can detect the GPU compute device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -c 'import torch' 2> /dev/null && echo 'Success' || echo 'Failure'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c27cd1d",
   "metadata": {},
   "source": [
    "The expected result is `Success`.\n",
    "\n",
    "**Step 2**: Confirm the GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65231a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -c 'import torch; print(torch.cuda.is_available())'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e19d6a",
   "metadata": {},
   "source": [
    "The expected result is `True`.\n",
    "\n",
    "**Step 3**: Display the installed GPU device name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e935c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -c \"import torch; print(f'device name [0]:', torch.cuda.get_device_name(0))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8eed72",
   "metadata": {},
   "source": [
    "The expected result should be similar to: `device name [0]: Radeon RX 7900 XTX`\n",
    "\n",
    "``` bash\n",
    "device name[0]: <Supported AMD GPU>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9c01e",
   "metadata": {},
   "source": [
    "### ComfyUI installation\n",
    "\n",
    "Install ComfyUI from source on the system with the AMD GPU.\n",
    "\n",
    "Clone the following repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86b6e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/comfyanonymous/ComfyUI.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a7096",
   "metadata": {},
   "source": [
    "Ensure that PyTorch will not be reinstalled with the CUDA version:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf465fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ComfyUI\n",
    "!sed -i.bak -E '/^(torch|torchaudio|torchvision)([<>=~!0-9.]*)?$/s/^/# /' requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bdfd3f",
   "metadata": {},
   "source": [
    "Install the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2805070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c83bf4",
   "metadata": {},
   "source": [
    "## Running text-to-video generation\n",
    "\n",
    "Follow these steps to generate video from your text.\n",
    "\n",
    "### Model preparation\n",
    "\n",
    "LTX Video is an efficient video model from Lightricks. `Ltx-video-2B-v0.9.5` is used for this tutorial.\n",
    "\n",
    "Download the LTX Video model [ltx-video-2b-v0.9.5.safetensors](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltx-video-2b-v0.9.5.safetensors) and text encoder model [t5xxl_fp16.safetensors](https://huggingface.co/Comfy-Org/mochi_preview_repackaged/blob/main/split_files/text_encoders/t5xxl_fp16.safetensors) files. Place these model files in the `ComfyUI/models/checkpoints` and `ComfyUI/models/text_encoders` folders. Use the following code to automate the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed55b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Define the URLs and destination paths\n",
    "models = {\n",
    "    \"https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltx-video-2b-v0.9.5.safetensors\":\n",
    "        \"models/checkpoints/ltx-video-2b-v0.9.5.safetensors\",\n",
    "    \"https://huggingface.co/Comfy-Org/mochi_preview_repackaged/resolve/main/split_files/text_encoders/t5xxl_fp16.safetensors\":\n",
    "        \"models/text_encoders/t5xxl_fp16.safetensors\"\n",
    "}\n",
    "\n",
    "# Create target directories if they don't exist\n",
    "for path in models.values():\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "# Download files\n",
    "for url, path in models.items():\n",
    "    print(f\"Downloading {url} to {path}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    with open(path, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    print(f\"✅ Downloaded to {path}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b527fc04",
   "metadata": {},
   "source": [
    "Different diffusion model pipelines might contain different submodels that should be placed into the proper subfolders under `ComfyUI/models`.\n",
    "\n",
    "### Launch the server\n",
    "\n",
    "Launch the ComfyUI server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a557ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc33086c",
   "metadata": {},
   "source": [
    "After the server launches, the log will show the following entries:\n",
    "\n",
    "```\n",
    "Starting server\n",
    "\n",
    "To see the GUI go to: http://127.0.0.1:8188\n",
    "```\n",
    "\n",
    "Copy the address `http://127.0.0.1:8188` and paste it into your browser. The resulting webpage displays a default workflow which looks like this:\n",
    "\n",
    "![default workflow](../assets/comfyui-default-json.png)\n",
    "\n",
    "### Load the workflow\n",
    "\n",
    "The ComfyUI workflow defines the full pipeline and parameters used to generate an image or video. It's formatted as a JSON file or encoded in a WebP animated image (`*.webp`). You can construct your own workflow from scratch or customize the workflow from third party.\n",
    "\n",
    "Download the LTX Video text-to-video workflow (in `*.json` or `*.webp` format) from the [example page](https://comfyanonymous.github.io/ComfyUI_examples/ltxv/), then load it or drag it on the ComfyUI GUI. The loaded workflow looks like the image below:\n",
    "\n",
    "![ltx workflow](../assets/comfyui-ltx-workflow.png)\n",
    "\n",
    "The workflow is composed of different nodes, each of which has different functionality.\n",
    "\n",
    "- The `Load Checkpoint` node handles diffusion model loading. The `ckpt_name` field can be changed if there are other diffusion models available.\n",
    "\n",
    "- The `Load Image` node handles loading the input image. Upload the required image using this node.\n",
    "\n",
    "- The `CLIP TEXT Encode (Positive Prompt)` node is a placeholder for the positive prompt. You can change it to your own prompt. Here is the recommended prompt for this tutorial:\n",
    "\n",
    "  > A drone quickly rises through a bank of morning fog, revealing a pristine alpine lake surrounded by snow-capped mountains. The camera glides forward over the glassy water, capturing perfect reflections of the peaks. As it continues, the perspective shifts to reveal a lone wooden cabin with a curl of smoke from its chimney, nestled among tall pines at the lake's edge. The final shot tracks upward rapidly, transitioning from intimate to epic as the full mountain range comes into view, bathed in the golden light of sunrise breaking through scattered clouds.\n",
    "\n",
    "- The `EmptyLTXVLatentVideo` node is used to control the frame size and length for the generated video.\n",
    "\n",
    "For more information about the ComfyUI nodes, see the [ComfyUI wiki](https://comfyui-wiki.com/en/comfyui-nodes).\n",
    "\n",
    "### Run inference\n",
    "\n",
    "After the workflow is ready, click **Run** to initiate the whole pipeline to process. The output log for the inference will look like this:\n",
    "\n",
    "```\n",
    " got prompt\n",
    " model weight dtype torch.bfloat16, manual cast: None\n",
    " model_type FLUX\n",
    " VAE load device: cuda:0, offload device: cpu, dtype: torch.float32\n",
    " no CLIP/text encoder weights in checkpoint, the text encoder model will not be loaded.\n",
    " CLIP/text encoder model load device: cuda:0, offload device: cpu, current: cpu, dtype: torch.float16\n",
    " Requested to load MochiTEModel_\n",
    " loaded completely 23280.8 9083.38671875 True\n",
    " Requested to load LTXV\n",
    " loaded completely 10667.71279296875 3667.902587890625 True\n",
    " 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:43<00:00,  1.46s/it]\n",
    " Requested to load VideoVAE\n",
    " loaded completely 5763.6171875 4756.450138092041 True\n",
    " Warning: Ran out of memory when regular VAE decoding, retrying with tiled VAE decoding.\n",
    " Prompt executed in 66.29 seconds\n",
    "```\n",
    "\n",
    "The final output video is shown in the `SaveAnimatedWEBP` node, while the `*.webp` file output can be found in the `ComfyUI/output/` folder.\n",
    "\n",
    "![ltx output](../assets/comfyui-output.webp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
