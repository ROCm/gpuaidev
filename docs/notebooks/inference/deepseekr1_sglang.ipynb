{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DeepSeek-R1 with SGLang and example applications\n",
    "\n",
    "Throughout this tutorial, you'll leverage AMD GPUs to deploy the powerful language model [DeepSeek-R1-Distill-Qwen-32B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B). The tutorial covers setting up an AI development environment and explores two main practical applications:\n",
    "\n",
    "*  **Advanced chatbot:** Using Open WebUI to create a sophisticated chatbot with web search and file interaction capabilities.\n",
    "*  **Code development assistant:** Installing and utilizing the AI Toolkit Code extension to perform code analysis and pair programming tasks.\n",
    "\n",
    "Let's dive in!\n",
    "\n",
    "**Note**: The same steps can be applied to serve DeepSeek-R1(671B) on a single AMD MI300X node. For more information about running this model, see [this blog post](https://rocm.blogs.amd.com/artificial-intelligence/DeepSeekR1-Part2/README.html). \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup. \n",
    "\n",
    "### Operating system\n",
    "\n",
    "* **Ubuntu 22.04**: Ensure your system is running Ubuntu version 22.04.\n",
    "\n",
    "### Hardware\n",
    "\n",
    "* **AMD Instinct™ GPUs**: This tutorial was tested on an AMD Instinct MI300X GPU. Ensure you are using an AMD Instinct GPU or compatible hardware with ROCm support and that your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "### Software\n",
    "\n",
    "* **ROCm 6.2 or 6.3**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html). After installation, confirm your setup using:\n",
    "\n",
    "    ``` bash\n",
    "    amd-smi\n",
    "    ```\n",
    "\n",
    "    This command lists your AMD GPUs with relevant details.\n",
    "    \n",
    "    **Note**: For ROCm 6.4 and earlier, use the `rocm-smi` command instead.\n",
    "\n",
    "* **Docker**: Ensure Docker is installed and configured correctly. Follow the Docker installation guide for your operating system.\n",
    "\n",
    "   **Note**: Ensure the Docker permissions are correctly configured. To configure permissions to allow non-root access, run the following commands:\n",
    "\n",
    "   ``` bash\n",
    "   sudo usermod -aG docker $USER\n",
    "   newgrp docker\n",
    "   ```\n",
    "\n",
    "   Verify Docker is working correctly with:\n",
    "\n",
    "   ``` bash\n",
    "   docker run hello-world\n",
    "   ```\n",
    "\n",
    "* **Visual Studio Code**:  Ensure Visual Studio Code (VS Code) is downloaded. You can download VS Code from the [official download page](https://code.visualstudio.com/Download). \n",
    "\n",
    "\n",
    "## Launch Jupyter notebooks \n",
    "\n",
    "Install Jupyter using the following command:\n",
    "\n",
    "``` bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Start the Jupyter server:\n",
    "\n",
    "``` bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`.\n",
    "\n",
    "**Note**: The following steps should be executed within your Jupyter notebook after successfully launching the Jupyter server.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching the SGLang server on AMD GPUs\n",
    "\n",
    "The following command pulls the specified SGLang Docker image, starts the container, and then runs the `sglang.launch_server` command to initiate the server.\n",
    "\n",
    "**Important**: The latest version of the SGLang Docker image tag must be retrieved from [their official Docker Hub page](https://hub.docker.com/r/lmsysorg/sglang/tags). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "export INFERENCE_PORT=30000\n",
    "export INFERENCE_MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\n",
    "export CUDA_VISIBLE_DEVICES=0\n",
    "export DEBUG_HIP_BLOCK_SYNC=1024\n",
    "export API_KEY=\"abc-123\"\n",
    "export GPU_FORCE_BLIT_COPY_SIZE=6\n",
    "export SGLANG_DIMG=\"lmsysorg/sglang:v0.4.5.post3-rocm630\"\n",
    "\n",
    "docker run -d --rm \\\n",
    "    --ipc=host \\\n",
    "    --privileged \\\n",
    "    --shm-size 16g \\\n",
    "    --device=/dev/kfd \\\n",
    "    --device=/dev/dri \\\n",
    "    --group-add video \\\n",
    "    --cap-add=SYS_PTRACE \\\n",
    "    --cap-add=CAP_SYS_ADMIN \\\n",
    "    --security-opt seccomp=unconfined \\\n",
    "    --security-opt apparmor=unconfined \\\n",
    "    --env \"HIP_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES\" \\\n",
    "    -p 3000:3000 \\\n",
    "    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
    "    --name sglang_server \"lmsysorg/sglang:v0.4.5.post3-rocm630\" \\\n",
    "    python3 -m sglang.launch_server \\\n",
    "    --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B \\\n",
    "    --port 3000 \\\n",
    "    --trust-remote-code \\\n",
    "    --disable-radix-cache \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --api-key \"abc-123\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Note:** Select an appropriate port number that is open to the public. Ensure the network and firewall settings allow connections to your server via the selected port number.\n",
    "\n",
    "Verify that the model is loaded and the server is launched successfully. The following script is designed to continuously check until either the server is ready or the command times out. \n",
    "\n",
    "**Note**: Downloading and loading the model can take several minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, requests, sys, subprocess\n",
    "\n",
    "# ---------- tweakables ------------------------------------------------------\n",
    "PORT          = int(os.getenv(\"INFERENCE_PORT\", 30000))\n",
    "URL           = f\"http://localhost:{PORT}/health_generate\"\n",
    "HEADERS       = {\"Authorization\": f\"Bearer {os.getenv('API_KEY', 'abc-123')}\"}\n",
    "\n",
    "SLEEP         = 30          # seconds between polls\n",
    "TIMEOUT       = 20 * 60     # give up after 20 min  (0 ⇢ wait forever)\n",
    "CONTAINER_NAME = \"sglang_server\"   # <-- the one you used in docker run\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def dump_logs(container, lines=100):\n",
    "    \"\"\"Print the last *lines* of docker logs for *container*.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"docker\", \"logs\", \"--tail\", str(lines), container],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            check=False,\n",
    "        )\n",
    "        print(\"\\n--- docker logs (tail) -----------------------------\")\n",
    "        print(result.stdout or \"(no output)\")\n",
    "        print(\"--- end docker logs --------------------------------\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\n Docker CLI not found—cannot show logs\\n\")\n",
    "\n",
    "start = time.time()\n",
    "print(f\"Polling {URL} every {SLEEP}s (timeout: {TIMEOUT or '∞'} s)…\")\n",
    "\n",
    "while True:\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    # --- timeout handling ----------------------------------------------------\n",
    "    if TIMEOUT and elapsed >= TIMEOUT:\n",
    "        print(f\"\\n Timeout: model didn’t load within {TIMEOUT/60:.1f} min\")\n",
    "        dump_logs(CONTAINER_NAME)\n",
    "        sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        if requests.get(URL, headers=HEADERS, timeout=10).ok:\n",
    "            print(f\"\\n✅  Model is ready! Total load time: {elapsed:.1f}s\")\n",
    "            break\n",
    "        else:\n",
    "            status = \"server up, model still loading\"\n",
    "    except requests.exceptions.RequestException:\n",
    "        status = \"server not responding yet\"\n",
    "\n",
    "    print(f\"[{elapsed:5.0f}s] {status}. Retrying in {SLEEP}s …\")\n",
    "    time.sleep(SLEEP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Upon a successful launch, your server should be accepting incoming traffic through an OpenAI-compatible API. Retrieve the public IP address of your server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl ifconfig.me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can resume the rest of this tutorial on your own local device. Verify the endpoint is reachable by executing the following from your local device using the IP address from the previous cell. The value of `PORT_NUMBER` was set to `INFERENCE_PORT` when you executed the SGLang server cell.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://YOUR_SERVER_PUBLIC_IP:PORT_NUMBER/v1/models -H \"Authorization: Bearer abc-123\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Advanced chatbot with OpenWebUI \n",
    "\n",
    "**Note**: The rest of this tutorial is designed to be executed on your own local device.\n",
    "\n",
    "Follow the installation instructions from the [Open WebUI GitHub repository](https://github.com/open-webui/open-webui).\n",
    "\n",
    "After installation, configure your endpoint URL in the Open WebUI client as follows:\n",
    "\n",
    "- Navigate to `Settings` as shown in the image below:\n",
    "\n",
    "  ![OpenWebUI Setup 1](../assets/openwebui1.png)\n",
    "\n",
    "- Select `Connections` from the left tab.\n",
    "  - Enter the `URL` so that it matches this format: `http://YOUR_SERVER_PUBLIC_IP:PORT_NUMBER/v1`.\n",
    "  - Enter the `Key` to match the API key you passed to `sglang.launch_server`.\n",
    "  - Enter the model name (under `Model IDs`) that exactly matches the argument you passed to `sglang.launch_server`. For example, `deepseek-ai/DeepSeek-R1-Distill-Qwen-32B`. \n",
    "  - Click on the `+` button.\n",
    "  - Click on the `Save` button. \n",
    "\n",
    "  ![OpenWebUI Setup 2](../assets/openwebui2.png)\n",
    "\n",
    "\n",
    "## Chatbot testing with DeepSeek-R1\n",
    "\n",
    "Use Open WebUI to interact with your chatbot. Here is an example prompt:\n",
    "\n",
    "```\n",
    "Imagine facing east. Turn 90° left, then 270° right, then 180° left. Which direction are you facing?\n",
    "```\n",
    "\n",
    "Follow up with a request for code visualization:\n",
    "\n",
    "```\n",
    "Can you give me a simple Python code without importing external libraries to visualize this step-by-step with Unicode arrows?\n",
    "```\n",
    "\n",
    "![OpenWebUI Example](../assets/webui_example.gif)\n",
    "\n",
    "\n",
    "## Code development assistant using the VS Code AI Toolkit\n",
    "\n",
    "Follow these steps to install the AI Toolkit for VS Code extension in VS Code:\n",
    "\n",
    "- Open VS Code.\n",
    "- Navigate to **Extensions** (`Ctrl+Shift+X`).\n",
    "- Search for and install **VS Code AI Toolkit**.\n",
    "- Click on `remote inference` as shown in the image below:\n",
    "\n",
    "  ![AI Toolkit Setup 1](../assets/aitoolkit1.png)\n",
    "\n",
    "- Select `Add a custom model`.\n",
    "\n",
    "  ![AI Toolkit Setup 2](../assets/aitoolkit2.png)\n",
    "\n",
    "- Enter the Open AI-compatible URL matching this format: `http://YOUR_SERVER_PUBLIC_IP:PORT_NUMBER/v1/chat/completions`.\n",
    "\n",
    "  ![AI Toolkit Setup 3](../assets/aitoolkit_url.png)\n",
    "\n",
    "- Enter the model name so that it exactly matches the argument passed to `sglang.launch_server`, for example, `deepseek-ai/DeepSeek-R1-Distill-Qwen-32B`.\n",
    "\n",
    "  ![AI Toolkit Setup 4](../assets/aitoolkit3.png)\n",
    "\n",
    "- Press **Enter** to display the model name. \n",
    "\n",
    "  ![AI Toolkit Setup 5](../assets/aitoolkit5.png)\n",
    "\n",
    "- Enter the HTTP header for authorization matching this format `Authorization: Bearer API KEY` exactly as specified, where `API KEY` must match the key you passed to `sglang.launch_server`. If you used the exact same command provided in this tutorial, enter `Authorization: Bearer abc-123`.\n",
    "\n",
    "  ![AI Toolkit Setup 6](../assets/aitoolkit6.png)\n",
    "\n",
    "After you've completed the steps above, your model should be listed under `MY MODELS` on the left. Click your model to start the corresponding playground.\n",
    "\n",
    "  ![AI Toolkit Setup 7](../assets/aitoolkit.png)\n",
    "\n",
    "\n",
    "## Build a snake game \n",
    "\n",
    "In VS Code, make this request:\n",
    "\n",
    "```\n",
    "\"Can you build a classic snake game? Include 'Powered by DeepSeek-R1 on AMD MI300X' in the corner. Use Python.\"\n",
    "```\n",
    "\n",
    "## Optional advanced challenge: Pac-Man\n",
    "Try building a Pac-Man game with a maximum of three prompts.\n",
    "\n",
    "\n",
    "Happy coding! If you encounter issues or have questions, don’t hesitate to ask or raise an issue on our [Github page](https://github.com/ROCm/gpuaidev)!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
