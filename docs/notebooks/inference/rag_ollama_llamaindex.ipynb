{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e91244a-3b94-471a-a5de-fc8d931df16a",
   "metadata": {},
   "source": [
    "# Constructing a RAG System: LlamaIndex & Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa90a7b-a662-4e20-adcb-b233d1afc709",
   "metadata": {},
   "source": [
    "AMD Radeon GPUs are officially supported by [ROCm](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/), ensuring compatibility with industry-standard software frameworks. This Jupyter notebook leverages [Ollama](https://ollama.com/) and [Llamaindex]((https://docs.llamaindex.ai/en/stable/)), powered by ROCm, to build a Retrieval-Augmented Generation (RAG) application. Llamaindex facilitates the creation of a pipeline from reading PDFs to indexing datasets and building a query engine, while Ollama provides the backend service for Large Language Model (LLM) inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This tutorial was developed and tested using the following setup:\n",
    "\n",
    "### 1. Hardware\n",
    "AMD Radeon™ GPUs: Ensure you have an AMD Radeon™ GPU that supports ROCm. This tutorial was tested on the AMD Radeon PRO W7900.\n",
    "### 2. Software\n",
    "- **ROCm 6.2**: Install ROCm by following the official installation instructions for Radeon GPUs.\n",
    "- **Python 3.8**: Ensure Python is installed and available in your environment.\n",
    "### 3. Environment\n",
    "Root or sudo access is required for software installations and configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c02ae1c",
   "metadata": {},
   "source": [
    "\n",
    "## Install and Launch Jupyter Notebooks\n",
    "\n",
    "If Jupyter is not already installed on your system, you can install it and launch JupyterLab using the following commands:\n",
    "\n",
    "```bash\n",
    "pip install jupyter\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "**Note**: Ensure that port 8888 is not already in use on your system before running the above command. If it is, you can specify a different port by replacing --port=8888 with another port number (e.g., --port=8890).\n",
    "\n",
    "Once the command is executed, the terminal will display an output containing a URL and token. Copy and paste this URL into your web browser on the host machine to access JupyterLab. After launching JupyterLab, upload this notebook to the environment and continue following the steps in this tutorial.\n",
    "\n",
    "## Install Ollama\n",
    "\n",
    "Ollama provides seamless support for AMD ROCm GPUs, offering optimized performance out of the box. Use the following command to install Ollama on Linux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8c7f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e9454-54a8-45f4-95dd-94f5206c6199",
   "metadata": {},
   "source": [
    "**Note**: Ollama's official installation guide is available [here](https://github.com/ollama/ollama/blob/main/docs/linux.md).\n",
    "\n",
    "Start Ollama and verify it is running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3580a83-1e19-4f7e-b19e-8c58e6f899c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo systemctl start ollama\n",
    "!sudo systemctl status ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752c18f8-c078-49b1-ab7d-d301e9e9a67d",
   "metadata": {},
   "source": [
    "## Download Models\n",
    "Pull the required models for RAG:\n",
    "\n",
    "**IMPORTANT**: If the Ollama server is running as a foreground process from the previous step, you need to continue the rest of this notebook in a new instance.\n",
    "\n",
    "**Note**: Alternative models are available to use [here](https://ollama.com/search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ae8c42-1503-4048-a674-beeac5ad1b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull nomic-embed-text\n",
    "!ollama pull llama3.1:8b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aed7f87",
   "metadata": {},
   "source": [
    "Verify the downloaded models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5793f235-a868-4e75-a89c-38b3908ac565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME           ID              SIZE      MODIFIED     \n",
      "llama3.1:8b    42182419e950    4.7 GB    2 months ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list llama3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bbcae7-cd5d-4f15-9498-d50aa31c5558",
   "metadata": {},
   "source": [
    "Refer to [Ollama's documentation](https://github.com/ollama/ollama) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ffe628-6f6b-4aa1-a2cf-a7381d66c176",
   "metadata": {},
   "source": [
    "## (Optional) Install PyTorch\n",
    "\n",
    "For this tutorial, PyTorch is optional. PyTorch utilities are used in this section for verificaiton purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0184d5c1-ada2-46e0-87e6-2438c186fd85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf44525-ab00-4b38-929b-aceaf02d38fb",
   "metadata": {},
   "source": [
    "Check installed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38fec6ac-1bdd-4bd6-90c8-371eede9e2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-triton-rocm                      3.1.0\n",
      "torch                                    2.5.1+rocm6.2\n",
      "torchaudio                               2.5.1+rocm6.2\n",
      "torchvision                              0.20.1+rocm6.2\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe41347",
   "metadata": {},
   "source": [
    "Verify GPU functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc3616ed-475d-49c5-b0ad-db8200e77491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "# Query GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    print('Using GPU:', torch.cuda.get_device_name(0))\n",
    "    print('GPU properties:', torch.cuda.get_device_properties(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f7191a-fdaa-4b83-86b1-75d1811a76a8",
   "metadata": {},
   "source": [
    "## Install LlamaIndex and Dependencies\n",
    "Run the following command to install LlamaIndex and related packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec9028-1516-4355-8c79-70967753e22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index llama-index-llms-ollama llama-index-embeddings-ollama llama-index-vector-stores-chroma chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b471006",
   "metadata": {},
   "source": [
    "Verify installations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6aed23c-a92c-4482-bab1-d467f15c148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama-index                              0.12.10\n",
      "llama-index-agent-openai                 0.4.1\n",
      "llama-index-cli                          0.4.0\n",
      "llama-index-core                         0.12.10.post1\n",
      "llama-index-embeddings-ollama            0.5.0\n",
      "llama-index-embeddings-openai            0.3.1\n",
      "llama-index-indices-managed-llama-cloud  0.6.3\n",
      "llama-index-llms-ollama                  0.5.0\n",
      "llama-index-llms-openai                  0.3.13\n",
      "llama-index-multi-modal-llms-openai      0.4.2\n",
      "llama-index-program-openai               0.3.1\n",
      "llama-index-question-gen-openai          0.3.0\n",
      "llama-index-readers-file                 0.4.3\n",
      "llama-index-readers-llama-parse          0.4.0\n",
      "llama-index-readers-web                  0.3.3\n",
      "llama-index-vector-stores-chroma         0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9913d2",
   "metadata": {},
   "source": [
    "## Build RAG Pipeline\n",
    "\n",
    "### Set Up Indexing and Query Engine\n",
    "\n",
    "Import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e99b042-d0d5-4809-bc52-a5a3b0b1da21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa14454-f5cc-4b84-b979-6337a8622678",
   "metadata": {},
   "source": [
    "### Configure Embedding and LLM Models\n",
    "\n",
    "LlamaIndex implements the Ollama client interface to interact with the Ollama service. Here we request both embedding and LLM services from Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1931a4cc-4f0d-42fd-a7b8-68227a9f91fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embedding model\n",
    "emb_fn=\"nomic-embed-text\"\n",
    "Settings.embed_model = OllamaEmbedding(model_name=emb_fn)\n",
    "\n",
    "# Set ollama model\n",
    "Settings.llm = Ollama(model=\"llama3.1:8b\", request_timeout=120.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe9b9d8-186e-4027-a87f-8fb5ece1c6d1",
   "metadata": {},
   "source": [
    "### Use Data for RAG\n",
    "\n",
    "Download a PDF (e.g., ROCm documentation) and save it to the `./data` directory:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c9dc649-b2d4-43ab-90e9-486d76b6ffaa",
   "metadata": {},
   "source": [
    "!mkdir ./data && cd ./data && wget --recursive --level=1 --content-disposition --accept=pdf -np -nH --cut-dirs=6 https://rocm.docs.amd.com/_/downloads/radeon/en/latest/pdf/ && cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae962fa-ba30-47ea-95e6-443c81874641",
   "metadata": {},
   "source": [
    "The SimpleDirectoryReader is the most commonly used data connector that just works.\n",
    "Simply pass in a input directory or a list of files.\n",
    "It will select the best file reader based on the file extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5e0a9e5-b50d-4675-9419-b22ed11ee99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_dir=\"./data/\").load_data()\n",
    "\n",
    "# Check the content\n",
    "print(documents[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08adb51-a555-4b3d-80f1-a477cdfbefbf",
   "metadata": {},
   "source": [
    "### Create Vector Dataset with Chroma\n",
    "\n",
    "[Chroma DB](https://www.trychroma.com/) is a database that stores and queries embeddings, documents, and metadata for LLM apps which is also well integrated with LlamaIndex. We use it to create the vector dataset by sourcing the PDF file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "741d2124-3812-4ba4-bc80-67411bf33d45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize client and save data\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db/rocm_db\")\n",
    "# create collection\n",
    "chroma_collection = db.get_or_create_collection(\"rocm_db\")\n",
    "\n",
    "# assign chroma as the vector_store to the context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f46bb50a-a21b-4078-a540-43c714567014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vector index per-document\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    transformations=[SentenceSplitter(chunk_size=512, chunk_overlap=20)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab337a76-3799-4b6d-8fb8-f833bb85cb37",
   "metadata": {},
   "source": [
    "### Create Query Engine\n",
    "\n",
    "Next, create the query engine with a response mode. You can select the response mode based on your specific needs. For detailed guidance, refer to the [LlamaIndex documentation on response modes](https://docs.llamaindex.ai/en/v0.10.19/module_guides/deploying/query_engine/response_modes.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8130646a-d736-4a49-8905-ed943f7fc07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query your data\n",
    "query_engine = vector_index.as_query_engine(response_mode=\"refine\", similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda38be-f9d8-40dc-af6c-c1905ccee060",
   "metadata": {},
   "source": [
    "## Customize Query Prompts\n",
    "\n",
    "Define task-specific prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc952e78-acc1-4434-ad96-9084bcc4cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating Prompt for Q&A\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = (\n",
    "    \"You are proudct expert of car and very faimilay with car user manual and provide guide to the end user.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the information from multiple sources and not prior knowledge\\n\"\n",
    "    \"answer the question according to the index dataset.\\n\"\n",
    "    \"if the question is not releate with ROCm and Radeon GPU, just say it is not releated with my knowledge base.\\n\"\n",
    "    \"if you don't know the answer, just say that I don't know.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"if the question is in chinese, please transclate chinese to english in advance\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_template}\n",
    ")\n",
    "\n",
    "template = (\n",
    "    \"The original query is as follows: {query_str}.\\n\"\n",
    "    \"We have provided an existing answer: {existing_answer}.\\n\"\n",
    "    \"We have the opportunity to refine the existing answer (only if needed) with some more context below.\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\n\"\n",
    "    \"if the question is 'who are you' , just say I am expert of AMD ROCm.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"Refined Answer: \"\n",
    ")\n",
    "\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:refine_template\": qa_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fcfe8d",
   "metadata": {},
   "source": [
    "## Query Examples\n",
    "\n",
    "Run the following queries:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59269115",
   "metadata": {},
   "source": [
    "- Query 1: Brief the steps to install the ROCm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0453dbca-e301-488e-ac55-22cc0548eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Brief the steps to install the ROCm?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5f0ede",
   "metadata": {},
   "source": [
    "- Query 2: Which chapter is about installing PyTorch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328c8fa-5430-4630-a783-70fd1f428a6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Which chapter is about installing PyTorch?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb66b0",
   "metadata": {},
   "source": [
    "- Query 3: How to verify PyTorch Installation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e04428-0aaf-438b-8776-88319b329fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"How to verify PyTorch Installation?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b84dee7",
   "metadata": {},
   "source": [
    "- Query 4: Could run ONNX on Radeon GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78e2bc-6d40-4ef0-b036-91d8c17e3571",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Could run ONNX on Radeon GPU?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial demonstrates how to construct a RAG pipeline using LlamaIndex and Ollama on AMD Radeon GPUs with ROCm. For further details, refer to the respective documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
