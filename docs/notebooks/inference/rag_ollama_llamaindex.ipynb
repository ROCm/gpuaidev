{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e91244a-3b94-471a-a5de-fc8d931df16a",
   "metadata": {},
   "source": [
    "# Constructing a RAG system using LlamaIndex and Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa90a7b-a662-4e20-adcb-b233d1afc709",
   "metadata": {},
   "source": [
    "AMD Radeonâ„¢ GPUs are officially supported by [ROCm](https://rocm.docs.amd.com/en/latest/index.html), ensuring compatibility with industry-standard software frameworks. This Jupyter notebook leverages [Ollama](https://ollama.com/) and [LlamaIndex](https://docs.llamaindex.ai/en/stable/), powered by ROCm, to build a Retrieval-Augmented Generation (RAG) application. LlamaIndex facilitates the creation of a pipeline from reading PDFs to indexing datasets and building a query engine, while Ollama provides the backend service for large language model (LLM) inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup:\n",
    "\n",
    "### Hardware\n",
    "\n",
    "* **AMD Radeon GPUs**: Ensure you are using an AMD Radeon GPU that supports ROCm. This tutorial was tested on the AMD Radeon PRO W7900.\n",
    "\n",
    "### Software\n",
    "\n",
    "* **ROCm 6.2**: Install ROCm by following the [Radeon GPU install guide](https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-radeon.html).\n",
    "* **Python 3.8**: Ensure Python is installed and accessible in your environment.\n",
    "\n",
    "### Environment\n",
    "\n",
    "* Root or `sudo` access is required to install and configure the software."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c02ae1c",
   "metadata": {},
   "source": [
    "\n",
    "## Install and launch Jupyter Notebooks\n",
    "\n",
    "If Jupyter is not already installed on your system, install it and launch JupyterLab using the following commands:\n",
    "\n",
    "``` bash\n",
    "pip install jupyter\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`.\n",
    "\n",
    "After the command executes, the terminal output displays a URL and token. Copy and paste this URL into your web browser on the host machine to access JupyterLab. After launching JupyterLab, upload this notebook to the environment and continue to follow the steps in this tutorial.\n",
    "\n",
    "## Install Ollama\n",
    "\n",
    "Ollama provides seamless support for AMD ROCm GPUs, offering optimized performance without further configuration. To install Ollama on Linux, use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8c7f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e9454-54a8-45f4-95dd-94f5206c6199",
   "metadata": {},
   "source": [
    "Start Ollama and verify it's running:\n",
    "\n",
    "**Note**: The Ollama installation guide is available [here](https://github.com/ollama/ollama/blob/main/docs/linux.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3580a83-1e19-4f7e-b19e-8c58e6f899c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo systemctl start ollama\n",
    "!sudo systemctl status ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752c18f8-c078-49b1-ab7d-d301e9e9a67d",
   "metadata": {},
   "source": [
    "## Download the models\n",
    "\n",
    "Use Ollama to pull the required models for RAG:\n",
    "\n",
    "**Important**: If the Ollama server is running as a foreground process from the previous step, you must run the rest of this notebook in a new instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ae8c42-1503-4048-a674-beeac5ad1b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull nomic-embed-text\n",
    "!ollama pull llama3.1:8b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aed7f87",
   "metadata": {},
   "source": [
    "Verify the downloaded models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5793f235-a868-4e75-a89c-38b3908ac565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME           ID              SIZE      MODIFIED     \n",
      "llama3.1:8b    42182419e950    4.7 GB    2 months ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list llama3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bbcae7-cd5d-4f15-9498-d50aa31c5558",
   "metadata": {},
   "source": [
    "See the [Ollama documentation](https://github.com/ollama/ollama) for more details.\n",
    "\n",
    "**Note**: Alternative models are available to use [here](https://ollama.com/search)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ffe628-6f6b-4aa1-a2cf-a7381d66c176",
   "metadata": {},
   "source": [
    "## Install PyTorch (optional) \n",
    "\n",
    "PyTorch is optional for this tutorial. This section uses PyTorch utilities for verification purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0184d5c1-ada2-46e0-87e6-2438c186fd85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf44525-ab00-4b38-929b-aceaf02d38fb",
   "metadata": {},
   "source": [
    "Verify the list of installed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38fec6ac-1bdd-4bd6-90c8-371eede9e2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-triton-rocm                      3.1.0\n",
      "torch                                    2.5.1+rocm6.2\n",
      "torchaudio                               2.5.1+rocm6.2\n",
      "torchvision                              0.20.1+rocm6.2\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe41347",
   "metadata": {},
   "source": [
    "Verify the GPU functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc3616ed-475d-49c5-b0ad-db8200e77491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "# Query GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    print('Using GPU:', torch.cuda.get_device_name(0))\n",
    "    print('GPU properties:', torch.cuda.get_device_properties(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f7191a-fdaa-4b83-86b1-75d1811a76a8",
   "metadata": {},
   "source": [
    "## Install LlamaIndex and dependencies\n",
    "\n",
    "Use the following command to install LlamaIndex and related packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec9028-1516-4355-8c79-70967753e22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index llama-index-llms-ollama llama-index-embeddings-ollama llama-index-vector-stores-chroma chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b471006",
   "metadata": {},
   "source": [
    "Verify the installations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6aed23c-a92c-4482-bab1-d467f15c148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama-index                              0.12.10\n",
      "llama-index-agent-openai                 0.4.1\n",
      "llama-index-cli                          0.4.0\n",
      "llama-index-core                         0.12.10.post1\n",
      "llama-index-embeddings-ollama            0.5.0\n",
      "llama-index-embeddings-openai            0.3.1\n",
      "llama-index-indices-managed-llama-cloud  0.6.3\n",
      "llama-index-llms-ollama                  0.5.0\n",
      "llama-index-llms-openai                  0.3.13\n",
      "llama-index-multi-modal-llms-openai      0.4.2\n",
      "llama-index-program-openai               0.3.1\n",
      "llama-index-question-gen-openai          0.3.0\n",
      "llama-index-readers-file                 0.4.3\n",
      "llama-index-readers-llama-parse          0.4.0\n",
      "llama-index-readers-web                  0.3.3\n",
      "llama-index-vector-stores-chroma         0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9913d2",
   "metadata": {},
   "source": [
    "## Build the RAG pipeline\n",
    "\n",
    "This section explains how to configure and build the RAG pipeline.\n",
    "\n",
    "### Set up indexing and the query engine\n",
    "\n",
    "Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e99b042-d0d5-4809-bc52-a5a3b0b1da21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa14454-f5cc-4b84-b979-6337a8622678",
   "metadata": {},
   "source": [
    "### Configure embedding and LLM models\n",
    "\n",
    "LlamaIndex implements the Ollama client interface to interact with the Ollama service. In this example, it requests both embedding and LLM services from Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1931a4cc-4f0d-42fd-a7b8-68227a9f91fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embedding model\n",
    "emb_fn=\"nomic-embed-text\"\n",
    "Settings.embed_model = OllamaEmbedding(model_name=emb_fn)\n",
    "\n",
    "# Set ollama model\n",
    "Settings.llm = Ollama(model=\"llama3.1:8b\", request_timeout=120.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe9b9d8-186e-4027-a87f-8fb5ece1c6d1",
   "metadata": {},
   "source": [
    "### Download data for RAG\n",
    "\n",
    "Download a PDF (for example, the ROCm Radeon documentation) and save it to the `./data` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca58b57",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir ./data && cd ./data && wget --recursive --level=1 --content-disposition --accept=pdf -np -nH --cut-dirs=6 https://rocm.docs.amd.com/_/downloads/radeon/en/latest/pdf/ && cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae962fa-ba30-47ea-95e6-443c81874641",
   "metadata": {},
   "source": [
    "The SimpleDirectoryReader is the most commonly used data connector.\n",
    "Provide it with an input directory or a list of files and it selects the best file reader based on the file extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5e0a9e5-b50d-4675-9419-b22ed11ee99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_dir=\"./data/\").load_data()\n",
    "\n",
    "# Check the content\n",
    "print(documents[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08adb51-a555-4b3d-80f1-a477cdfbefbf",
   "metadata": {},
   "source": [
    "### Create a vector dataset with Chroma\n",
    "\n",
    "[Chroma DB](https://www.trychroma.com/) is a database that stores and queries embeddings, documents, and metadata for LLM apps that integrates well with LlamaIndex. It creates the vector dataset by sourcing the PDF file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "741d2124-3812-4ba4-bc80-67411bf33d45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize client and save data\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db/rocm_db\")\n",
    "# create collection\n",
    "chroma_collection = db.get_or_create_collection(\"rocm_db\")\n",
    "\n",
    "# assign chroma as the vector_store to the context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f46bb50a-a21b-4078-a540-43c714567014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vector index per-document\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    transformations=[SentenceSplitter(chunk_size=512, chunk_overlap=20)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab337a76-3799-4b6d-8fb8-f833bb85cb37",
   "metadata": {},
   "source": [
    "### Create the query engine\n",
    "\n",
    "Next, create the query engine with a response mode. Select the response mode based on your specific needs. For detailed guidance, see the [LlamaIndex response modes documentation](https://docs.llamaindex.ai/en/v0.10.19/module_guides/deploying/query_engine/response_modes.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8130646a-d736-4a49-8905-ed943f7fc07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query your data\n",
    "query_engine = vector_index.as_query_engine(response_mode=\"refine\", similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda38be-f9d8-40dc-af6c-c1905ccee060",
   "metadata": {},
   "source": [
    "## Customize the query prompts\n",
    "\n",
    "Define task-specific prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc952e78-acc1-4434-ad96-9084bcc4cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating Prompt for Q&A\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = (\n",
    "    \"You are a car product expert who is very familiar with the car user manual and provides the guide to the end user.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the information from multiple sources and not prior knowledge\\n\"\n",
    "    \"answer the question according to the index dataset.\\n\"\n",
    "    \"if the question is not related to ROCm and Radeon GPU, just say it is not related to my knowledge base.\\n\"\n",
    "    \"if you don't know the answer, just say that I don't know.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"if the question is in Chinese, please translate Chinese to English in advance\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "qa_template = PromptTemplate(template)\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_template}\n",
    ")\n",
    "\n",
    "template = (\n",
    "    \"The original query is as follows: {query_str}.\\n\"\n",
    "    \"We have provided an existing answer: {existing_answer}.\\n\"\n",
    "    \"We have the opportunity to refine the existing answer (only if needed) with some more context below.\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"-------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\\n\"\n",
    "    \"if the question is 'who are you', just say I am an expert of AMD ROCm.\\n\"\n",
    "    \"Answers need to be precise and concise.\\n\"\n",
    "    \"Refined Answer: \"\n",
    ")\n",
    "\n",
    "qa_template = PromptTemplate(template)\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:refine_template\": qa_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fcfe8d",
   "metadata": {},
   "source": [
    "## Query examples\n",
    "\n",
    "Run the following queries:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59269115",
   "metadata": {},
   "source": [
    "- Query 1: Briefly describe the steps to install ROCm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0453dbca-e301-488e-ac55-22cc0548eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Briefly describe the steps to install ROCm?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5f0ede",
   "metadata": {},
   "source": [
    "- Query 2: Which chapter is about installing PyTorch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328c8fa-5430-4630-a783-70fd1f428a6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Which chapter is about installing PyTorch?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb66b0",
   "metadata": {},
   "source": [
    "- Query 3: How to verify a PyTorch installation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e04428-0aaf-438b-8776-88319b329fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"How to verify a PyTorch installation?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b84dee7",
   "metadata": {},
   "source": [
    "- Query 4: Could ONNX run on a Radeon GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78e2bc-6d40-4ef0-b036-91d8c17e3571",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Could ONNX run on a Radeon GPU?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial demonstrates how to construct a RAG pipeline using LlamaIndex and Ollama on AMD Radeon GPUs with ROCm. For further details, see the documentation for the different components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
