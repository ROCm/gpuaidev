{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bdc7a27",
   "metadata": {},
   "source": [
    "# LLM Inference with AMD Radeon™ and Instinct™ GPUs\n",
    "In this tutorial, we’ll explore how to leverage Hugging Face Transformers, Text Generation Inference (TGI), and vLLM to serve and test LLMs on AMD hardware. You’ll learn how to install and configure ROCm for AMD Radeon™ and Instinct™ GPUs, set environment variables for multi-GPU setups (e.g., HIP_VISIBLE_DEVICES), and launch your favorite models with reduced-precision (like FP16 or BF16) to balance performance and memory usage. We’ll also walk through best practices for containerizing your workflow with Docker, ensuring a reproducible setup for model deployment. By following these steps, you’ll be able to fine-tune, serve, and query advanced LLMs in a ROCm-accelerated environment, capitalizing on AMD GPU performance for state-of-the-art natural language processing tasks.\n",
    "\n",
    "## Prerequisites\n",
    "### 1. Hardware Requirements\n",
    "* AMD ROCm GPUs (e.g., MI210, MI300X, 7900-xt)\n",
    "* Ensure your system meets the System Requirements, including ROCm 6.0+ and Ubuntu 22.04\n",
    "\n",
    "### 2. Software\n",
    "* **ROCm installed** and verified on your system\n",
    "* **Docker installed** on your system. Refer to the [Docker installation guide](https://docs.docker.com/get-docker/) if needed\n",
    "\n",
    "### 3. System Configuration\n",
    "* **NUMA auto-balancing** disabled for optimal performance\n",
    "```bash\n",
    "(shell)sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'\n",
    "(shell)cat /proc/sys/kernel/numa_balancing\n",
    "--> output should be 0\n",
    "```\n",
    "* **ROCm** environment validated using rocm-smi\n",
    "```bash\n",
    "(shell)rocm-smi\n",
    "--> user should see all the available GPUs as shown in below table(MI300x case):\n",
    "\n",
    "============================================ ROCm System Management Interface ============================================\n",
    "====================================================== Concise Info ======================================================\n",
    "Device  Node  IDs              Temp        Power     Partitions          SCLK    MCLK    Fan  Perf  PwrCap  VRAM%  GPU%  \n",
    "              (DID,     GUID)  (Junction)  (Socket)  (Mem, Compute, ID)                                                  \n",
    "==========================================================================================================================\n",
    "0       2     0x74a1,   28851  41.0°C      133.0W    NPS1, SPX, 0        132Mhz  900Mhz  0%   auto  750.0W  0%     0%    \n",
    "1       3     0x74a1,   51499  37.0°C      133.0W    NPS1, SPX, 0        132Mhz  900Mhz  0%   auto  750.0W  0%     0%    \n",
    "2       4     0x74a1,   57603  38.0°C      136.0W    NPS1, SPX, 0        132Mhz  900Mhz  0%   auto  750.0W  0%     0%    \n",
    "3       5     0x74a1,   22683  34.0°C      133.0W    NPS1, SPX, 0        132Mhz  900Mhz  0%   auto  750.0W  0%     0%    \n",
    "4       6     0x74a1,   53458  38.0°C      133.0W    NPS1, SPX, 0        132Mhz  900Mhz  0%   auto  750.0W  0%     0%    \n",
    "5       7     0x74a1,   26954  35.0°C      132.0W    NPS1, SPX, 0        132Mhz  900Mhz  0%   auto  750.0W  0%     0%    \n",
    "6       8     0x74a1,   16738  39.0°C      134.0W    NPS1, SPX, 0        132Mhz  900Mhz  0%   auto  750.0W  0%     0%    \n",
    "7       9     0x74a1,   63738  37.0°C      131.0W    NPS1, SPX, 0        132Mhz  900Mhz  0%   auto  750.0W  0%     0%    \n",
    "==========================================================================================================================\n",
    "================================================== End of ROCm SMI Log ===================================================\n",
    "```\n",
    "### 3.Hugging Face API Access\n",
    "* Obtain an API token from Hugging Face for downloading models\n",
    "* Ensure you have a Hugging Face API token with the necessary permissions and approval to access [Meta's LLaMA checkpoints](https://huggingface.co/meta-llama)\n",
    "```bash\n",
    "access token starting with hf_xxxxxx\n",
    "need Hugging Face API token for rest of the tutorial (make sure to save the token on a seperate txt file)\n",
    "```\n",
    "\n",
    "## Inference on Hugging Face Transformers\n",
    "Hugging Face Transformers is a popular open-source library that provides an easy-to-use interface for working with state-of-the-art language models, such as BERT, GPT, and Llama variants. These models can be fine-tuned or used off-the-shelf for tasks like text generation, question answering, and sentiment analysis.   \n",
    "\n",
    "In this tutorial, we’ll demonstrate how to run inference on Hugging Face Transformers models using AMD Radeon™ and Instinct™ GPUs. We will cover configuring ROCm for GPU support, installing the necessary libraries, and running a LLM(meta-llama/Meta-Llama-3.1-8B-Instruct) in a containerized environment. \n",
    "\n",
    "### Prepare Inference Environment\n",
    "#### Step 1: Launch the Docker Container\n",
    "Run the following command in your terminal to pull the prebuilt Docker image containing all necessary dependencies and launch the Docker container with proper configuration:\n",
    "```bash\n",
    "(shell)docker run -it --rm --device=/dev/kfd --device=/dev/dri --group-add video --shm-size 1G --security-opt seccomp=unconfined --security-opt apparmor=unconfined -v $(pwd):/workspace --env HUGGINGFACE_HUB_CACHE=/workspace rocm/pytorch:latest\n",
    "--> if docker is launched it will look like root@xxx:\n",
    "```\n",
    "#### Step 2: Install Necessary Libraries\n",
    "This step sets up the essential dependencies needed for working with large language models and running your workflows in a notebook interface:\n",
    "```bash\n",
    "(docker)pip install accelerate transformers jupyter\n",
    "```\n",
    "#### Step 3: Provide Your Hugging Face API Token\n",
    "Hugging Face Token can be generated by signing into your account at **[Hugging Face Tokens](https://huggingface.co/settings/tokens)**\n",
    "Ensure the token has the necessary permissions for your tasks (e.g., \\\"read\\\" or \\\"write\\\"). Tokens typically start with \\\"hf_\\\".\n",
    "```bash\n",
    "(docker)cd && cd /workspace\n",
    "(docker)export HF_TOKEN=\"Your hugging face token to access gated models\" \n",
    "ex. (docker)export HF_TOKEN=hf_xxxx\n",
    "```\n",
    "**Note:**Mounts the current host directory ($(pwd)) to **/workspace** in the container, allowing files to be shared between the host and the container.\n",
    "\n",
    "### Run LLM Inference using Hugging Face Transformers \n",
    "Inside the docker container, run the following command inside the container to launch jupyter notebook server:\n",
    "```bash\n",
    "(docker)jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root\n",
    "```\n",
    "**Note:**Save the token or URL provided in the terminal output to access the notebook from your host machine.\n",
    "Now move on to the jupyter notebook and execute the following python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5061090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "query = \"Explain the concept of AI to me.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert in the field of AI. Make sure to provide an explanation in few sentences.\"},\n",
    "    {\"role\": \"user\", \"content\": query},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=512,\n",
    "    top_p = 0.7,     \n",
    "    temperature=0.2,               \n",
    ")\n",
    "\n",
    "response = outputs[0][\"generated_text\"][-1]['content']\n",
    "print('-------------------------------')\n",
    "print('Query:\\n', query)\n",
    "print('-------------------------------')\n",
    "print('Response:\\n', response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c7a67",
   "metadata": {},
   "source": [
    "## Inference on Hugging Face TGI\n",
    "Hugging Face Text Generation Inference (TGI) is a high-performance, low-latency solution for serving advanced language models in production. It streamlines the process of text generation, enabling developers to deploy and scale language models for tasks like summarization, conversational AI, and content creation.\n",
    "\n",
    "In this tutorial, we’ll demonstrate how to configure and run TGI using AMD Radeon™ and Instinct™ GPUs, leveraging the ROCm software stack for accelerated performance. You’ll learn how to set up your environment, containerize your workflow, and test your inference server by sending customized queries. \n",
    "\n",
    "### Prepare Inference Environment\n",
    "#### Step 1: Launch the Docker Container\n",
    "Run the following command in your terminal to pull the prebuilt Docker image containing all necessary dependencies and launch the Docker container with proper configuration:\n",
    "```bash\n",
    "(shell)docker run -it --rm --device=/dev/kfd --device=/dev/dri --group-add video --shm-size 1G --security-opt seccomp=unconfined --security-opt apparmor=unconfined -v $(pwd):/workspace --env HUGGINGFACE_HUB_CACHE=/workspace --ipc=host --net host --entrypoint /bin/bash ghcr.io/huggingface/text-generation-inference:latest-rocm\n",
    "--> if docker is launched it will look like root@xxx:\n",
    "```\n",
    "#### Step 2: Provide Your Hugging Face API Token\n",
    "Hugging Face Token can be generated by signing into your account at **[Hugging Face Tokens](https://huggingface.co/settings/tokens)**\n",
    "Ensure the token has the necessary permissions for your tasks (e.g., \\\"read\\\" or \\\"write\\\"). Tokens typically start with \\\"hf_\\\".\n",
    "```bash\n",
    "(docker)cd && cd /workspace\n",
    "(docker)export HF_TOKEN=\"Your hugging face token to access gated models\" \n",
    "ex. (docker)export HF_TOKEN=hf_xxxx\n",
    "```\n",
    "**Note:**Mounts the current host directory ($(pwd)) to **/workspace** in the container, allowing files to be shared between the host and the container.\n",
    "\n",
    "### Deploying LLM using Hugging Face TGI \n",
    "Start deploying LLM(meta-llama/Llama-3.1-8B-Instruct) using Hugging Face TGI:\n",
    "```bash\n",
    "(docker)HIP_VISIBLE_DEVICES=4 text-generation-launcher --model-id  meta-llama/Llama-3.1-8B-Instruct --num-shard 1 --cuda-graphs 1 --max-batch-prefill-tokens 131072 --max-batch-total-tokens 139264 --dtype float16 --port 8000 --trust-remote-code\n",
    "--> after successful connection it will show: INFO text_generation_router::server: router/src/server.rs:2402: Connected\n",
    "```\n",
    "**Note:** In a multi-GPU environment, it is recommended to set **HIP_VISIBLE_DEVICES=x** to deploy the LLM on the user’s preferred GPU.\n",
    "\n",
    "The LLM running in the Docker container acts as a server. To test it, open a new shell and send a query to the server.\n",
    "```bash\n",
    "(shell) curl -X POST http://localhost:8000/generate \\\n",
    "     -H \"Content-Type: application/json\" \\\n",
    "     -d '{\n",
    "       \"inputs\": \"System: You are an expert in the field of AI. Make sure to provide an explanation in few sentences.\\nUser: Explain the concept of AI to me.\\nAssistant:\",\n",
    "       \"parameters\": {\n",
    "         \"max_new_tokens\": 128,\n",
    "         \"do_sample\": false\n",
    "       }\n",
    "     }'\n",
    "```\n",
    "**Note:** Remember to match the docker --port **8000** and http://localhost:**8000**. If the port is used by other application you can modify the number. \n",
    "\n",
    "If the connection is successful the output will be:\n",
    "```bash\n",
    "(shell){\"generated_text\":\" AI, or Artificial Intelligence, refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, problem-solving, ...}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352b8b6",
   "metadata": {},
   "source": [
    "## Inference on Hugging Face vLLM\n",
    "Hugging Face vLLM is an open-source library designed to deliver high throughput and low latency for large language model inference. It optimizes text generation workloads by efficiently batching requests and making full use of GPU resources, enabling developers to handle demanding tasks such as summarization, code generation, and conversational AI at scale.\n",
    "\n",
    "In this tutorial, we’ll guide you through setting up and running vLLM on AMD Radeon™ and Instinct™ GPUs using the ROCm software stack. You’ll learn how to configure your environment, containerize your workflow, and send test queries to the inference server supported by vLLM.  \n",
    "\n",
    "### Prepare Inference Environment\n",
    "#### Step 1: Launch the Docker Container\n",
    "Run the following command in your terminal to pull the prebuilt Docker image containing all necessary dependencies and launch the Docker container with proper configuration:\n",
    "```bash\n",
    "(shell)docker run -it --rm --device=/dev/kfd --device=/dev/dri --group-add video --shm-size 1G --security-opt seccomp=unconfined --security-opt apparmor=unconfined -v $(pwd):/workspace --env HUGGINGFACE_HUB_CACHE=/workspace --ipc=host --net host --entrypoint /bin/bash rocm/vllm:rocm6.2_mi300_ubuntu20.04_py3.9_vllm_0.6.4\n",
    "--> if docker is launched it will look like root@xxx:\n",
    "```\n",
    "#### Step 2: Provide Your Hugging Face API Token\n",
    "Hugging Face Token can be generated by signing into your account at **[Hugging Face Tokens](https://huggingface.co/settings/tokens)**\n",
    "Ensure the token has the necessary permissions for your tasks (e.g., \\\"read\\\" or \\\"write\\\"). Tokens typically start with \\\"hf_\\\".\n",
    "```bash\n",
    "(docker)cd && cd /workspace\n",
    "(docker)export HF_TOKEN=\"Your hugging face token to access gated models\" \n",
    "ex. (docker)export HF_TOKEN=hf_xxxx\n",
    "```\n",
    "**Note:**Mounts the current host directory ($(pwd)) to **/workspace** in the container, allowing files to be shared between the host and the container.\n",
    "\n",
    "### Deploying LLM using Hugging Face vLLM \n",
    "Start deploying LLM(meta-llama/Llama-3.1-8B-Instruct) using Hugging Face vLLM:\n",
    "```bash\n",
    "(docker)HIP_VISIBLE_DEVICES=2 python -m vllm.entrypoints.openai.api_server \\\n",
    "        --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "        --gpu-memory-utilization 0.9 \\\n",
    "        --swap-space 16 \\\n",
    "        --disable-log-requests \\\n",
    "        --dtype float16 \\\n",
    "        --max-model-len 131072 \\\n",
    "        --tensor-parallel-size 1 \\\n",
    "        --host 0.0.0.0 \\\n",
    "        --port 3000 \\\n",
    "        --num-scheduler-steps 10 \\\n",
    "        --enable-chunked-prefill False \\\n",
    "        --max-num-seqs 128 \\\n",
    "        --max-num-batched-tokens 131072 \\\n",
    "        --max-model-len 131072 \\\n",
    "        --distributed-executor-backend \"mp\"\n",
    "--> after successful connection it will show: INFO: Uvicorn running on socket ('0.0.0.0', 88) (Press CTRL+C to quit) \n",
    "```\n",
    "**Note:** In a multi-GPU environment, it is recommended to set **HIP_VISIBLE_DEVICES=x** to deploy the LLM on the user’s preferred GPU.\n",
    "\n",
    "The LLM running in the Docker container acts as a server. To test it, open a new shell and send a query to the server.\n",
    "```bash\n",
    "(shell) curl localhost:3000/v1/chat/completions \\\n",
    "    -X POST \\\n",
    "    -d '{\n",
    "  \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are an expert in the field of AI. Make sure to provide an explanation in few sentences.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Explain the concept of AI.\"\n",
    "    }\n",
    "  ],\n",
    "  \"stream\": false,\n",
    "  \"max_tokens\": 128  \n",
    "\n",
    "}' \\\n",
    "    -H 'Content-Type: application/json'\n",
    "```\n",
    "**Note:** Remember to match the docker --port **3000** and http://localhost:**3000**. If the port is used by other application you can modify the number. \n",
    "\n",
    "If the connection is successful the output will be:\n",
    "```bash\n",
    "(shell){\"id\":\"chat-xx\",\"object\":\"chat.completion\",\"created\":1736494622,\"model\":\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Artificial Intelligence (AI) is a field of computer science ...}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4e6149",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "By following this tutorial, you’ve learned how to set up your environment for AMD Radeon™ or Instinct™ GPUs using ROCm, install and configure Hugging Face Transformers, TGI, and vLLM, and serve large language models for high-performance inference. You also discovered how to containerize your workflow and test the deployment by sending text-generation queries to the inference server. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
