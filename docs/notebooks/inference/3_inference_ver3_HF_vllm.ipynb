{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on vLLM\n",
    "vLLM is an open-source library designed to deliver high throughput and low latency for large language model inference. It optimizes text generation workloads by efficiently batching requests and making full use of GPU resources, empowering developers to manage complex tasks like code generation and large-scale conversational AI.\n",
    "\n",
    "In this tutorial, we’ll guide you through setting up and running vLLM on AMD Radeon™ and Instinct™ GPUs using the ROCm software stack. You’ll learn how to configure your environment, containerize your workflow, and send test queries to the inference server supported by vLLM.  \n",
    "\n",
    "### Prepare Inference Environment\n",
    "#### 1. Launch the Docker Container\n",
    "Run the following command in your terminal to pull the prebuilt Docker image containing all necessary dependencies and launch the Docker container with proper configuration:\n",
    "```bash\n",
    "(shell)docker run -it --rm --device=/dev/kfd --device=/dev/dri --group-add video --shm-size 1G --security-opt seccomp=unconfined --security-opt apparmor=unconfined -v $(pwd):/workspace --env HUGGINGFACE_HUB_CACHE=/workspace --ipc=host --net host --entrypoint /bin/bash rocm/vllm:rocm6.2_mi300_ubuntu20.04_py3.9_vllm_0.6.4\n",
    "--> if docker is launched it will look like root@xxx:\n",
    "```\n",
    "```bash\n",
    "(docker)cd && cd /workspace\n",
    "```\n",
    "**Note:**Mounts the current host directory ($(pwd)) to **/workspace** in the container, allowing files to be shared between the host and the container.\n",
    "\n",
    "### 2. Install and Launch Jupyter\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "```bash\n",
    "\n",
    "(docker)pip install --upgrade pip setuptools wheel\n",
    "(docker)pip install jupyter\n",
    "(docker)jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root\n",
    "```\n",
    "**Note:**Save the token or URL provided in the terminal output to access the notebook from your host machine.\n",
    "\n",
    "### 3. Provide Your Hugging Face Token\n",
    "You will need a Hugging Face API token to access meta-llama/Llama-3.1-8B-Instruct. Tokens typically start with \"hf_\". Generate your token at Hugging Face Tokens and request access for [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct).\n",
    "\n",
    "Run the following interactive block in your Jupyter notebook to set up the token:\n",
    "**Note:** Please uncheck the \"Add token as Git credential?\" option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "# Prompt the user to log in\n",
    "status = notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that your token was captured correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Validate the token\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying LLM using vLLM \n",
    "Start deploying LLM(meta-llama/Llama-3.1-8B-Instruct) using vLLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!HIP_VISIBLE_DEVICES=2 python -m vllm.entrypoints.openai.api_server \\\n",
    "        --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "        --gpu-memory-utilization 0.9 \\\n",
    "        --swap-space 16 \\\n",
    "        --disable-log-requests \\\n",
    "        --dtype float16 \\\n",
    "        --max-model-len 131072 \\\n",
    "        --tensor-parallel-size 1 \\\n",
    "        --host 0.0.0.0 \\\n",
    "        --port 3000 \\\n",
    "        --num-scheduler-steps 10 \\\n",
    "        --enable-chunked-prefill False \\\n",
    "        --max-num-seqs 128 \\\n",
    "        --max-num-batched-tokens 131072 \\\n",
    "        --max-model-len 131072 \\\n",
    "        --distributed-executor-backend \"mp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> after successful connection it will show: INFO: Uvicorn running on socket ('0.0.0.0', 88) (Press CTRL+C to quit) \n",
    "**Note:** In a multi-GPU environment, it is recommended to set **HIP_VISIBLE_DEVICES=x** to deploy the LLM on the user’s preferred GPU.\n",
    "\n",
    "The LLM running in the Docker container acts as a server. To test it, open a new notebook and send a query to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:3000/v1/chat/completions\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\n",
    "    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an expert in the field of AI. Make sure to provide an explanation in few sentences.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the concept of AI.\"\n",
    "        }\n",
    "    ],\n",
    "    \"stream\": False,\n",
    "    \"max_tokens\": 128\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Remember to match the docker --port **3000** and http://localhost:**3000**. If the port is used by other application you can modify the number. \n",
    "\n",
    "If the connection is successful the output will be:\n",
    "```bash\n",
    "{\"id\":\"chat-xx\",\"object\":\"chat.completion\",\"created\":1736494622,\"model\":\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Artificial Intelligence (AI) is a field of computer science ...}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
