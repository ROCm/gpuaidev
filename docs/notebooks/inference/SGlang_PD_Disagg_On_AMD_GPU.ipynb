{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ed5955",
   "metadata": {},
   "source": [
    "# LLM Distributed Inference/PD Disaggregation On AMD Instinct GPU\n",
    "\n",
    "With the rapid growth of LLM model size, single-node inference optimization starts to show its limitations on LLM serving scaling, and distributed inference on multiple nodes has been more and more important for efficient LLM serving. Prefill and Decode (PD) Disaggregation is a typical use case of LLM distributed inference on GPU nodes. LLM inference comprises two distinct phases: Prefill and Decode. The Prefill phase is computation-intensive, processing the entire input sequence, while the Decode phase is memory-intensive, managing the Key-Value (KV) cache for token generation. PD disaggregation will run these two phases independently on different GPU nodes, which has key benefits of efficient GPU resource allocation and independent performance tuning. In this tutorial, we will demonstrate how to set up 1P1D distributed inference on a single MI300 node or two MI300x GPU nodes. \n",
    "\n",
    "# Prerequisites\n",
    "This tutorial was developed and tested using the following setup.\n",
    "\n",
    "## Operating system\n",
    "Ubuntu 22.04/24.04: Ensure your system is running Ubuntu version 22.04/24.04.\n",
    "\n",
    "## Hardware\n",
    "AMD GPUs: This tutorial was tested on both a single AMD Instinct MI300X node and two AMD Instinct MI300X GPU nodes(each node has 8 MI300x GPUs, and RDMA NIC device is also a must for better performance).You can choose the test case according to the number of your GPU nodes. Ensure you are using an AMD Instinct GPU or compatible hardware with ROCm support and that your system meets [the official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "## Software\n",
    "ROCm 6.3 or later version: Install and verify ROCm by following [the ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html). After installation, confirm your setup using rocm-smi command. AMD and LLM Opensource community have also provided the pre-built ROCm docker images, for example, [rocm SGlang image](https://hub.docker.com/r/lmsysorg/sglang/tags), [rocm ubuntu22.04 image](https://hub.docker.com/r/rocm/dev-ubuntu-22.04) and [rocm ubuntu24.04 image](https://hub.docker.com/r/rocm/dev-ubuntu-24.04). Developers can use these pre-built docker images to reduce the efforts of setting up ROCm environment.\n",
    "\n",
    "### Hugging Face API access\n",
    "\n",
    "* Obtain an API token from [Hugging Face](https://huggingface.co) for downloading models.\n",
    "* Ensure the Hugging Face API token has the necessary permissions and approval to access:\n",
    "- For 1 node you need to have acces to [Meta Llama Llama-3.1-8B-Instruct checkpoints](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct).\n",
    "- For 2 nodes you need to have acces to [Meta Llama Llama-3.3-70B-Instruct checkpoints](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct).\n",
    "\n",
    "## Set up PD Disaggregation environment \n",
    "In this tutorial, we will work on the pre-built ROCm SGLang image, which has integrated SGlang with AMD ROCm software stack successfully. Developers can also try other ROCm as the base image if needed.  \n",
    "\n",
    "### Step1: Prepare the tutorial environment\n",
    "Follow these steps to configure your tutorial environment:\n",
    "\n",
    "### 1. Pull the Docker image\n",
    "\n",
    "We use the lmsysorg/sglang:v0.4.9-rocm630 docker image as the base image, since it is the latest version when we run PD test on MI300x nodes for this tutorial. SGLang community will continue to release more ROCm SGlang docker images, and developers are strongly advised to try the latest SGlang docker for the better performance.\n",
    "\n",
    "``` bash\n",
    "docker pull lmsysorg/sglang:v0.4.9-rocm630\n",
    "```\n",
    "\n",
    "### 2. Launch the Docker container\n",
    "\n",
    "In order to have the good network transfer performance, RDMA NIC is required for GPU nodes to run PD disaggregation, so when we launch the docker images, we need to map the RDMA device into the docker container, as shown in the below command.\n",
    "\n",
    "``` bash\n",
    "docker run -it --rm \\\n",
    "  --network=host \\\n",
    "  --device=/dev/kfd \\\n",
    "  --device=/dev/dri \\\n",
    "  --group-add=video \\\n",
    "  --ipc=host \\\n",
    "  --device=/dev/infiniband \\\n",
    "  --device=/dev/infiniband/rdma_cm \\\n",
    "  --privileged \\\n",
    "  --cap-add=SYS_ADMIN \\\n",
    "  --cap-add=SYS_PTRACE \\\n",
    "  --security-opt seccomp=unconfined \\\n",
    "  --shm-size 32G \\\n",
    "  -v $(pwd):/workspace \\\n",
    "  -w /workspace/notebooks \\\n",
    "  lmsysorg/sglang:v0.4.9-rocm630\n",
    "```\n",
    "\n",
    "**Note**: This command mounts the current directory to the `/workspace` directory in the container. Ensure the notebook file is either copied to this directory before running the Docker command or uploaded into the Jupyter Notebook environment after it starts. Save the token or URL provided in the terminal output to access the notebook from your web browser. You can download this notebook from the [AI Developer Hub GitHub repository](https://github.com/ROCm/gpuaidev).\n",
    "\n",
    "### 3. Install and launch Jupyter\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "``` bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Start the Jupyter server:\n",
    "\n",
    "``` bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e063c030",
   "metadata": {},
   "source": [
    "<div style=\"border-left:6px solid #d32f2f;background:#fdecea;color:#5f2120;padding:12px 16px;border-radius:6px;\">\n",
    "<strong>⚠️ NOTE:</strong> The rest of this notebook is designed to run as jupyter notebook. This notebook demonstrates Prefill/Decode (PD) disaggregation on AMD Instinct GPUs. It runs fully on a single node (intra‑node 1P1D) by default. Two‑node (inter‑node 1P1D) steps are optional and clearly marked.\n",
    "</div>\n",
    "\n",
    "**Run modes**\n",
    "- Single node (default): No etcd required. Use “Intra‑Node 1P1D” section.\n",
    "- Two nodes (optional): Requires etcd + RDMA + SSH setup. Use “Two‑Node (Inter‑Node) 1P1D” section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f624c5d8",
   "metadata": {},
   "source": [
    "### Provide your Hugging Face token\n",
    "\n",
    "You'll require a Hugging Face API token to access Llama models with appropoiate permissions as indicated in earlier sections of this notebook. Let's first install Huggingface Hub library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9215daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a72e2be",
   "metadata": {},
   "source": [
    "\n",
    "Run the following interactive block in your Jupyter notebook to set up the token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21a9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "# Prompt the user to log in\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e442d58",
   "metadata": {},
   "source": [
    "\n",
    "Verify that your token was accepted correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468fd921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the token\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d9946",
   "metadata": {},
   "source": [
    "### Step2 : Install the necessary software components \n",
    "For intra-node 1P1D, you need to install mooncake transfer engine, etcd is not required for this case. For inter-node 1P1D, you need to install both etcd and mooncake transfer engine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d26c7f",
   "metadata": {},
   "source": [
    "#### 2.1. [optional]Install etcd\n",
    "You can skip this step if running the single node test.etcd is a strongly consistent, distributed key-value store that provides a reliable way to store data that needs to be accessed by a distributed system or cluster of machines. In the design of SGLang PD disaggregation solution, etcd server is required to run on each GPU node as the cluster metadata storage. So we also need to install etcd.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /sgl-workspace\n",
    "apt update && apt install -y wget\n",
    "wget https://github.com/etcd-io/etcd/releases/download/v3.6.0-rc.5/etcd-v3.6.0-rc.5-linux-amd64.tar.gz -O /tmp/etcd.tar.gz\n",
    "tar -xvf /tmp/etcd.tar.gz -C /usr/local/bin/ --strip-components=1 && rm /tmp/etcd.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c51ee4",
   "metadata": {},
   "source": [
    "#### 2.2. Install Mooncake \n",
    "Mooncake features a KVCache-centric disaggregated architecture that separates the prefill and decoding clusters. Its core components, like transfer engine, has been integrated into SGLang PD disaggregation solution to transfer KV cache between nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd6094",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "apt update && apt install -y zip unzip openssh-server\n",
    "apt -y install gcc make libtool autoconf  librdmacm-dev rdmacm-utils infiniband-diags ibverbs-utils perftest ethtool  libibverbs-dev rdma-core strace\n",
    "cd /sgl-workspace\n",
    "pip install mooncake-transfer-engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c3edf",
   "metadata": {},
   "source": [
    "### Step 3: [optional]Install RDMA related libraries and do network transfer configurations  \n",
    "You can skip the network configuration step for the single node 1P1D test. But SGlang/Mooncake transfer engine still needs to enable RDMA NIC device even if this is a intra-node test.You can run ibv_devices command to test RDMA device status. if RDMA device can't be found, you still need to install RDMA NIC driver to enable them as the step 3.1.\n",
    "\n",
    "#### 3.1. [optional]Install NIC RDMA driver \n",
    "In this tutorial, we take two MI300x GPU nodes as an example, which have been equipped with Thor2/BCM-57608 NIC devices. If you use the different NIC devices, you need to download the corresponding NIC driver packages from device vendor, and install it according to the steps of official NIC user guide. The below steps are only for Thor2/BCM-57608 device.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo -e \"\\n\\n============Installing required pkgs============\\n\\n\"\n",
    "apt -y install libelf-dev\n",
    "\n",
    "cd bcm5760x_230.2.52.0a/drivers_linux/bnxt_rocelib\n",
    "tar xvf libbnxt_re-230.2.52.0.tar.gz\n",
    "cd libbnxt_re-230.2.52.0\n",
    "echo -e \"\\n\\n============Compiling RoCE Lib now============\\n\\n\"\n",
    "sh autogen.sh\n",
    "./configure\n",
    "make\n",
    "find /usr/lib64/  /usr/lib -name \"libbnxt_re-rdmav*.so\"  -exec mv {} {}.inbox \\;\n",
    "make install all\n",
    "sh -c \"echo /usr/local/lib >> /etc/ld.so.conf\"\n",
    "ldconfig\n",
    "cp -f bnxt_re.driver /etc/libibverbs.d/\n",
    "find . -name \"*.so\" -exec md5sum {} \\;\n",
    "BUILT_MD5SUM=$(find . -name \"libbnxt_re-rdmav*.so\" -exec md5sum {} \\; |  cut -d \" \" -f 1)\n",
    "echo -e \"\\n\\nmd5sum of the built libbnxt_re is $BUILT_MD5SUM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59044d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ibv_devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c7f6ed",
   "metadata": {},
   "source": [
    "After running the above steps, you can list the RDMA device through ibv_devices command and check the RDMA device information through ibv_devinfo command. On our test device, ibv_devices command can give the below output, which means all RDMA devices can be enumerated successfully and work well now.\n",
    "\n",
    "    device                 node GUID\n",
    "    ------              ----------------\n",
    "    rdma0               d604e6fffe0e9fb4\n",
    "    rdma1               d604e6fffe0e9938\n",
    "    xeth0               d604e6fffee921d0\n",
    "    rdma2               d604e6fffe780000\n",
    "    rdma3               d604e6fffe0e9d34\n",
    "    rdma4               d604e6fffe780370\n",
    "    rdma5               d604e6fffe0e8678\n",
    "    rdma6               d604e6fffe0e8718\n",
    "    rdma7               d604e6fffe7801f4\n",
    "\n",
    "#### 3.2. Build and install ROCm-Aware UCX library\n",
    "\n",
    "The [Unified Communication Framework](https://github.com/openucx/ucx) (UCX), is an open source, cross-platform framework designed to provide a common set of communication interfaces for various network programming models and interfaces. UCX uses ROCm technologies to implement various network operation primitives. UCX is the standard communication library for InfiniBand and RDMA over Converged Ethernet (RoCE) network interconnect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/openucx/ucx.git -b v1.18.1 \n",
    "cd ucx \n",
    "./autogen.sh\n",
    "./configure --with-rocm=/opt/rocm --enable-mt --prefix=/opt/ucx \n",
    "make -j \n",
    "make install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4e8bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] = os.environ['PATH'] + ':/opt/ucx/bin'\n",
    "os.environ['LD_LIBRARY_PATH'] = os.environ['LD_LIBRARY_PATH'] + ':/opt/ucx/lib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d24f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ucx_info -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bfb1db",
   "metadata": {},
   "source": [
    "after installing UCX, you can use ucx_info command to check whether your UCX library was built with ROCm support. On our MI300x machine, \"ucx_info -v\" outputs the below information.\n",
    "\n",
    " Library version: 1.20.0\n",
    " Library path: /opt/ucx/lib/libucs.so.0\n",
    " API headers version: 1.20.0\n",
    " Git branch 'v1.18.1', revision 6022e2a\n",
    " Configured with: --with-rocm=/opt/rocm --enable-mt --prefix=/opt/ucx\n",
    "\n",
    "\n",
    "#### 3.3. Build and install ROCm-Aware OpenMPI library\n",
    "[Open MPI](https://www.open-mpi.org/) is a Message Passing Interface implementation used as a communication protocol for parallel and distributed computers. ROCm-aware support \n",
    "means that the MPI library can send and receive data from AMD GPU device buffers directly. As of today,ROCm support is available through UCX. While other communication transports might work as well, UCX is the only transport formally supported in Open MPI head of development for ROCm devices. So we need to enable UCX when building OpenMPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8563b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone --recursive https://github.com/open-mpi/ompi.git -b v5.0.x\n",
    "cd ompi \n",
    "./autogen.pl\n",
    "./configure --prefix=/opt/ompi --with-rocm=/opt/rocm --with-ucx=/opt/ucx\n",
    "make -j 8\n",
    "make install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee697916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] = os.environ['PATH'] + ':/opt/ompi/bin:/opt/ucx/bin'\n",
    "os.environ['LD_LIBRARY_PATH'] = os.environ['LD_LIBRARY_PATH'] + ':/opt/ompi/lib:/opt/ucx/lib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ompi_info | grep \"extensions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3a1da",
   "metadata": {},
   "source": [
    "after installing OpenMPI, you can use ompi_info command to check whether your OpenMPI library was built with ROCm support. On the tested MI300x machine, \"ompi_info | grep \"extensions\"\" outputs the below information.\n",
    "  MPI extensions: affinity, cuda, ftmpi, rocm\n",
    "\n",
    "#### 3.4. [optional]SSH Passwordless Login Configuration\n",
    "When running Open MPI applications in a cluster, SSH is typically used to launch commands on remote nodes to set up the distributed inference. SSH Passwordless login, without entering a password or passphrase, is also required to be configured for all remote nodes. The below steps are required to run on each node of GPU cluster. In this tutorial, we just take two GPU nodes as the example.\n",
    "\n",
    "First, use ssh-keygen to generate a key pair consisting of a public key and a private key on each node, id_rsa contains the private key and id_rsa.pub contains the public key\n",
    "\n",
    "Second, copy the content of local public key to the authorized_keys file in remote node, assuming the authorized_keys file path is ~/.ssh/authorized_keys \n",
    "\n",
    "Third, disable password authentication on each node. Most servers allow both username/password authentication and SSH key authentication, but if you want to allow only SSH key authentication, then you can disable the use of usernames and passwords. You need to uncomment below contents in /etc/ssh/sshd_config : 'PermitRootLogin prohibit-password' and 'PubkeyAuthentication yes'. \n",
    "\n",
    "The default SSH Port is 22, which may be occupied by other SSH applications in the cluster. So we had better change the default SSH port inside current docker container, which is only for PD disaggregation application. This step is also accomplished through the sshd_config file, adding \"port \"self-defined port number\"\" in this file.\n",
    "We can also add the above self-defined port into SSH configuration, to override the default port setting for remote SSH connection, which is done by adding the below content into ~/.ssh/config file.\n",
    "  Host Remote_node_IP\n",
    "      Port  \"self-defined port number\"\n",
    "\n",
    "After running the above steps, you still need to run the below commands to have the settings work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419a0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "chmod 600 ~/.ssh/authorized_keys \n",
    "chmod 600 ~/.ssh/config \n",
    "service ssh restart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ea2ce",
   "metadata": {},
   "source": [
    "#### 3.5. [optional] Run RCCL-Test Benchmark to test RDMA settings \n",
    "[RCCL-Tests](https://github.com/ROCm/rccl-tests) is an open source tool provided by AMD to test the bandwidth and latency between GPUs/Nodes through performing the collective operations benchmark of ROCm Collective Communications Library (RCCL), such as all_reduce, all_gather, etc. In this tutorial, we use this benchmark tool to check whether RDMA devices have been enabled for LLM distributed inference. \n",
    "\n",
    "First, build RCCL-Tests benchmark. To compile RCCL tests with MPI support, you need to set MPI=1 and set MPI_HOME to the path where MPI is installed. If HIP is not installed in /opt/rocm, you may specify HIP_HOME. Similarly, if RCCL (librccl.so) is not installed in /opt/rocm/lib/, you may specify NCCL_HOME and CUSTOM_RCCL_LIB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab9153",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "make MPI=1 MPI_HOME=/path/to/mpi HIP_HOME=/path/to/hip NCCL_HOME=/path/to/rccl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db0432a",
   "metadata": {},
   "source": [
    "Second, specify the GPU node in Hostfile. The hostfile is a text file that contains the IP address of hosts/nodes, the number of available GPU slots on each host/node.\n",
    "\n",
    "Third, run the RCCL benchmark tool. If the above configurations have been setup well, you will find that the bandwidth of RDMA device will be far higher than normal Ethernet device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1f12ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "TORCH_NCCL_HIGH_PRIORITY=1 RCCL_MSCCL_ENABLE=0 mpirun -np <Total GPU number> --map-by ppr:<GPU number>:node --hostfile <mpi_hosts> --allow-run-as-root --mca pml ucx --mca btl ^openib  -x NCCL_SOCKET_IFNAME=<IP interfaces for communication> -x NCCL_DEBUG=INFO -x NCCL_IB_HCA=<rdma device> -x NCCL_IB_GID_INDEX=3  /home/rccl-tests/build/all_reduce_perf -b 1k -e 2G -f 2 -g 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e4b7a4",
   "metadata": {},
   "source": [
    "## Run SGLang PD Disaggregation\n",
    "SGLang has supported prefill-decode (PD) disaggregation on AMD Instinct GPUs, which is through mooncake to transfer KV cache. From the view of system architecture, SGLang PD disaggregation comprises 3 distinct components: proxy server, prefill server and decode server. When a request comes in, the proxy server will select a pair of prefill and decode servers based on workload balancing scheme. The selected Prefill Server and decode Server will pair via a handshake, establishing a local sender and receiver, respectively. The Decode Server pre-allocates the KV cache, signaling the Prefill Server to begin LLM prefill inference and compute the KV caches. Once prefill is done, the KV Cache data transfers to the Decode Server, which handles iterative token generation.\n",
    "\n",
    "In this tutorial, we will test SGLang PD Disaggregation in two cases: Intra-node 1P1D and Inter-node 1P1D. For Intra-node, you need at least two GPUs, one GPU run prefill server and the other run decode server. For inter-node 1P1D, you need two MI300x nodes. One node will run prefill server, and the other node will run decode server. Since proxy server doesn't need high GPU resource, we put it on the prefill node. If you have a larger cluster, proxy node can run on a standalone node to have the better performance. Assuming that IP address of prefill node is 10.21.9.10, decode node is 10.21.9.15, we list the steps of SGLang PD Disaggregation as an example. Developer can modify the related parameters and settings according to your cluster info. \n",
    "\n",
    "### Intra-Node 1P1D\n",
    "\n",
    "#### Run Prefill server\n",
    "At this step, we used sglang.launch_server command to launch prefill server. The detailed description of this command's options can be found from SGLang document or source codes. Developers refer the latest version document once the options have been changed with the upgrade of SGLang framework. RDMA device names can be found through ibv_devices of previous steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea35dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "HIP_VISIBLE_DEVICES=0 python -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct \\\n",
    "         --disaggregation-mode prefill --port 30000 \\\n",
    "         --disaggregation-ib-device rdma0,rdma1,rdma2,rdma3,rdma4,rdma5,rdma6,rdma7 2>&1 | tee /workspace/prefill.log >/dev/null &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd85b4d",
   "metadata": {},
   "source": [
    "#### Run Decode server\n",
    "At this step, we used sglang.launch_server command to launch decode server. The detailed description of this command's options can be found from SGLang document or source codes. Developers refer the latest version document once the options have been changed with the upgrade of SGLang framework. RDMA device names can be found through ibv_devices of previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "HIP_VISIBLE_DEVICES=1 python -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct \\\n",
    "        --disaggregation-mode decode --port 30001 \\\n",
    "        --disaggregation-ib-device rdma0,rdma1,rdma2,rdma3,rdma4,rdma5,rdma6,rdma7 2>&1 | tee /workspace/decode.log >/dev/null &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ffb23",
   "metadata": {},
   "source": [
    "#### Run Proxy server \n",
    "At this step, prefill and decode server ports will be configured when launching proxy server at the same node. Proxy server port will be also provided for test client program to connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f98384",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python -m sglang.srt.disaggregation.mini_lb --prefill http://127.0.0.1:30000 --decode http://127.0.0.1:30001 --host 0.0.0.0 --port 40000 2>&1 | tee /workspace/proxy.log >/dev/null &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4408f507",
   "metadata": {},
   "source": [
    "### Inter-Node 1P1D\n",
    "#### Run ETCD server on each node\n",
    "\n",
    "Assuming that both of prefill and decode nodes have done all the previous settings in this tutorial, we need to run below commands in the SGLang ROCm containers of each node.\n",
    "\n",
    "On prefill node, run etcd server is the below command. The below etcd server ports are just for reference, if you find they have been used by other processes, please try other ports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad9f809",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "etcd --name infra0 --data-dir /var/lib/etcd --initial-advertise-peer-urls http://10.21.9.10:2380 \\\n",
    "  --listen-peer-urls http://10.21.9.10:2380 \\\n",
    "  --listen-client-urls http://10.21.9.10:2379,http://127.0.0.1:2379 \\\n",
    "  --advertise-client-urls http://10.21.9.10:2379 \\\n",
    "  --initial-cluster-token etcd-cluster-1 \\\n",
    "  --initial-cluster infra0=http://10.21.9.10:2380,infra1=http://10.21.9.15:2380 \\\n",
    "  --initial-cluster-state new \\\n",
    "  2>&1 | tee /workspace/etcd_infra0.log >/dev/null &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9298d8",
   "metadata": {},
   "source": [
    "On decode node, run etcd server is the below command.The below etcd server ports are just for reference, if you find they have been used by other processes, please try other ports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55240762",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "etcd --name infra1 --data-dir /var/lib/etcd --initial-advertise-peer-urls http://10.21.9.15:2380 \\\n",
    "  --listen-peer-urls http://10.21.9.15:2380 \\\n",
    "  --listen-client-urls http://10.21.9.15:2379,http://127.0.0.1:2379 \\\n",
    "  --advertise-client-urls http://10.21.9.15:2379 \\\n",
    "  --initial-cluster-token etcd-cluster-1 \\\n",
    "  --initial-cluster infra0=http://10.21.9.10:2380,infra1=http://10.21.9.15:2380 \\\n",
    "  --initial-cluster-state new \\\n",
    "  2>&1 | tee /workspace/etcd_infra1.log >/dev/null &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c529af",
   "metadata": {},
   "source": [
    "#### Run Proxy server \n",
    "As mentioned before, this server will run on prefill node in this tutorial. You can also put it on a standalone node in the cluster for better performance. \n",
    "\n",
    "At this step, IP address/port of prefill and decode node pools will be configured, IP address/port of proxy server will be also provided for test client program to connect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4818ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nohup python -m sglang.srt.disaggregation.mini_lb --prefill http://10.21.9.10:30000 \\\n",
    "                        --decode http://10.21.9.15:30000 --host 0.0.0.0 --port 40000 \\\n",
    "                        2>&1 | tee /workspace/proxy.log >/dev/null & "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4b5b9",
   "metadata": {},
   "source": [
    "#### Run Prefill server\n",
    "At this step, we used sglang.launch_server command to launch prefill server. The detailed description of this command's options can be found from SGLang document or source codes. Developers refer the latest version document once the options have been changed with the upgrade of SGLang framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db9d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "python3 -m sglang.launch_server --model meta-llama/Llama-3.3-70B-Instruct \\\n",
    "                        --disaggregation-mode prefill --disaggregation-ib-device rdma0,rdma1,rdma2,rdma3,rdma4,rdma5,rdma6,rdma7 \\\n",
    "                        --host 10.21.9.10 --port 30000  --trust-remote-code  \\\n",
    "                        --tp 8  --disable-radix-cache --disable-cuda-graph \\\n",
    "                        --max-running-requests 1024 --stream-output \\\n",
    "                        --dist-init-addr 10.21.9.10:5757 --nnodes 1 --node-rank 0 \\\n",
    "                        --mem-fraction-static 0.8 2>&1 | tee /workspace/prefill.log >/dev/null &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f24cfd0",
   "metadata": {},
   "source": [
    "#### Run Decode server\n",
    "At this step, we used sglang.launch_server command to launch decode server. The detailed description of this command's options can be found from SGLang document or source codes. Developers refer the latest version document once the options have been changed with the upgrade of SGLang framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "python3 -m sglang.launch_server --model meta-llama/Llama-3.3-70B-Instruct \\\n",
    "                        --disaggregation-mode decode --disaggregation-ib-device rdma0,rdma1,rdma2,rdma3,rdma4,rdma5,rdma6,rdma7 \\\n",
    "                        --host 10.21.9.15 --port 30000 --trust-remote-code \\\n",
    "                        --tp 8 --disable-radix-cache --disable-cuda-graph \\\n",
    "                        --max-running-requests 1024 --stream-output \\\n",
    "                        --dist-init-addr 10.21.9.15:5757 --nnodes 1 --node-rank 0 \\\n",
    "                        --mem-fraction-static 0.8 2>&1 | tee /workspace/decode.log >/dev/null &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56dcf60",
   "metadata": {},
   "source": [
    "### Test PD Disaggregation \n",
    "\n",
    "At this step, we used sglang.bench_serving to test 1P1D like normal SGLang benchmark test. In this tutorial, we also run it on prefill node to simplify the demo. If you need to run it on other machine which can connect this cluster, you need to set the host IP address/port of proxy server in this command. Other test parameters can be changed as your need.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46817ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 -m sglang.bench_serving --backend sglang --host 127.0.0.1 --port 40000 --dataset-name generated-shared-prefix \\\n",
    "           --gsp-system-prompt-len 0 \\\n",
    "           --gsp-question-len 1024 \\\n",
    "           --gsp-output-len 1024 \\\n",
    "           --gsp-num-groups 1 \\\n",
    "           --gsp-prompts-per-group 16\\\n",
    "           --random-range-ratio 1 \\\n",
    "           --max-concurrency 16 \\\n",
    "           --pd-separated \\\n",
    "           2>&1 | tee test.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439e0dc4",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffb3d86",
   "metadata": {},
   "source": [
    "### xPyD setup\n",
    "If you have a larger GPU cluster to run PD disaggregation, you can run xPyD (multiple prefill and decode instances) to have a better performance. xPyD setup are the same with the above steps, just needing to modify some multi-node related configurations: 1) change the prefill and decode configuration in proxy server, like --prefill \"http://YOUR_FIRST_PREFILL_NODE_IP:30000\" --decode \"http://YOUR_FIRST_DECODE_NODE_IP:30000\"  2) change the multi-node distributed serving options, like dist-init-addr, nnodes and node-rank, when launching prefill and decode server 3) change the tp/dp/ep-size options of SGLang serving program if needed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f798046",
   "metadata": {},
   "source": [
    "## Summary \n",
    "Through this tutorial, developer has already know how to set up and run SGLang PD disaggregation on AMD MI300 GPUs. Although we demonstrate 1P1D on both a single MI300x node and 2 MI300x GPU nodes, developer can implement xPyD by the steps easily on their own GPU cluster. If developer would like to study more about PD disaggregation, [Mooncake](https://kvcache-ai.github.io/Mooncake/), [LLM-d](https://llm-d.ai/docs/architecture/architecture) and [vLLM disagg_prefill](https://docs.vllm.ai/en/stable/features/disagg_prefill.html#development) can be very useful. LLM distributed inference, especially PD disaggregation, is still under development. We hope that this tutorial will encourage you to tune, test, and contribute to LLM distributed inference on AMD GPUs, and help us shape the future of AI acceleration.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comfyui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
