{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ed5955",
   "metadata": {},
   "source": [
    "# LLM distributed inference and PD disaggregation on AMD Instinct GPUs\n",
    "\n",
    "With the rapid growth of LLM model sizes, single-node inference optimization starts to show its limitations on LLM serving scaling. Distributed inference on multiple nodes becomes more important for efficient LLM serving. Prefill and Decode (PD) disaggregation is a typical use case for LLM distributed inference on GPU nodes. LLM inference comprises two distinct phases: prefill and decode. The prefill phase is computationally intensive, processing the entire input sequence, while the decode phase is memory-intensive, managing the key-value (KV) cache for token generation. PD disaggregation runs these two phases independently on different GPU nodes, which provides the benefits of efficient GPU resource allocation and independent performance tuning. This tutorial demonstrates how to set up 1P1D distributed inference on either one or two nodes with AMD Instinct™ GPUs. \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup.\n",
    "\n",
    "### Operating system\n",
    "\n",
    "* **Ubuntu 22.04/24.04**:  Ensure your system is running Ubuntu 22.04 or 24.04.\n",
    "\n",
    "### Hardware\n",
    "\n",
    "- **AMD Instinct GPUs**: Tested on MI300X. Works on a single MI300X node or two MI300X nodes (each node has 8 MI300X GPUs).\n",
    "- **RDMA NIC** (required for single-node and two-node for transferring the KV cache over RDMA):\n",
    "  - Use an RDMA‑capable NIC (for example, Broadcom Thor2/BCM‑57608 or NVIDIA/Mellanox ConnectX).\n",
    "  - Install the appropriate vendor driver and RDMA userspace (`rdma-core/libibverbs`). Verify with the following commands:\n",
    "    - `ibv_devices` and `ibv_devinfo`\n",
    "    - `ls /dev/infiniband`\n",
    "  - If running in Docker, ensure `/dev/infiniband` is mapped into the container (see the launch command below).\n",
    "  - For a two-node configuraton, RDMA must be available on both nodes (RoCEv2 with PFC or InfiniBand) and properly cabled and switch configured.\n",
    "- **ROCm compatibility**: Use AMD Instinct GPUs with ROCm support and ensure your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "### Software\n",
    "\n",
    "- **ROCm 6.3 or later** (host): Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html). After installation, confirm your setup using the `rocm-smi` command.\n",
    "- **Docker** (host): Install the Docker Engine on Linux and verify with the `docker --version` command. You will run this tutorial inside a container. Ensure the host allows you to map GPU and RDMA devices.\n",
    "  - When launching the container, map the devices and use the recommended flags:\n",
    "    - Devices: `/dev/kfd`, `/dev/dri`, `/dev/infiniband`, and `/dev/infiniband/rdma_cm`\n",
    "    - Flags: `--network=host --ipc=host --shm-size 32G --group-add=video`\n",
    "- **Prebuilt ROCm Docker images** (recommended to reduce the setup effort):\n",
    "  - [SGLang ROCm image](https://hub.docker.com/r/lmsysorg/sglang/tags)\n",
    "  - [ROCm Ubuntu 22.04](https://hub.docker.com/r/rocm/dev-ubuntu-22.04)\n",
    "  - [ROCm Ubuntu 24.04](https://hub.docker.com/r/rocm/dev-ubuntu-24.04)\n",
    "- **Jupyter (in container)**: Install with `pip install jupyter` to run this notebook.\n",
    "\n",
    "### Hugging Face API access\n",
    "\n",
    "* Obtain an API token from [Hugging Face](https://huggingface.co) for downloading models.\n",
    "* Ensure the Hugging Face API token has the necessary permissions and approvals to access the required checkpoints:\n",
    "  - For one node, you must have access to the [Meta Llama Llama-3.1-8B-Instruct checkpoints](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct).\n",
    "  - For two nodes, you must have access to the [Meta Llama Llama-3.3-70B-Instruct checkpoints](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct).\n",
    "\n",
    "## Set up the PD disaggregation environment \n",
    "\n",
    "In this tutorial, you will work on the prebuilt ROCm SGLang image, which integrates SGLang with the AMD [ROCm](https://rocm.docs.amd.com/en/latest/index.html) software stack. You can also try other ROCm images as the base image if you want.  \n",
    "\n",
    "### Step 1: Prepare the tutorial environment\n",
    "\n",
    "Follow these steps to configure your tutorial environment:\n",
    "\n",
    "#### Pull the Docker image\n",
    "\n",
    "Use the `lmsysorg/sglang:v0.4.9-rocm630` Docker image as the base image. This is the latest image that was tested for this tutorial.\n",
    "\n",
    "**Note**: The SGLang community continues to release additional ROCm SGLang Docker images. You are strongly encouraged to try the latest available image for better performance.\n",
    "\n",
    "``` bash\n",
    "docker pull lmsysorg/sglang:v0.4.9-rocm630\n",
    "```\n",
    "\n",
    "#### Launch the Docker container\n",
    "\n",
    "To achieve good network transfer performance, an RDMA NIC is required for the nodes running PD disaggregation. When launching the Docker images, map the RDMA device into the Docker container, as shown in the command below.\n",
    "\n",
    "``` bash\n",
    "docker run -it --rm \\\n",
    "  --network=host \\\n",
    "  --device=/dev/kfd \\\n",
    "  --device=/dev/dri \\\n",
    "  --group-add=video \\\n",
    "  --ipc=host \\\n",
    "  --device=/dev/infiniband \\\n",
    "  --device=/dev/infiniband/rdma_cm \\\n",
    "  --privileged \\\n",
    "  --cap-add=SYS_ADMIN \\\n",
    "  --cap-add=SYS_PTRACE \\\n",
    "  --security-opt seccomp=unconfined \\\n",
    "  --shm-size 32G \\\n",
    "  -v $(pwd):/workspace \\\n",
    "  -w /workspace \\\n",
    "  lmsysorg/sglang:v0.4.9-rocm630\n",
    "```\n",
    "\n",
    "**Note**: This command mounts the current directory to the `/workspace` directory in the container. Ensure the notebook file is either copied to this directory before running the Docker command or uploaded into the Jupyter Notebook environment after it starts. Save the token or URL provided in the terminal output to access the notebook from your web browser. You can download this notebook from the [AI Developer Hub GitHub repository](https://github.com/ROCm/gpuaidev).\n",
    "\n",
    "#### Install and launch Jupyter\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "``` bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Start the Jupyter server:\n",
    "\n",
    "``` bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e063c030",
   "metadata": {},
   "source": [
    "**Note**: The rest of this notebook is designed to run as a Jupyter notebook. This notebook demonstrates Prefill and Decode (PD) disaggregation on AMD Instinct GPUs. It runs on a single node (Intra‑node 1P1D) by default. The two‑node (Inter‑node 1P1D) steps are optional and clearly marked.\n",
    "\n",
    "**Run modes**\n",
    "\n",
    "* Single node (default): The [etcd](https://etcd.io/) key-value store server is not required. Use the \"Intra‑node 1P1D\" section.\n",
    "* Two nodes (optional): Requires a setup with the etcd server, RDMA, and SSH. Use the two‑node \"Inter‑node 1P1D\" section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f624c5d8",
   "metadata": {},
   "source": [
    "#### Provide your Hugging Face token\n",
    "\n",
    "You'll require a Hugging Face API token to access Llama models with the appropriate permissions as indicated in earlier sections of this notebook. First, install the Hugging Face Hub library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9215daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a72e2be",
   "metadata": {},
   "source": [
    "\n",
    "Run the following interactive block in your Jupyter notebook to set up the token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21a9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "# Prompt the user to log in\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e442d58",
   "metadata": {},
   "source": [
    "\n",
    "Verify that your token was accepted correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468fd921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the token\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d9946",
   "metadata": {},
   "source": [
    "### Step 2: Install the necessary software components \n",
    "\n",
    "For the Intra-node 1P1D mode, you must install the Mooncake transfer engine. The etcd server is not required for this case. For the Inter-node 1P1D mode, you must install both etcd and the Mooncake transfer engine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d26c7f",
   "metadata": {},
   "source": [
    "#### Step 2.1: (Optional) Install etcd\n",
    "\n",
    "You can skip this step if you are running the single-node test. etcd is a strongly consistent, distributed key-value store that provides a reliable way to store data that needs to be accessed by a distributed system or cluster of machines. In the SGLang PD disaggregation solution design, the etcd server is required on each GPU node to provide cluster metadata storage. It must be installed to run this tutorial on two nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /sgl-workspace\n",
    "apt update && apt install -y wget\n",
    "wget https://github.com/etcd-io/etcd/releases/download/v3.6.0-rc.5/etcd-v3.6.0-rc.5-linux-amd64.tar.gz -O /tmp/etcd.tar.gz\n",
    "tar -xvf /tmp/etcd.tar.gz -C /usr/local/bin/ --strip-components=1 && rm /tmp/etcd.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c51ee4",
   "metadata": {},
   "source": [
    "#### Step 2.2: Install Mooncake\n",
    "\n",
    "Mooncake features a KV cache-centric disaggregated architecture that separates the prefill and decoding clusters. Its core components, such as the transfer engine, have been integrated into the SGLang PD disaggregation solution to transfer the KV cache between nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd6094",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "apt update && apt install -y zip unzip openssh-server\n",
    "apt -y install gcc make libtool autoconf  librdmacm-dev rdmacm-utils infiniband-diags ibverbs-utils perftest ethtool  libibverbs-dev rdma-core strace\n",
    "cd /sgl-workspace\n",
    "pip install mooncake-transfer-engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c3edf",
   "metadata": {},
   "source": [
    "### Step 3: Install RDMA-related libraries and add the network transfer configuration\n",
    "\n",
    "For the single-node 1P1D test, you can skip the network configuration in Step 3.4. However, the SGLang/Mooncake transfer engine is still required to enable the RDMA NIC, even for intra-node runs. Verify RDMA availability with `ibv_devices`. If no device is listed, install the appropriate RDMA NIC driver as described in Step 3.1.\n",
    "\n",
    "#### Step 3.1: Install the NIC RDMA driver \n",
    "\n",
    "These instructions were validated on Instinct MI300X systems equipped with Broadcom Thor2/BCM-57608 RDMA NICs (single-node and two-node). If you are using a different RDMA NIC, download and install the appropriate vendor driver according to the documentation. The commands below apply only to the Thor2/BCM-57608 NIC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819902dc",
   "metadata": {},
   "source": [
    "⚠️ **Note**: Driver-specific instructions: The commands below are curated for Broadcom Thor2/BCM‑57608 RDMA NICs on Instinct MI300X systems. If your system uses a different RDMA NIC, obtain and install the correct driver and userspace from your NIC vendor following the documentation before proceeding. You can find your NIC vendor by running `lspci | grep -i Eth`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo -e \"\\n\\n============Installing required pkgs============\\n\\n\"\n",
    "apt -y install libelf-dev\n",
    "\n",
    "cd bcm5760x_230.2.52.0a/drivers_linux/bnxt_rocelib\n",
    "tar xvf libbnxt_re-230.2.52.0.tar.gz\n",
    "cd libbnxt_re-230.2.52.0\n",
    "echo -e \"\\n\\n============Compiling RoCE Lib now============\\n\\n\"\n",
    "sh autogen.sh\n",
    "./configure\n",
    "make\n",
    "find /usr/lib64/  /usr/lib -name \"libbnxt_re-rdmav*.so\"  -exec mv {} {}.inbox \\;\n",
    "make install all\n",
    "sh -c \"echo /usr/local/lib >> /etc/ld.so.conf\"\n",
    "ldconfig\n",
    "cp -f bnxt_re.driver /etc/libibverbs.d/\n",
    "find . -name \"*.so\" -exec md5sum {} \\;\n",
    "BUILT_MD5SUM=$(find . -name \"libbnxt_re-rdmav*.so\" -exec md5sum {} \\; |  cut -d \" \" -f 1)\n",
    "echo -e \"\\n\\nmd5sum of the built libbnxt_re is $BUILT_MD5SUM\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a978f9",
   "metadata": {},
   "source": [
    "List the RDMA devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59044d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ibv_devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c7f6ed",
   "metadata": {},
   "source": [
    "After running the steps above, list the RDMA devices using the `ibv_devices` command and check the RDMA device information using the `ibv_devinfo` command. The output of the `ibv_devices` command should look similar to the following results, confirming that all RDMA devices can be enumerated and are operating successfully.\n",
    "\n",
    "    device                 node GUID\n",
    "    ------              ----------------\n",
    "    rdma0               d604e6fffe0e9fb4\n",
    "    rdma1               d604e6fffe0e9938\n",
    "    xeth0               d604e6fffee921d0\n",
    "    rdma2               d604e6fffe780000\n",
    "    rdma3               d604e6fffe0e9d34\n",
    "    rdma4               d604e6fffe780370\n",
    "    rdma5               d604e6fffe0e8678\n",
    "    rdma6               d604e6fffe0e8718\n",
    "    rdma7               d604e6fffe7801f4\n",
    "\n",
    "#### Step 3.2: Build and install the ROCm-aware UCX library\n",
    "\n",
    "The [Unified Communication Framework](https://github.com/openucx/ucx) (UCX) is an open-source, cross-platform framework designed to provide a common set of communication interfaces for various network programming models and interfaces. UCX uses ROCm technologies to implement various network operation primitives. It's the standard communication library for InfiniBand and RDMA over Converged Ethernet (RoCE) network interconnection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "apt-get install -y flex\n",
    "git clone https://github.com/openucx/ucx.git -b v1.18.1 \n",
    "cd ucx \n",
    "./autogen.sh\n",
    "./configure --with-rocm=/opt/rocm --enable-mt --prefix=/opt/ucx \n",
    "make -j \n",
    "make install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93b7eff",
   "metadata": {},
   "source": [
    "Set the UCX environment variables for the Jupyter session so that `ucx_info` and the UCX libraries are discoverable. These changes only affect the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4e8bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] = os.environ['PATH'] + ':/opt/ucx/bin'\n",
    "os.environ['LD_LIBRARY_PATH'] = os.environ['LD_LIBRARY_PATH'] + ':/opt/ucx/lib'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d89df76",
   "metadata": {},
   "source": [
    "Verify UCX ROCm support by running this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d24f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ucx_info -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bfb1db",
   "metadata": {},
   "source": [
    "After installing UCX, use the `ucx_info` command to check whether your UCX library was built with ROCm support. On the Instinct MI300X node under test, `ucx_info -v` shows the following information:\n",
    "\n",
    "    Library version: 1.20.0\n",
    "    Library path: /opt/ucx/lib/libucs.so.0\n",
    "    API headers version: 1.20.0\n",
    "    Git branch 'v1.18.1', revision 6022e2a\n",
    "    Configured with: --with-rocm=/opt/rocm --enable-mt --prefix=/opt/ucx\n",
    "\n",
    "\n",
    "#### Step 3.3: Build and install the ROCm-Aware Open MPI library\n",
    "\n",
    "[Open MPI](https://www.open-mpi.org/) is a Message Passing Interface (MPI) implementation used as a communication protocol for parallel and distributed computers. It features ROCm-aware support, which means the MPI library can send and receive data from the AMD GPU device buffers directly. ROCm support is available through UCX, so you must enable UCX when building Open MPI.\n",
    "\n",
    "**Note**: Other communication transports might also work, but UCX is the only transport mechanism formally supported in Open MPI for ROCm devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8563b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone --recursive https://github.com/open-mpi/ompi.git -b v5.0.x\n",
    "cd ompi \n",
    "./autogen.pl\n",
    "./configure --prefix=/opt/ompi --with-rocm=/opt/rocm --with-ucx=/opt/ucx\n",
    "make -j 8\n",
    "make install "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a2e9b",
   "metadata": {},
   "source": [
    "Set Open MPI and UCX environment variables for this Jupyter session so `mpirun`, `ompi_info`, `ucx_info`, and the shared libraries are discoverable. These changes affect only the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee697916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] = os.environ['PATH'] + ':/opt/ompi/bin:/opt/ucx/bin'\n",
    "os.environ['LD_LIBRARY_PATH'] = os.environ['LD_LIBRARY_PATH'] + ':/opt/ompi/lib:/opt/ucx/lib'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81188fba",
   "metadata": {},
   "source": [
    "Verify Open MPI ROCm support by running this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ompi_info | grep \"extensions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3a1da",
   "metadata": {},
   "source": [
    "After installing Open MPI, use the `ompi_info` command to verify whether your Open MPI library was built with ROCm support. When run on the Instinct MI300X device used for the test, `ompi_info | grep \"extensions\"` displays the following information:\n",
    "\n",
    "```\n",
    "MPI extensions: affinity, cuda, ftmpi, rocm\n",
    "```\n",
    "\n",
    "#### Step 3.4: (Optional) SSH passwordless login configuration\n",
    "\n",
    "When running Open MPI applications in a cluster, SSH is typically used to run commands on remote nodes to set up the distributed inference. SSH passwordless login, which lets you run commands without entering a password or passphrase, must be configured on all remote nodes to work properly. The following steps must be performed on each node of the GPU cluster. In this tutorial, the commands are run on two GPU nodes as an example.\n",
    "\n",
    "1. Use `ssh-keygen` to generate a key pair, consisting of a public key and a private key, on each node. `id_rsa` contains the private key and `id_rsa.pub` contains the public key.\n",
    "\n",
    "2. Copy the contents of the local public key to the `authorized_keys` file on the remote node. The file path for `authorized_keys` is typically `~/.ssh/authorized_keys`. \n",
    "\n",
    "3. Disable password authentication on each node. Most servers allow both username/password authentication and SSH key authentication. To allow only SSH key authentication, disable the use of usernames and passwords. To do so, uncomment the lines `PermitRootLogin prohibit-password` and `PubkeyAuthentication yes` in `/etc/ssh/sshd_config`. \n",
    "\n",
    "4. The default SSH Port is `22`, which might be occupied by other SSH applications in the cluster. To avoid conflicts, change the default SSH port inside the Docker container, which is only used for the PD disaggregation application. To change the default port, edit `/etc/ssh/sshd_config` and add `port \"self-defined port number\"` to the file, replacing \"self-defined port number\" with an unoccupied port number.\n",
    "\n",
    "   Additionally, add the same self-defined port number to the SSH configuration to override the default port setting for remote SSH connections. Add the following lines to the  `~/.ssh/config` file.\n",
    "\n",
    "   ```\n",
    "   Host Remote_node_IP\n",
    "      Port  \"self-defined port number\"\n",
    "   ```\n",
    "\n",
    "After completing the steps above, run the commands below to apply the settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419a0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "chmod 600 ~/.ssh/authorized_keys \n",
    "chmod 600 ~/.ssh/config \n",
    "service ssh restart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ea2ce",
   "metadata": {},
   "source": [
    "#### Step 3.5: (Optional) Run the RCCL-Test benchmark to test the RDMA settings \n",
    "\n",
    "[RCCL-Tests](https://github.com/ROCm/rccl-tests) is an open-source tool provided by AMD to test the bandwidth and latency between GPUs and nodes by performing the collective operations benchmarks from the ROCm Collective Communications Library (RCCL), such as all_reduce and all_gather. This tutorial uses this benchmarking tool to verify whether the RDMA devices have been enabled for LLM distributed inference. \n",
    "\n",
    "First, build the RCCL-Tests benchmark. To compile RCCL tests with MPI support, set `MPI=1` and set `MPI_HOME` to the path where MPI is installed. If HIP is not installed in `/opt/rocm`, specify the location in `HIP_HOME`. Similarly, if RCCL (`librccl.so`) is not installed in `/opt/rocm/lib/`, set `NCCL_HOME` and `CUSTOM_RCCL_LIB` to the actual location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab9153",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "make MPI=1 MPI_HOME=/path/to/mpi HIP_HOME=/path/to/hip NCCL_HOME=/path/to/rccl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db0432a",
   "metadata": {},
   "source": [
    "Next, specify the GPU node in the hostfile. The hostfile is a text file that contains the IP addresses of the hosts (nodes) and the number of available GPU slots on each node.\n",
    "\n",
    "Third, run the RCCL benchmark tool. If the node configuration is correct, the bandwidth for the RDMA device will be far higher than that of a normal Ethernet device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1f12ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "TORCH_NCCL_HIGH_PRIORITY=1 RCCL_MSCCL_ENABLE=0 mpirun -np <Total GPU number> --map-by ppr:<GPU number>:node --hostfile <mpi_hosts> --allow-run-as-root --mca pml ucx --mca btl ^openib  -x NCCL_SOCKET_IFNAME=<IP interfaces for communication> -x NCCL_DEBUG=INFO -x NCCL_IB_HCA=<rdma device> -x NCCL_IB_GID_INDEX=3  /home/rccl-tests/build/all_reduce_perf -b 1k -e 2G -f 2 -g 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e4b7a4",
   "metadata": {},
   "source": [
    "## Run SGLang PD disaggregation\n",
    "\n",
    "SGLang supports Prefill-Decode (PD) disaggregation on AMD Instinct GPUs, which uses Mooncake to transfer the KV cache. From a system architecture perspective, SGLang PD disaggregation includes 3 distinct components: a proxy server, prefill server, and decode server. When a request comes in, the proxy server selects a pair of prefill and decode servers based on a workload-balancing scheme. The selected prefill server and decode server pair using a handshake, establishing a local sender and receiver, respectively. The decode server preallocates the KV cache, then signals the prefill server to begin LLM prefill inference and compute the KV caches. After the prefill work is done, the KV cache data is transferred to the decode server, which handles iterative token generation.\n",
    "\n",
    "This tutorial tests SGLang PD Disaggregation for two configurations: Intra-node 1P1D and Inter-node 1P1D. For the Intra-node case, you need at least two GPUs: one GPU to run the prefill server and the other to run the decode server. For inter-node 1P1D, you need two nodes. One node will run the prefill server, and the other node will run the decode server. Because the proxy server doesn't require a large amount of GPU resources, it runs on the prefill node. If you have a larger cluster, the proxy node can run on a standalone node for better performance. For the following steps demonstrating SGLang PD Disaggregation, the example prefill node has the IP address `10.21.9.10` and the decode node has the address `10.21.9.15`. Modify the relevant parameters and settings according to your cluster configuration. \n",
    "\n",
    "### Intra-node (single-node) 1P1D\n",
    "\n",
    "For intra-node testing, follow these steps:\n",
    "\n",
    "**Note**: Run all commands in this section from a terminal, not from notebook code cells. In JupyterLab, open a terminal using **Launcher → Terminal** (or **File → New → Terminal**). Use separate terminals for the prefill, decode, and proxy servers.\n",
    "\n",
    "#### Run the prefill server\n",
    "\n",
    "Use the `sglang.launch_server` command to launch the prefill server. For more information and a detailed description of the command options, see the latest version of the SGLang documentation or source code. The RDMA device names can be found by using `ibv_devices` (see the earlier section)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254e4cfd",
   "metadata": {},
   "source": [
    "\n",
    "**Note**: For a multi-node configuration, Ensure `PATH` and `LD_LIBRARY_PATH` include UCX and Open MPI (see the earlier cells, or export them in the terminal). Replace any placeholders (for example, IPs, ports, and RDMA device names) before running the commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ea35dc",
   "metadata": {},
   "source": [
    "```bash\n",
    "HIP_VISIBLE_DEVICES=0 python -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct \\\n",
    "         --disaggregation-mode prefill --port 30000 \\\n",
    "         --disaggregation-ib-device rdma0,rdma1,rdma2,rdma3,rdma4,rdma5,rdma6,rdma7\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd85b4d",
   "metadata": {},
   "source": [
    "#### Run the decode server\n",
    "\n",
    "Use the `sglang.launch_server` command to launch the decode server. The RDMA device names can be found by using `ibv_devices`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b06b4",
   "metadata": {},
   "source": [
    "```bash\n",
    "HIP_VISIBLE_DEVICES=1 python -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct \\\n",
    "        --disaggregation-mode decode --port 30001 \\\n",
    "        --disaggregation-ib-device rdma0,rdma1,rdma2,rdma3,rdma4,rdma5,rdma6,rdma7\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ffb23",
   "metadata": {},
   "source": [
    "#### Run the proxy server\n",
    "\n",
    "This step configures the prefill and decode server ports when launching the proxy server on the same node. The proxy server port is also provided so the test client program can connect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f98384",
   "metadata": {},
   "source": [
    "```bash\n",
    "python -m sglang.srt.disaggregation.mini_lb --prefill http://127.0.0.1:30000 --decode http://127.0.0.1:30001 --host 0.0.0.0 --port 40000 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4408f507",
   "metadata": {},
   "source": [
    "### Inter-node (multi-node) 1P1D\n",
    "\n",
    "For inter-node testing, follow these steps:\n",
    "\n",
    "**Note**: Run all commands in this section from a terminal, not from notebook code cells. In JupyterLab, open a terminal using **Launcher → Terminal** (or **File → New → Terminal**). Use separate terminals for the prefill, decode, and proxy servers.\n",
    "\n",
    "#### Run the etcd server on each node\n",
    "\n",
    "Run the commands below in the SGLang ROCm containers of both the prefill and decode nodes. The etcd server ports in these commands are for reference only. If they are in use by other processes, try different ports.\n",
    "\n",
    "On the prefill node, start the etcd server using the following command.\n",
    "```bash\n",
    "etcd --name infra0 --data-dir /var/lib/etcd --initial-advertise-peer-urls http://10.21.9.10:2380 \\\n",
    "  --listen-peer-urls http://10.21.9.10:2380 \\\n",
    "  --listen-client-urls http://10.21.9.10:2379,http://127.0.0.1:2379 \\\n",
    "  --advertise-client-urls http://10.21.9.10:2379 \\\n",
    "  --initial-cluster-token etcd-cluster-1 \\\n",
    "  --initial-cluster infra0=http://10.21.9.10:2380,infra1=http://10.21.9.15:2380 \\\n",
    "  --initial-cluster-state new \n",
    "```\n",
    "On the decode node, use this command to run the etcd server.\n",
    "```bash\n",
    "etcd --name infra1 --data-dir /var/lib/etcd --initial-advertise-peer-urls http://10.21.9.15:2380 \\\n",
    "  --listen-peer-urls http://10.21.9.15:2380 \\\n",
    "  --listen-client-urls http://10.21.9.15:2379,http://127.0.0.1:2379 \\\n",
    "  --advertise-client-urls http://10.21.9.15:2379 \\\n",
    "  --initial-cluster-token etcd-cluster-1 \\\n",
    "  --initial-cluster infra0=http://10.21.9.10:2380,infra1=http://10.21.9.15:2380 \\\n",
    "  --initial-cluster-state new \n",
    "```\n",
    "#### Run the proxy server\n",
    "\n",
    "As previously mentioned, this server runs on the prefill node in this tutorial. You can run it on a standalone node in the same cluster for better performance. \n",
    "\n",
    "In this step, the IP addresses and ports of the prefill and decode node pools are configured. The IP address and port of the proxy server are also provided for the test client program to connect to. \n",
    "```bash\n",
    "python -m sglang.srt.disaggregation.mini_lb --prefill http://10.21.9.10:30000 \\\n",
    "                        --decode http://10.21.9.15:30000 --host 0.0.0.0 --port 40000\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4b5b9",
   "metadata": {},
   "source": [
    "#### Run the prefill server\n",
    "\n",
    "Use the `sglang.launch_server` command to launch the prefill server. For more information and a detailed description of the command options, see the latest version of the SGLang documentation or source code. The RDMA device names can be found by using `ibv_devices` (see the earlier section).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5db9d9c",
   "metadata": {},
   "source": [
    "```bash\n",
    "python3 -m sglang.launch_server --model meta-llama/Llama-3.3-70B-Instruct \\\n",
    "                        --disaggregation-mode prefill --disaggregation-ib-device rdma0,rdma1,rdma2,rdma3,rdma4,rdma5,rdma6,rdma7 \\\n",
    "                        --host 10.21.9.10 --port 30000  --trust-remote-code  \\\n",
    "                        --tp 8  --disable-radix-cache --disable-cuda-graph \\\n",
    "                        --max-running-requests 1024 --stream-output \\\n",
    "                        --dist-init-addr 10.21.9.10:5757 --nnodes 1 --node-rank 0 \\\n",
    "                        --mem-fraction-static 0.8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f24cfd0",
   "metadata": {},
   "source": [
    "#### Run the decode server\n",
    "\n",
    "Use the `sglang.launch_server` command to launch the decode server. The RDMA device names can be found by using `ibv_devices`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffd006c",
   "metadata": {},
   "source": [
    "```bash\n",
    "python3 -m sglang.launch_server --model meta-llama/Llama-3.3-70B-Instruct \\\n",
    "                        --disaggregation-mode decode --disaggregation-ib-device rdma0,rdma1,rdma2,rdma3,rdma4,rdma5,rdma6,rdma7 \\\n",
    "                        --host 10.21.9.15 --port 30000 --trust-remote-code \\\n",
    "                        --tp 8 --disable-radix-cache --disable-cuda-graph \\\n",
    "                        --max-running-requests 1024 --stream-output \\\n",
    "                        --dist-init-addr 10.21.9.15:5757 --nnodes 1 --node-rank 0 \\\n",
    "                        --mem-fraction-static 0.8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56dcf60",
   "metadata": {},
   "source": [
    "### Test PD disaggregation \n",
    "\n",
    "In this step, use `sglang.bench_serving` to test the 1P1D configuration in the same way as a normal SGLang benchmark test. This tutorial runs the command on the prefill node to simplify the demo. To run the command on another machine in the cluster, set the host IP address and port of the proxy server in this command. The other test parameters can be changed as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46817ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 -m sglang.bench_serving --backend sglang --host 127.0.0.1 --port 40000 --dataset-name generated-shared-prefix \\\n",
    "           --gsp-system-prompt-len 0 \\\n",
    "           --gsp-question-len 1024 \\\n",
    "           --gsp-output-len 1024 \\\n",
    "           --gsp-num-groups 1 \\\n",
    "           --gsp-prompts-per-group 16\\\n",
    "           --random-range-ratio 1 \\\n",
    "           --max-concurrency 16 \\\n",
    "           --pd-separated \\\n",
    "           2>&1 | tee test.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffb3d86",
   "metadata": {},
   "source": [
    "### xPyD setup\n",
    "\n",
    "If you have a larger GPU cluster for running PD disaggregation, you can use xPyD (multiple prefill and decode instances) for better performance. The xPyD setup is the same as the steps described above with the following modifications to the multi-node related configuration:\n",
    "\n",
    "* Change the prefill and decode node configuration in proxy server, for example, `--prefill \"http://YOUR_FIRST_PREFILL_NODE_IP:30000\"` and `--decode \"http://YOUR_FIRST_DECODE_NODE_IP:30000\"`.\n",
    "* Change the multi-node distributed serving options, such as `dist-init-addr`, `nnodes`, and `node-rank`, when launching the prefill and decode server.\n",
    "* Change the `tp`, `dp`, and `ep-size` options of the SGLang serving program, if required.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f798046",
   "metadata": {},
   "source": [
    "## Summary \n",
    "In this tutorial, you learned how to set up and run SGLang PD disaggregation on AMD Instinct MI300X GPUs. The tutorial demonstrated how to configure 1P1D on both a single Instinct MI300X node and two Instinct MI300X GPU nodes, but you can easily implement xPyD on your own GPU cluster. To learn more about PD disaggregation, see the [Mooncake](https://kvcache-ai.github.io/Mooncake/), [LLM-d](https://llm-d.ai/), and [vLLM disagg_prefill](https://docs.vllm.ai/en/stable/features/disagg_prefill.html#development) resources. This tutorial aims to encourage you to tune, test, and contribute to LLM distributed inference on AMD GPUs, helping to shape the future of AI acceleration.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comfyui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
