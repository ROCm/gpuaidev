{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bdc7a27",
   "metadata": {},
   "source": [
    "# LLM Inference with AMD Instinct™ GPUs\n",
    "In this tutorial, we’ll explore how to leverage Hugging Face Transformers, Text Generation Inference (TGI), and vLLM to serve and test LLMs on AMD hardware. You’ll learn how to install and configure ROCm for AMD Instinct™ GPUs, set environment variables for multi-GPU setups (e.g., HIP_VISIBLE_DEVICES), and launch your favorite models in FP16 or BF16 to balance performance and memory usage. We’ll also walk through best practices for containerizing your workflow with Docker, ensuring a reproducible setup for model deployment. By following these steps, you’ll be able to run advanced LLMs in a ROCm-accelerated environment, capitalizing on AMD GPU performance for state-of-the-art natural language processing tasks.\n",
    "\n",
    "## Prerequisites\n",
    "### 1. Hardware Requirements\n",
    "* AMD ROCm GPUs (e.g., MI210, MI300X, 7900-xt)\n",
    "* Ensure your system meets the System Requirements, including ROCm 6.0+\n",
    "\n",
    "### 2. Software\n",
    "* **ROCm installed** and verified on your system\n",
    "* **Docker installed** on your system. Refer to the [Docker installation guide](https://docs.docker.com/get-docker/) if needed\n",
    "\n",
    "### 3. System Configuration\n",
    "* **NUMA auto-balancing** disabled for optimal performance\n",
    "```bash\n",
    "(shell)sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'\n",
    "(shell)cat /proc/sys/kernel/numa_balancing\n",
    "--> output should be 0\n",
    "```\n",
    "* **ROCm** environment validated using rocm-smi\n",
    "```bash\n",
    "(shell)rocm-smi\n",
    "--> user should see all the available GPUs as shown in below table(MI300x case):\n",
    "\n",
    "============================================ ROCm System Management Interface ============================================\n",
    "====================================================== Concise Info ======================================================\n",
    "Device  Node  IDs              Temp        Power     Partitions          SCLK    MCLK    Fan  Perf  PwrCap  VRAM%  GPU%  \n",
    "              (DID,     GUID)  (Junction)  (Socket)  (Mem, Compute, ID)                                                  \n",
    "==========================================================================================================================\n",
    "0       2     0x74a1,   28851  41.0°C      133.0W    NPS1, SPX, 0        132Mhz  900Mhz  0%   auto  750.0W  0%     0%    \n",
    "1       3     0x74a1,   51499  37.0°C      133.0W    NPS1, SPX, 0        132Mhz  900Mhz  0%   auto  750.0W  0%     0%    \n",
    "2       4     0x74a1,   57603  38.0°C      136.0W    NPS1, SPX, 0        132Mhz  900Mhz  0%   auto  750.0W  0%     0%    \n",
    "3       5     0x74a1,   22683  34.0°C      133.0W    NPS1, SPX, 0        132Mhz  900Mhz  0%   auto  750.0W  0%     0%    \n",
    "4       6     0x74a1,   53458  38.0°C      133.0W    NPS1, SPX, 0        132Mhz  900Mhz  0%   auto  750.0W  0%     0%    \n",
    "5       7     0x74a1,   26954  35.0°C      132.0W    NPS1, SPX, 0        132Mhz  900Mhz  0%   auto  750.0W  0%     0%    \n",
    "6       8     0x74a1,   16738  39.0°C      134.0W    NPS1, SPX, 0        132Mhz  900Mhz  0%   auto  750.0W  0%     0%    \n",
    "7       9     0x74a1,   63738  37.0°C      131.0W    NPS1, SPX, 0        132Mhz  900Mhz  0%   auto  750.0W  0%     0%    \n",
    "==========================================================================================================================\n",
    "================================================== End of ROCm SMI Log ===================================================\n",
    "```\n",
    "### 3.Hugging Face API Access\n",
    "* Obtain an API token from Hugging Face for [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)\n",
    "* Ensure you have a Hugging Face API token with the necessary permissions and approval to access (token starts with hf_xxxxxx)\n",
    "**Note:** Need Hugging Face API token for rest of the tutorial (make sure to save the token on a separate txt file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
