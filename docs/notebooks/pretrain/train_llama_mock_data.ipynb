{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Training Llama 3.1 with Megatron-LM \n",
    "\n",
    "This tutorial demonstrates how to train the **Llama 3.1 model** using **mock data**. The **Llama 3.1 model** is a popular open-source large language model designed to handle a wide range of natural language processing tasks efficiently. You can learn more about Llama models at [Llama's official website](https://www.llama.com/).\n",
    "\n",
    "We use **mock data** in this tutorial to provide a quick and lightweight demonstration of the training workflow, enabling you to verify that your environment is correctly configured and functional. Mock data is a useful way to validate the training pipeline without requiring large datasets.\n",
    "\n",
    "The training process leverages the **Megatron-LM framework**, a specialized framework for pretraining and fine-tuning large-scale language models. For more information about Megatron-LM, see the [official GitHub repository](https://github.com/NVIDIA/Megatron-LM). All steps will be executed within a **Docker container**, which provides a ready-to-use environment with all necessary dependencies.\n",
    "\n",
    "This tutorial builds on the setup completed in **Tutorial 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before proceeding with this tutorial, ensure that you have the following:\n",
    "\n",
    "### 1. Hardware\n",
    "- **AMD Instinct™ GPUs** (e.g., MI300X) or compatible hardware with ROCm support.\n",
    "\n",
    "### 2. Software\n",
    "- **ROCm installed** and verified on your system. Follow the setup instructions in **Tutorial 1** if you haven’t done so.\n",
    "- **Docker installed** on your system. Refer to the [Docker installation guide](https://docs.docker.com/get-docker/) if needed.\n",
    "\n",
    "### 3. System Configuration\n",
    "Ensure your system meets the configuration requirements from **Tutorial 1**, including:\n",
    "- **NUMA auto-balancing disabled** for optimal performance.\n",
    "- **ROCm environment validated** using `rocm-smi`.\n",
    "\n",
    "### 4. Hugging Face API Token\n",
    "Ensure you have a Hugging Face API token with the necessary permissions and approval to access [Meta's LLaMA checkpoints](https://huggingface.co/meta-llama)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Environment\n",
    "\n",
    "Once your system meets the prerequisites, follow these steps to set up the training environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71551b21",
   "metadata": {},
   "source": [
    "### Step 1: Pull the Docker Image\n",
    "Run the following command in your terminal to pull the prebuilt Docker image containing all necessary dependencies:\n",
    "```bash\n",
    "docker pull rocm/megatron-lm:24.12-dev\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069dc1ff",
   "metadata": {},
   "source": [
    "### Step 2: Launch the Docker Container\n",
    "Run the following command in your terminal to launch the Docker container with the appropriate configuration:\n",
    "```bash\n",
    "docker run -it --device /dev/dri --device /dev/kfd --network host --ipc host \\\n",
    "    --group-add video --cap-add SYS_PTRACE --security-opt seccomp=unconfined \\\n",
    "    --privileged -v /path/to/notebooks:/workspace/notebooks \\\n",
    "    --name megatron-dev-env rocm/megatron-lm:24.12-dev /bin/bash\n",
    "```\n",
    "**Important:** Replace `/path/to/notebooks` with the full path to the directory on your host machine where your notebooks are stored. Ensure this directory is accessible to Docker and contains the necessary files for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3dcef6",
   "metadata": {},
   "source": [
    "### Step 3: Install Jupyter and Start the Server\n",
    "After launching the Docker container, run the following commands inside the container to install Jupyter and start the notebook server:\n",
    "```bash\n",
    "pip install jupyter\n",
    "jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root\n",
    "```\n",
    "**Note:** Jupyter installation inside Docker is necessary to execute notebook cells. Save the token or URL provided in the terminal output to access the notebook from your host machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9601ad38",
   "metadata": {},
   "source": [
    "### Step 4: Clone the Megatron-LM Repository\n",
    "Run the following commands inside the Docker container to clone the Megatron-LM repository and navigate to the validated commit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bac989",
   "metadata": {
    "tags": [
     "docker"
    ]
   },
   "outputs": [],
   "source": [
    "# Clone the Megatron-LM repository and navigate to the validated commit\n",
    "!git clone https://github.com/ROCm/Megatron-LM && cd Megatron-LM && git checkout bb93ccbfeae6363c67b361a97a27c74ab86e7e92"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e506c4",
   "metadata": {},
   "source": [
    "### Step 5: Provide Your Hugging Face Token \n",
    "Hugging Face Token can be generated by signing into your account at **[Hugging Face Tokens](https://huggingface.co/settings/tokens)**.\n",
    "\n",
    "You will need a Hugging Face API token to access Llama-3.1-8B. Tokens typically start with \"hf_\". Generate your token at Hugging Face Tokens and request access for Llama-3.1-8B.\n",
    "\n",
    "Run the following interactive block in your Jupyter notebook to set up the token:\n",
    "\n",
    "Note: Please uncheck the \"Add token as Git credential\" option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d549b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "# Prompt the user to log in\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b8452",
   "metadata": {},
   "source": [
    "Verify that your token was captured correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d624e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d0a5f3",
   "metadata": {},
   "source": [
    "## Run Training Script\n",
    "\n",
    "### Single-Node Training Overview\n",
    "The training process involves running a pre-configured script that initializes and executes the training of the **Llama 3.1 model**. The script leverages the **Megatron-LM framework** and mock data to simulate a full training pipeline. This approach ensures your environment is configured correctly and functional for real-world use cases.\n",
    "\n",
    "Before running the script, ensure all environment variables are set correctly and verify your system's network interface as described in **Step 4**.\n",
    "\n",
    "### Key Parameters for Training:\n",
    "- **Batch Size (BS):** Set to `64` for optimal GPU usage.\n",
    "- **Sequence Length (SEQ_LENGTH):** Input sequence length, set to `4096`.\n",
    "- **Tensor Parallelism (TP):** Set to `8` for efficient parallelism.\n",
    "- **Precision (TE_FP8):** Set to `0` for BF16 precision.\n",
    "\n",
    "### Run the Training Script\n",
    "Use the following command to train the model on a single node:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb180bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd Megatron-LM && TEE_OUTPUT=1 MBS=2 BS=64 TP=8 TE_FP8=0 SEQ_LENGTH=4096  \\\n",
    "TOKENIZER_MODEL='meta-llama/Llama-3.1-8B' MODEL_SIZE='8' \\\n",
    "bash examples/llama/train_llama3.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c748d2e8",
   "metadata": {},
   "source": [
    "### What This Command Does\n",
    "This command configures the training process with the following parameters:\n",
    "- **`TEE_OUTPUT=1`**: Enables logging output to the console.\n",
    "- **`MBS=2`**: Micro-batch size per GPU.\n",
    "- **`BS=64`**: Total batch size across all GPUs.\n",
    "- **`TP=8`**: Tensor parallelism for distributing the model across GPUs.\n",
    "- **`TE_FP8=0`**: Sets precision to BF16 for training.\n",
    "- **`SEQ_LENGTH=4096`**: Maximum input sequence length.\n",
    "\n",
    "The training script will:\n",
    "- Use mock data as input.\n",
    "- Train the **Llama 3.1 model** with the specified configurations.\n",
    "\n",
    "You can customize these parameters based on your hardware and desired configurations by modifying the command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51275b54",
   "metadata": {},
   "source": [
    "## Monitor Training Progress\n",
    "\n",
    "Monitor the output logs during the training process for the following:\n",
    "- **Iteration Progress**: The number of completed iterations.\n",
    "- **Loss Values**: Indicates the model's learning progress. Lower values suggest better learning.\n",
    "- **GPU Utilization**: Ensures optimal usage of your hardware resources.\n",
    "\n",
    "Logs are printed to the console and saved to a log file within the directory specified by the script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8378cfb",
   "metadata": {},
   "source": [
    "## Key Notes\n",
    "\n",
    "- Mock data is for validation only. To provide different dataset, please refer to Tutorial 1.\n",
    "- Tune hyperparameters based on your hardware. The hyperparameters set in this tutorial are based on one node of 8x MI300x GPUs.\n",
    "- This example illustrates how to run a training task on a single node. For multi-node training instructions please refer to Tutorial 1.\n",
    "- Verify the logs for correctness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
