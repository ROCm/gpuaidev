{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "52699cd0",
      "metadata": {},
      "source": [
        "# SE(3)-Transformer for molecular property prediction\n",
        "\n",
        "*Authored by: Anuya Welling, James E. T. Smith, Diptorup Deb, Mukhil Azhagan Mallaiyan Sathiaseelan, Yao Liu, Phani Vaddadi, and Vish Vadlamani*\n",
        "\n",
        "The [SE(3)-Transformer](https://arxiv.org/pdf/2006.10503) is a graph neural network that uses a variant of self-attention for 3D points and graphs processing.\n",
        "This model is equivariant under continuous 3D roto-translations, which means that when the inputs (graphs or sets of points) rotate in 3D space\n",
        "(or more generally experience a proper rigid transformation), the model outputs either stay invariant or transform with the input.\n",
        "\n",
        "In the SE(3)-Transformer model, both training and inference are framed as regression tasks over a set of molecular properties. These include parameters such as Î¼, Î±, HOMO, LUMO, gap, RÂ², ZPVE, Uâ‚€, U, H, G, and Cv, along with their atom-wise counterparts and rotational constants (A, B, C).\n",
        "\n",
        "While the full physical interpretation of many of these quantities is best left to subject matter experts, this tutorial focuses on a few key electronic properties that are commonly used in molecular modeling. HOMO (Highest Occupied Molecular Orbital) refers to the highest energy level that is still occupied by electrons. LUMO (Lowest Unoccupied Molecular Orbital) is the lowest energy level that does not contain electrons and is available for occupation. The HOMOâ€“LUMO gap, often simply called \"the gap\", is the energy difference between these two orbitals and is an important indicator of a molecule's electronic and chemical behavior.\n",
        "\n",
        "In drug discovery, SE(3)-Transformer models predict molecular properties that matter for real biological behavior. A good drug must bind strongly to its target, avoid off-target reactions, and remain stable in the body while being reactive at the right site.\n",
        "\n",
        "HOMO and LUMO capture this balance. They indicate a molecule's tendency to donate or accept electrons, which influences how it reacts with protein amino acids and whether it could be toxic. The HOMOâ€“LUMO gap acts as a proxy for reactivity -- too small can mean instability and side effects, while too large can mean inactivity. Poor reactivity often leads to drug failure, even when binding looks promising.\n",
        "\n",
        "## For more information\n",
        "\n",
        "To find out more about DGL, SE(3)-Transformers, and AMD ROCm performance benchmarks, see the following blogs:\n",
        "\n",
        "* [Graph Neural Networks at Scale: DGL with ROCm on AMD Hardware](https://rocm.blogs.amd.com/artificial-intelligence/why-graph-neural/README.html)  \n",
        "   Learn about DGL (Deep Graph Library), its design principles, and how it enables scalable graph neural networks on AMD hardware.\n",
        "\n",
        "* [DGL in the Real World: Running GNNs on Real Use Cases](https://rocm.blogs.amd.com/artificial-intelligence/dgl_blog2/README.html)  \n",
        "   Explore real-world DGL applications including GNN-FiLM, ARGO, GATNE for e-commerce recommendations, and EEG-GCNN for neurological disease diagnosis.\n",
        "\n",
        "* [DGL in Depth: SE(3)-Transformer on ROCm 7](https://rocm.blogs.amd.com/artificial-intelligence/dgl-in-depth/README.html)  \n",
        "   Review this step-by-step implementation guide for benchmark results showing the great latency and throughput performance of the AMD Instinct MI300X GPU on SE(3)-Transformer workloads.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "This tutorial was developed and tested using the following setup.\n",
        "\n",
        "### Hardware requirements\n",
        "\n",
        "* **GPU**: AMD Instinctâ„¢ MI300X or MI250X GPUs.\n",
        "\n",
        "**Note**: This tutorial has been tested and validated on AMD Instinct MI300X GPUs. For the official list of supported GPUs, see the [DGL Compatibility Documentation](https://rocm.docs.amd.com/en/latest/compatibility/ml-compatibility/dgl-compatibility.html). For performance metrics on AMD Instinct GPUs, see the [SE(3)-Transformer performance report blog](https://rocm.blogs.amd.com/artificial-intelligence/dgl-in-depth/README.html).\n",
        "\n",
        "### Software requirements\n",
        "\n",
        "AMD validates and publishes DGL images with ROCm backends on [Docker Hub](https://hub.docker.com/r/rocm/dgl). The following table shows the validated software stack configurations:\n",
        "\n",
        "| Software | Supported versions | Notes |\n",
        "|----------|-------------------|-------|\n",
        "| **ROCm** | 6.4.0, 6.4.3, 7.0.0 | AMD GPU compute platform |\n",
        "| **Python** | 3.10, 3.12 | Programming language |\n",
        "| **PyTorch** | 2.3.0, 2.4.1, 2.6.0, 2.7.1, 2.8.0 | Deep learning framework (ROCm build) |\n",
        "| **DGL** | 2.4.0 | Deep Graph Library for GNNs |\n",
        "| **Ubuntu** | 22.04, 24.04 | Operating system |\n",
        "| **Docker** | 20.10+ | Container runtime (recommended) |\n",
        "\n",
        "### Validated Docker images\n",
        "\n",
        "The recommended approach is to use the prebuilt Docker images from AMD. Select the image that matches your desired configuration:\n",
        "\n",
        "| Docker image tag | DGL release | ROCm | PyTorch | Ubuntu | Python |\n",
        "|------------------|-----|------|---------|--------|--------|\n",
        "| `rocm/dgl:dgl-2.4.0.amd0_rocm7.0.0_ubuntu24.04_py3.12_pytorch_2.8.0` | 25.10 | 7.0.0 | 2.8.0 | 24.04 | 3.12 |\n",
        "| `rocm/dgl:dgl-2.4.0.amd0_rocm7.0.0_ubuntu24.04_py3.12_pytorch_2.6.0` | 25.10 | 7.0.0 | 2.6.0 | 24.04 | 3.12 |\n",
        "| `rocm/dgl:dgl-2.4.0.amd0_rocm7.0.0_ubuntu22.04_py3.10_pytorch_2.7.1` | 25.10 | 7.0.0 | 2.7.1 | 22.04 | 3.10 |\n",
        "| `rocm/dgl:dgl-2.4.0.amd0_rocm6.4.3_ubuntu24.04_py3.12_pytorch_2.6.0` | 25.10 | 6.4.3 | 2.6.0 | 24.04 | 3.12 |\n",
        "| `rocm/dgl:dgl-2.4_rocm6.4_ubuntu24.04_py3.12_pytorch_release_2.6.0` | 25.07 | 6.4.0 | 2.6.0 | 24.04 | 3.12 |\n",
        "| `rocm/dgl:dgl-2.4_rocm6.4_ubuntu24.04_py3.12_pytorch_release_2.4.1` | 25.07 | 6.4.0 | 2.4.1 | 24.04 | 3.12 |\n",
        "| `rocm/dgl:dgl-2.4_rocm6.4_ubuntu22.04_py3.10_pytorch_release_2.4.1` | 25.07 | 6.4.0 | 2.4.1 | 22.04 | 3.10 |\n",
        "| `rocm/dgl:dgl-2.4_rocm6.4_ubuntu22.04_py3.10_pytorch_release_2.3.0` | 25.07 | 6.4.0 | 2.3.0 | 22.04 | 3.10 |\n",
        "\n",
        "\n",
        "## Set up the SE(3)-Transformer environment\n",
        "\n",
        "In this tutorial, you will work on the prebuilt ROCm DGL image as an example. You can also use other validated DGL images from the table above.\n",
        "\n",
        "### Step 1: Launch the Docker image\n",
        "\n",
        "Launch the Docker container. Replace `/path/to/workspace` with the full path to the directory on your host machine where you want to clone the DeepLearningExamples code and run the notebook. Choose the image tag from the validated Docker images table above that matches your desired configuration.\n",
        "\n",
        "``` bash\n",
        "docker run -it --rm --privileged \\\n",
        "  --network=host \\\n",
        "  --device=/dev/kfd \\\n",
        "  --device=/dev/dri \\\n",
        "  --group-add=video \\\n",
        "  --ipc=host \\\n",
        "  --cap-add=SYS_PTRACE \\\n",
        "  --security-opt seccomp=unconfined \\\n",
        "  -v /path/to/workspace:/workspace \\\n",
        "  -w /workspace \\\n",
        "  <IMAGE_TAG>\n",
        "```\n",
        "\n",
        "**Note**: This command mounts your host directory to `/workspace` in the container. Ensure the notebook file is in this directory or upload it after Jupyter starts. The remaining steps should be run inside the Docker container. Save the URL (and token, if shown) from the terminal output to access JupyterLab from your browser.\n",
        "\n",
        "For example, to use ROCm 7.0.0 with PyTorch 2.7.1:\n",
        "\n",
        "``` bash\n",
        "docker run -it --rm --privileged \\\n",
        "  --network=host \\\n",
        "  --device=/dev/kfd \\\n",
        "  --device=/dev/dri \\\n",
        "  --group-add=video \\\n",
        "  --ipc=host \\\n",
        "  --cap-add=SYS_PTRACE \\\n",
        "  --security-opt seccomp=unconfined \\\n",
        "  -v /path/to/workspace:/workspace \\\n",
        "  -w /workspace \\\n",
        "  rocm/dgl:dgl-2.4.0.amd0_rocm7.0.0_ubuntu22.04_py3.10_pytorch_2.7.1\n",
        "```\n",
        "\n",
        "### Step 2: Install and launch Jupyter\n",
        "\n",
        "Inside the Docker container, install Jupyter and the visualization packages used by this notebook:\n",
        "\n",
        "``` bash\n",
        "pip install jupyter\n",
        "```\n",
        "\n",
        "Start the Jupyter server:\n",
        "\n",
        "``` bash\n",
        "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
        "```\n",
        "\n",
        "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`.\n",
        "\n",
        "\n",
        "### Step 3: Open the notebook\n",
        "\n",
        "Once JupyterLab is running, open your browser and go to the URL shown in the terminal (typically `http://localhost:8888`). In the file browser, navigate to `DGLPyTorch/DrugDiscovery/SE3Transformer/` and click **se3transformer.ipynb** (or this intro notebook) to begin. Alternatively, you can upload this notebook to your Jupyter lab via the upload button in Jupyter.\n",
        "\n",
        "\n",
        "## Two ways to use this notebook\n",
        "\n",
        "This notebook supports two modes:\n",
        "\n",
        "* **Quick inference mode** (~10 minutes)\n",
        "   - Use a pretrained model trained for 100 epochs.\n",
        "   - Skip training and go straight to inference and results.\n",
        "   - Perfect for exploring model capabilities quickly.\n",
        "   \n",
        "* **Training mode** (~30-60 minutes)\n",
        "   - Train from scratch for five epochs.\n",
        "   - See the full pipeline: data â†’ model â†’ train â†’ evaluate.\n",
        "   - Perfect for understanding the complete workflow.\n",
        "\n",
        "To select a mode, set `USE_PRETRAINED = True` (for inference) or `USE_PRETRAINED = False` (for training) in the configuration cell below.\n",
        "\n",
        "**Note**: Run the rest of this notebook in Jupyter by executing the cells."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff82b11e",
      "metadata": {},
      "source": [
        "## Loading the dependencies and repository\n",
        "\n",
        "This section explains how to download and install the required code and dependencies.\n",
        "\n",
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f85e07b",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!pip install plotly torchinfo rdkit py3Dmol"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "887a6232",
      "metadata": {},
      "source": [
        "### Clone the DeepLearningExamples repository\n",
        "\n",
        "Clone the ROCm DeepLearningExamples repository using sparse checkout to download only the SE(3)-Transformer code. Run the following inside the Docker container (for example, in a terminal or in a notebook code cell):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e667c75",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "git clone --filter=blob:none --sparse https://github.com/ROCm/DeepLearningExamples.git\n",
        "cd DeepLearningExamples\n",
        "git sparse-checkout set DGLPyTorch/DrugDiscovery/SE3Transformer\n",
        "cd DGLPyTorch/DrugDiscovery/SE3Transformer\n",
        "pip install -r requirements.txt\n",
        "pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e05b64ab",
      "metadata": {},
      "source": [
        "\n",
        "### Add installed packages to path\n",
        "\n",
        "Import the necessary packages and add them to the path.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6841e153",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "  import os\n",
        "  import sys\n",
        "\n",
        "  # Get current working directory and build the SE3 path\n",
        "  base_dir = os.getcwd()\n",
        "  se3_path = os.path.abspath(os.path.join(base_dir, \"DeepLearningExamples/DGLPyTorch/DrugDiscovery/SE3Transformer\"))\n",
        "\n",
        "  # Verify it exists and add to path\n",
        "  if os.path.exists(se3_path):\n",
        "      if se3_path not in sys.path:\n",
        "          sys.path.insert(0, se3_path)\n",
        "      print(f\"âœ… Added {se3_path} to Python path\")\n",
        "      print(f\"   Current working directory: {base_dir}\")\n",
        "  else:\n",
        "      print(f\"âŒ Error: Path not found: {se3_path}\")\n",
        "      print(f\"   Please check if the repository was cloned correctly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52991f64",
      "metadata": {},
      "source": [
        "## What happens next?\n",
        "\n",
        "First, choose between inference or training mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "config_cell_001",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ðŸŽ¯ CONFIGURATION: Choose Your Path\n",
        "# ============================================\n",
        "\n",
        "USE_PRETRAINED = False  # Toggle: True = Inference only | False = Train from scratch\n",
        "\n",
        "if USE_PRETRAINED:\n",
        "    print(\"=\"*60)\n",
        "    print(\"ðŸ“Š MODE: Quick Inference with Pretrained Model\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"âœ… Will load: model_qm9_100_epochs.pth\")\n",
        "    print(\"â­ï¸  Training cells will be skipped\")\n",
        "    print(\"âš¡ Estimated time: ~10 minutes\\n\")\n",
        "    CHECKPOINT_PATH = \"model_qm9_100_epochs.pth\"\n",
        "    EPOCHS = None\n",
        "else:\n",
        "    print(\"=\"*60)\n",
        "    print(\"ðŸ‹ï¸ MODE: Training from Scratch\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"âœ… Will train for 5 epochs\")\n",
        "    print(\"ðŸ’¾ Will save to: model_qm9_5_epochs.pth\")\n",
        "    print(\"â±ï¸  Estimated time: ~30-60 minutes\\n\")\n",
        "    CHECKPOINT_PATH = \"model_qm9_5_epochs.pth\"\n",
        "    EPOCHS = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "path_helper",
      "metadata": {},
      "source": [
        "Depending on which setting you choose, the next steps vary somewhat:\n",
        "\n",
        "- If `USE_PRETRAINED = True` (inference mode):\n",
        "  1. Import the libraries\n",
        "  2. Load the dataset and inspect the molecules\n",
        "  3. Initialize the model architecture\n",
        "  4. Skip training, logging setup, and visualization\n",
        "  5. Load a pretrained checkpoint (`model_qm9_100_epochs.pth`)\n",
        "  6. Run inference and see the results\n",
        "\n",
        "- If `USE_PRETRAINED = False` (training mode):\n",
        "  1. Import the libraries\n",
        "  2. Load the dataset and inspect the molecules\n",
        "  3. Initialize the model architecture\n",
        "  4. Train for five epochs\n",
        "  5. Visualize training progress\n",
        "  6. Load your trained checkpoint (`model_qm9_5_epochs.pth`)\n",
        "  7. Run inference and see the results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ff322de",
      "metadata": {},
      "source": [
        "## Imports for training and evaluation\n",
        "\n",
        "These imported modules set up the full SE(3)-Transformer training and evaluation pipeline on the QM9 molecular dataset. They cover data loading, distributed training, optimization, logging, and inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eab7a12f",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Core imports needed for both training and inference\n",
        "import torch.nn as nn\n",
        "import dgl\n",
        "\n",
        "from se3_transformer.data_loading import QM9DataModule\n",
        "from se3_transformer.model import SE3TransformerPooled\n",
        "from se3_transformer.model.fiber import Fiber\n",
        "from se3_transformer.runtime.arguments import PARSER\n",
        "from se3_transformer.runtime.utils import (\n",
        "    seed_everything,\n",
        "    using_tensor_cores,\n",
        ")\n",
        "import logging\n",
        "   \n",
        "from se3_transformer.runtime.callbacks import (\n",
        "    QM9MetricCallback,\n",
        "    QM9LRSchedulerCallback,\n",
        ")\n",
        "from se3_transformer.runtime.loggers import (\n",
        "    LoggerCollection,\n",
        "    DLLogger,\n",
        ")\n",
        "from se3_transformer.runtime.training import train\n",
        "    \n",
        "print(\"âœ… Loaded training modules\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cca427d",
      "metadata": {},
      "source": [
        "### Using the CLI arguments to set up training\n",
        "\n",
        "The SE(3)-Transformer example from the DeepLearningExamples repository was originally designed to run as a command-line program, but you can easily adapt it to use in a Jupyter notebook. The training configuration, including model, optimizer, and runtime settings, is managed through an argparse parser, which you can leverage directly within the notebook for flexible experimentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc0f98a6",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "if not USE_PRETRAINED:\n",
        "    # Uncomment the line below to see all available training and runtime arguments\n",
        "    # PARSER.print_help()\n",
        "\n",
        "    # Training configuration - adjust these parameters as needed\n",
        "    TRAINING_ARGS = [\n",
        "        \"--epochs\", str(EPOCHS),              # Number of training epochs (5 for quick training)\n",
        "        \"--eval_interval\", \"1\",               # Evaluate every N epochs\n",
        "        \"--batch_size\", \"240\",                # Batch size per GPU\n",
        "        \"--num_workers\", \"16\",                # Data loader workers\n",
        "        \"--precompute_bases\",                 # Precompute geometric bases for speed\n",
        "        \"--use_layer_norm\",                   # Use layer normalization\n",
        "        \"--norm\",                             # Normalize features\n",
        "        \"--save_ckpt_path\", CHECKPOINT_PATH,  # Where to save the trained model\n",
        "        \"--task\", \"homo\",                     # Prediction task (homo, lumo, gap, etc.)\n",
        "        \"--amp\",                              # Enable automatic mixed precision\n",
        "        \"--learning_rate\", \"0.002\",           # Initial learning rate\n",
        "        \"--weight_decay\", \"0.1\",              # L2 regularization\n",
        "        \"--seed\", \"42\",                       # Random seed for reproducibility\n",
        "    ]\n",
        "\n",
        "    # Parse arguments\n",
        "    args = PARSER.parse_args(TRAINING_ARGS)\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"ðŸ”§ Training Configuration\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Epochs: {args.epochs}\")\n",
        "    print(f\"Batch Size: {args.batch_size}\")\n",
        "    print(f\"Learning Rate: {args.learning_rate}\")\n",
        "    print(f\"Weight Decay: {args.weight_decay}\")\n",
        "    print(f\"Checkpoint Path: {args.save_ckpt_path}\")\n",
        "    print(f\"Task: {args.task}\")\n",
        "    print(f\"AMP Enabled: {args.amp}\")\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    # For pretrained mode, we still need args for model initialization\n",
        "    # but we won't use them for training\n",
        "    args = PARSER.parse_args([\n",
        "        \"--epochs\", \"100\",\n",
        "        \"--batch_size\", \"240\",\n",
        "        \"--num_workers\", \"16\",\n",
        "        \"--use_layer_norm\",\n",
        "        \"--norm\",\n",
        "        \"--task\", \"homo\",\n",
        "    ])\n",
        "    print(\"â­ï¸  Skipping training configuration (using pretrained model)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fe8c03c",
      "metadata": {},
      "source": [
        "## Download the pretrained model from Hugging Face\n",
        "\n",
        "If you are using pretrained mode (`USE_PRETRAINED = True`), the `model_qm9_100_epochs.pth` checkpoint must be available locally. This checkpoint is hosted on the Hugging Face Hub. To download it, follow these steps:\n",
        "\n",
        "1. Install the Hugging Face Hub client using `pip install huggingface_hub`.\n",
        "2. Log in to Hugging Face (recommended for gated or rate-limited access). Run `huggingface-cli login` in a terminal and enter your token from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens).\n",
        "3. Download the model into the current directory with:\n",
        "\n",
        "   ```bash\n",
        "   huggingface-cli download amd/se3_transformers model_qm9_100_epochs.pth --local-dir .\n",
        "   ```\n",
        "\n",
        "The cell below automatically runs these steps when `USE_PRETRAINED` is `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d3a065d",
      "metadata": {},
      "outputs": [],
      "source": [
        "if USE_PRETRAINED:\n",
        "    # Install Hugging Face Hub if not already installed\n",
        "    try:\n",
        "        from huggingface_hub import hf_hub_download\n",
        "    except ImportError:\n",
        "        import subprocess\n",
        "        import sys\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"huggingface_hub\"])\n",
        "        from huggingface_hub import hf_hub_download\n",
        "\n",
        "    # Advise logging in for gated repos or to avoid rate limits\n",
        "    print(\"ðŸ’¡ If the model is gated or you hit rate limits, log in first: huggingface-cli login\")\n",
        "    print(\"   Get a token at: https://huggingface.co/settings/tokens\\n\")\n",
        "\n",
        "    # Download pretrained checkpoint to current directory\n",
        "    print(\"ðŸ“¥ Downloading model_qm9_100_epochs.pth from amd/se3_transformers ...\")\n",
        "    path = hf_hub_download(\n",
        "        repo_id=\"amd/se3_transformers\",\n",
        "        filename=\"model_qm9_100_epochs.pth\",\n",
        "        local_dir=\".\",\n",
        "        local_dir_use_symlinks=False,\n",
        "    )\n",
        "    print(f\"âœ… Downloaded to: {path}\")\n",
        "else:\n",
        "    print(\"â­ï¸  Skipping download (training from scratch)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50c15f3b",
      "metadata": {},
      "source": [
        "## Dataset and model setup\n",
        "\n",
        "Start by loading the QM9 molecular dataset using QM9DataModule, which handles data preprocessing, batching, and splitting for training and evaluation.\n",
        "Next, initialize the SE(3)-Transformer model (SE3TransformerPooled) with input, edge, and output fibers that define how geometric and feature information flows through the network.\n",
        "Finally, define the L1 loss (`nn.L1Loss`) â€” a simple yet effective choice for molecular property regression tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96f3d1d8",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "datamodule = QM9DataModule(**vars(args))\n",
        "model = SE3TransformerPooled(\n",
        "    fiber_in=Fiber({0: datamodule.NODE_FEATURE_DIM}),\n",
        "    fiber_out=Fiber({0: args.num_degrees * args.num_channels}),\n",
        "    fiber_edge=Fiber({0: datamodule.EDGE_FEATURE_DIM}),\n",
        "    output_dim=1,\n",
        "    tensor_cores=using_tensor_cores(args.amp),  # use Tensor Cores more effectively\n",
        "    **vars(args),\n",
        ")\n",
        "loss_fn = nn.L1Loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "376800f0",
      "metadata": {},
      "source": [
        "## Inspecting the molecules\n",
        "Before diving into training, itâ€™s helpful to visually inspect the molecules from the QM9 dataset.\n",
        "\n",
        "Review one of the molecules in the dataset:\n",
        "\n",
        "![molecule](../assets/molecule.png)\n",
        "\n",
        "### Basic graph information\n",
        "\n",
        "``` text\n",
        "--- BASIC INFO ---\n",
        "Nodes: 14\n",
        "Edges: 28\n",
        "```\n",
        "\n",
        "The molecule is represented as a graph with 14 nodes corresponding to atoms and 28 edges representing atomâ€“atom interactions.\n",
        "Edges are constructed based on interatomic proximity rather than explicit chemical bonds.\n",
        "\n",
        "### Node features (ndata)\n",
        "\n",
        "Each node (atom) is associated with geometric and chemical features.\n",
        "\n",
        "#### Atomic positions\n",
        "\n",
        "``` text\n",
        "Key: pos\n",
        "Shape: (14, 3)\n",
        "Dtype: torch.float32\n",
        "\n",
        "tensor([[ 0.6781, -0.0583,  0.7324],\n",
        "        [-0.1432,  0.3297, -0.3889],\n",
        "        [-1.5010, -0.2886, -0.2937],\n",
        "        ...])\n",
        "\n",
        "```\n",
        "Each row represents the 3D Cartesian coordinates `[x,y,z]` of an atom in the molecule.\n",
        "\n",
        "#### Atomic attributes\n",
        "\n",
        "``` text\n",
        "Key: attr\n",
        "Shape: (14, 11)\n",
        "Dtype: torch.float32\n",
        "\n",
        "tensor([[0., 0., 0., 1., 0., 8., 0., 0., 0., 0., 1.],\n",
        "        [0., 1., 0., 0., 0., 6., 0., 0., 0., 0., 2.],\n",
        "        [0., 1., 0., 0., 0., 6., 0., 0., 0., 0., 0.],\n",
        "        ...])\n",
        "```\n",
        "Each row encodes atom-specific properties, such as atomic type and related categorical or numerical descriptors, which allow the model to distinguish between different elements.\n",
        "\n",
        "### Edge features (edata)\n",
        "\n",
        "Edges capture pairwise relationships between atoms.\n",
        "\n",
        "``` text\n",
        "Edge Attributes\n",
        "Key: edge_attr\n",
        "Shape: (28, 4)\n",
        "Dtype: torch.float32\n",
        "\n",
        "tensor([[1., 0., 0., 0.],\n",
        "        [1., 0., 0., 0.],\n",
        "        [1., 0., 0., 0.],\n",
        "        ...])\n",
        "```\n",
        "Each row represents a feature vector associated with an edge, typically encoding distance-based or radial information used to model interatomic interactions.\n",
        "\n",
        "### RAW values of HOMO, LUMO, and GAP\n",
        "\n",
        "The raw values of HOMO, LUMO and GAP in eV is as follows.\n",
        "\n",
        "#### HOMO (Highest Occupied Molecular Orbital) energy\n",
        "\n",
        "```\n",
        "Shape: torch.Size([1])\n",
        "Value: tensor([-5.7987]) eV\n",
        "```\n",
        "\n",
        "#### LUMO (Lowest Unoccupied Molecular Orbital) energy\n",
        "\n",
        "```\n",
        "Shape: torch.Size([1])\n",
        "Value: tensor([0.9905]) eV\n",
        "```\n",
        "\n",
        "#### GAP (gap between HOMO and LUMO)\n",
        "\n",
        "```\n",
        "Shape: torch.Size([1])\n",
        "Value: tensor([6.7892]) eV\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6ab24b7",
      "metadata": {},
      "source": [
        "## View the model summary\n",
        "\n",
        "Quickly inspect the SE(3)-Transformer architecture using `torchinfo.summary`, which prints a detailed overview of each layer, its input/output shapes, and the number of parameters. This helps you verify that the model has been built correctly before training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53f9f17d",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "training_section_header",
      "metadata": {},
      "source": [
        "## Part A: Training pipeline (optional)\n",
        "\n",
        "Use this code to configure the training pipeline.\n",
        "\n",
        "**Note**: If `USE_PRETRAINED` is set to `True`, the training cells below will be skipped, and you'll proceed directly to inference.\n",
        "\n",
        "### Logging and callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "443e9ad2",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Initialize logging, set seed, configure loggers and training callbacks\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "if args.seed is not None:\n",
        "    logging.info(f\"Using seed {args.seed}\")\n",
        "    seed_everything(args.seed)\n",
        "\n",
        "logging.info(f\"Saving info to {args.log_dir}/{args.dllogger_name}\")\n",
        "loggers = [DLLogger(save_dir=args.log_dir, filename=args.dllogger_name)]\n",
        "logger = LoggerCollection(loggers)\n",
        "callbacks = [\n",
        "    QM9MetricCallback(logger, targets_std=datamodule.targets_std, prefix=\"validation\"),\n",
        "    QM9LRSchedulerCallback(logger, epochs=args.epochs),\n",
        "]\n",
        "logger.log_hyperparams(vars(args))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b815199",
      "metadata": {},
      "source": [
        "### Train\n",
        "\n",
        "With everything configured, youâ€™re ready to kick off training. The `train()` function orchestrates the entire training loop â€” running forward and backward passes, computing losses, updating parameters, and periodically evaluating the validation set. It uses the dataloaders, callbacks, and logger you set up earlier to track progress, log metrics, and manage learning rate schedules throughout the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f999ffb",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "if not USE_PRETRAINED:\n",
        "    train(\n",
        "        model,\n",
        "        loss_fn,\n",
        "        datamodule.train_dataloader(),\n",
        "        datamodule.val_dataloader(),\n",
        "        callbacks,\n",
        "        logger,\n",
        "        args,\n",
        "    )\n",
        "else:\n",
        "    print(\"â­ï¸  Skipping training (USE_PRETRAINED=True)\")\n",
        "    print(\"ðŸ“Š Will load pretrained checkpoint for inference\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd1f8f60",
      "metadata": {},
      "source": [
        "### Visualizing the training progress\n",
        "After training, you can visualize and analyze the logged results. Import Plotly for interactive plotting and `dllogger` to access the saved training logs. Flushing the logger ensures all metrics have been written to disk before loading them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1114e35a",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "if not USE_PRETRAINED:\n",
        "    import plotly.graph_objects as go\n",
        "    import plotly.io as pio\n",
        "    import pandas as pd\n",
        "    from plotly.subplots import make_subplots\n",
        "    import json\n",
        "    import dllogger\n",
        "    import os\n",
        "\n",
        "    # If we're loading a checkpoint, we need to use the saved log file\n",
        "    # otherwise, we'll use the current log file\n",
        "    if args.load_ckpt_path is not None:\n",
        "        LOG_FILE = os.path.join(\"results\", \"dllogger_results_100.json\")\n",
        "        if not os.path.exists(LOG_FILE):\n",
        "            raise FileNotFoundError(f\"Log file {LOG_FILE} does not exist, please copy the log file to the results directory or turn off checkpoint loading\")\n",
        "    else:\n",
        "        LOG_FILE = os.path.join(\"results\", args.dllogger_name)\n",
        "        dllogger.flush()\n",
        "\n",
        "    print(f\"Using log file: {LOG_FILE}\")\n",
        "    pio.renderers.default = \"notebook\"\n",
        "else:\n",
        "    print(\"â­ï¸  Skipping training visualization (no training was performed)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f66b2fc9",
      "metadata": {},
      "source": [
        "This step parses and organizes the logged training data from `dllogger_results.json`. It reads the file line by line, cleans up any malformed entries, and filters out records without valid steps. Each log entry is then grouped by its training step, extracting key metrics such as training loss, learning rate, and validation mean absolute error (MAE). The results are compiled into a tidy Pandas DataFrame, making it easier to visualize and analyze how model performance and learning dynamics evolved throughout training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a89c47",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "if not USE_PRETRAINED:\n",
        "    # Read and parse the data\n",
        "    with open(LOG_FILE, \"r\") as f:\n",
        "        logs = [json.loads(line.replace(\"DLLL\", \"\")) for line in f.readlines()]\n",
        "\n",
        "    # Filter out entries where step is an empty list\n",
        "    logs = [log for log in logs if log.get(\"step\") != []]\n",
        "\n",
        "    # Create a dictionary to aggregate metrics by step\n",
        "    metrics_by_step = {}\n",
        "\n",
        "    for log in logs:\n",
        "        if log.get(\"type\") == \"LOG\":\n",
        "            step = log.get(\"step\")\n",
        "\n",
        "            # Skip if step is not an integer or if it's the PARAMETER step\n",
        "            if not isinstance(step, int):\n",
        "                continue\n",
        "\n",
        "            # Initialize the step if not exists\n",
        "            if step not in metrics_by_step:\n",
        "                metrics_by_step[step] = {\n",
        "                    \"step\": step,\n",
        "                    \"train loss\": None,\n",
        "                    \"learning rate\": None,\n",
        "                    \"validation MAE\": None,\n",
        "                }\n",
        "\n",
        "            # Update metrics for this step\n",
        "            data = log.get(\"data\", {})\n",
        "            if \"train loss\" in data:\n",
        "                metrics_by_step[step][\"train loss\"] = data[\"train loss\"]\n",
        "            if \"learning rate\" in data:\n",
        "                metrics_by_step[step][\"learning rate\"] = data[\"learning rate\"]\n",
        "            if \"validation MAE\" in data:\n",
        "                metrics_by_step[step][\"validation MAE\"] = data[\"validation MAE\"]\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(list(metrics_by_step.values()))\n",
        "    df = df.sort_values(\"step\").reset_index(drop=True)\n",
        "\n",
        "    print(df)\n",
        "else:\n",
        "    print(\"â­ï¸  Skipping training visualization (no training was performed)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d94b5794",
      "metadata": {},
      "source": [
        "To get a clear picture of how training evolved, plot the key metrics over epochs using Plotly. The figure below displays training loss, validation MAE, and learning rate in separate subplots, making it easy to observe the modelâ€™s convergence and learning dynamics. Ideally, you should see the training loss and validation MAE steadily decreasing as the learning rate adjusts, giving quick visual confirmation that training progressed smoothly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7b860b4",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "if not USE_PRETRAINED:\n",
        "    # Create subplots\n",
        "    fig = make_subplots(\n",
        "        rows=3,\n",
        "        cols=1,\n",
        "        subplot_titles=(\"Train Loss\", \"Validation MAE\", \"Learning Rate\"),\n",
        "        vertical_spacing=0.08,\n",
        "    )\n",
        "\n",
        "    # Train Loss\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=df[\"step\"],\n",
        "            y=df[\"train loss\"],\n",
        "            mode=\"lines+markers\",\n",
        "            name=\"Train Loss\",\n",
        "            line=dict(color=\"blue\"),\n",
        "        ),\n",
        "        row=1,\n",
        "        col=1,\n",
        "    )\n",
        "\n",
        "    # Validation MAE\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=df[\"step\"],\n",
        "            y=df[\"validation MAE\"],\n",
        "            mode=\"lines+markers\",\n",
        "            name=\"Validation MAE\",\n",
        "            line=dict(color=\"red\"),\n",
        "        ),\n",
        "        row=2,\n",
        "        col=1,\n",
        "    )\n",
        "\n",
        "    # Learning Rate\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=df[\"step\"],\n",
        "            y=df[\"learning rate\"],\n",
        "            mode=\"lines+markers\",\n",
        "            name=\"Learning Rate\",\n",
        "            line=dict(color=\"green\"),\n",
        "        ),\n",
        "        row=3,\n",
        "        col=1,\n",
        "    )\n",
        "\n",
        "    fig.update_xaxes(title_text=\"Epoch\", row=3, col=1)\n",
        "    fig.update_layout(height=1000, showlegend=False, title_text=\"SE(3) Training\")\n",
        "    fig.show()\n",
        "else:\n",
        "    print(\"â­ï¸  Skipping training visualization (no training was performed)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99a9a9a0",
      "metadata": {},
      "source": [
        "Here's an example of what the output graph might look like:\n",
        "\n",
        "![training graph](../assets/training_progress_graph.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "inference_section_header",
      "metadata": {},
      "source": [
        "## Part B: Inference and evaluation\n",
        "\n",
        "This section loads a trained model checkpoint and evaluates it on the test set.\n",
        "\n",
        "- If `USE_PRETRAINED = True`: Loads the 100-epoch pretrained model.\n",
        "- If `USE_PRETRAINED = False`: Loads the model you just trained for five epochs.\n",
        "\n",
        "### Set up inference configuration\n",
        "\n",
        "For inference, the model runs a forward-only pass. This means no gradients are computed, and the focus is on making predictions rather than updating weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d7d4fc0",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Get the major and minor compute capability of the current CUDA device\n",
        "major_cc, minor_cc = torch.cuda.get_device_capability()\n",
        "print(f\"CUDA Compute Capability: {major_cc}.{minor_cc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fd1d454",
      "metadata": {},
      "source": [
        "### Set up inference arguments\n",
        "\n",
        "If your SE(3)-Transformer code uses argparse to manage configurations, you can simulate command-line arguments in the notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a70a2e5e",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "args_inference = PARSER.parse_args([\n",
        "    \"--amp\",                # Enable automatic mixed precision (faster inference)\n",
        "    \"true\",\n",
        "    \"--batch_size\",         # Number of molecules to process at once\n",
        "    \"240\",\n",
        "    \"--use_layer_norm\",     # Enable layer normalization\n",
        "    \"--norm\",               # Use normalization in the model\n",
        "    \"--load_ckpt_path\",     # Path to the trained model checkpoint\n",
        "    CHECKPOINT_PATH,\n",
        "    \"--task\",               # Prediction task (e.g., HOMO/LUMO energies)\n",
        "    \"homo\",\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eae35b02",
      "metadata": {},
      "source": [
        "### Get local GPU info\n",
        "\n",
        "Before running inference, check the GPU and prepare the device-specific settings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a74c1e7f",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from se3_transformer.runtime.utils import init_distributed, get_local_rank\n",
        "\n",
        "# Initialize distributed utilities (still works for single-GPU)\n",
        "is_distributed = init_distributed()  # False for single-GPU\n",
        "local_rank = get_local_rank()        # GPU index, usually 0\n",
        "print(f\"Running on GPU: {local_rank}, Distributed: {is_distributed}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7c03047-45b6-4e6c-8096-d8a4e5e900de",
      "metadata": {},
      "source": [
        "### Initialize the model\n",
        "\n",
        "Create the SE3Transformer for inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9c260656",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from se3_transformer.model import SE3TransformerPooled, Fiber\n",
        "\n",
        "model = SE3TransformerPooled(\n",
        "    fiber_in=Fiber({0: datamodule.NODE_FEATURE_DIM}),         # Node feature dimensions\n",
        "    fiber_out=Fiber({0: args.num_degrees * args.num_channels}),  # Output fiber dimensions\n",
        "    fiber_edge=Fiber({0: datamodule.EDGE_FEATURE_DIM}),      # Edge features\n",
        "    output_dim=1,                                            # Single target prediction\n",
        "    tensor_cores=(args.amp and major_cc >= 7) or major_cc >= 8, # Use tensor cores if available\n",
        "    **vars(args)                                             # Other parser arguments\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa2120af",
      "metadata": {},
      "source": [
        "### Set up evaluation callbacks\n",
        "This computes relevant QM9 metrics during inference and uses dataset normalization to scale predictions properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "00fc47c1",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    QM9MetricCallback(logger, targets_std=datamodule.targets_std, prefix='test'),\n",
        "    QM9LRSchedulerCallback(logger, epochs=args.epochs),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd890dac",
      "metadata": {},
      "source": [
        "### Load the pretrained checkpoint\n",
        "\n",
        "The following code loads the checkpoint you previously downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d641498f",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "print(f\"ðŸ“‚ Loading checkpoint: {CHECKPOINT_PATH}\")\n",
        "\n",
        "import torch\n",
        "\n",
        "checkpoint = torch.load(\n",
        "    str(args_inference.load_ckpt_path),\n",
        "    map_location=f'cuda:{local_rank}',  # Map weights to the active GPU\n",
        "    weights_only=True                   # Only load model parameters\n",
        ")\n",
        "\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "torch.set_float32_matmul_precision('high')\n",
        "test_dataloader = datamodule.test_dataloader()\n",
        "device = torch.cuda.current_device()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39971ca7",
      "metadata": {},
      "source": [
        "After this step, the model is ready for inference."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fea996f5",
      "metadata": {},
      "source": [
        "### Running evaluation on the test set\n",
        "\n",
        "After the model is loaded and ready, you can run evaluation on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37343539",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from se3_transformer.runtime.training import evaluate\n",
        "# Run the evaluation function\n",
        "evaluate(model, test_dataloader, callbacks, args_inference)\n",
        "\n",
        "# Trigger the 'on_validation_end' hook for all callbacks\n",
        "for callback in callbacks:\n",
        "    callback.on_validation_end()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8340828",
      "metadata": {},
      "source": [
        "## Post-inference analysis\n",
        "\n",
        "Referring to the previous example, you can now inspect the molecule, examine its regression targets, and compare it with the prediction.\n",
        "\n",
        "![Molecular structure](../assets/molecule.png)\n",
        "\n",
        "To obtain these values, run separate training and inference sessions for each task, specifying the `--task` argument as `homo`, `lumo`, or `gap`.\n",
        "\n",
        "**HOMO (Highest Occupied Molecular Orbital) energy**\n",
        "```\n",
        "TARGET: tensor([1.2334])\n",
        "PREDICTION: tensor([1.2236], dtype=torch.float16)\n",
        "```\n",
        "\n",
        "**LUMO (Lowest Unoccupied Molecular Orbital) energy**\n",
        "```\n",
        "TARGET : tensor([0.5246])\n",
        "PREDICTION : tensor([0.5127], dtype=torch.float16)\n",
        "```\n",
        "**Gap (Energy difference between HOMO and LUMO)**\n",
        "```\n",
        "TARGET: tensor([-0.0547])\n",
        "PRED: tensor([-0.1097], dtype=torch.float16)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b2598ce",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook walked you through the end-to-end workflow for training and evaluating an SE(3)-Transformer model on the QM9 molecular dataset. You explored how to set up training configurations originally designed for CLI use, adapted them for an interactive Jupyter workflow, and visualized molecules directly from graph data to validate preprocessing. You then built and trained the SE(3)-Transformer, logged its performance, and used interactive plots to analyze key metrics like loss, MAE, and learning rate over time.\n",
        "\n",
        "With the workflow now validated, this setup provides a strong foundation for scaling up experiments, benchmarking performance, and adapting the SE(3)-Transformer to more complex or domain-specific datasets."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
