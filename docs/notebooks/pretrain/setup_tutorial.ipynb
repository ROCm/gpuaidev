{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining with Megatron-LM \n",
    "\n",
    "This tutorial walks you through the setup and configuration required to pretrain large-scale language models such as Llama-2 and Llama-3 using AMD’s ROCm Megatron-LM framework.\n",
    "\n",
    "The ROCm Megatron-LM framework is a specialized fork of Megatron-LM designed to train large-scale language models efficiently on AMD GPUs. By leveraging AMD Instinct™ MI300X accelerators, this framework offers:\n",
    "\n",
    "* Enhanced scalability and performance for AI workloads.\n",
    "* Support for powerful and popular models such as Llama-2, Llama-3, and Llama-3.1.\n",
    "\n",
    "Key features include:\n",
    "\n",
    "* **Transformer Engine (TE)**: Optimized transformer layer implementations.\n",
    "* **Flash Attention 2.0**: Faster and memory-efficient attention mechanisms.\n",
    "* **3D Parallelism (TP + SP + CP)**: Tensor, pipeline, and sequence parallelism.\n",
    "* **Fused Kernels**: For optimized training operations.\n",
    "* **GEMM Tuning**: Automatically selects optimal matrix multiplication kernels.\n",
    "\n",
    "Pre-optimized models include:\n",
    "\n",
    "* **Llama-2**: 7B and 70B\n",
    "* **Llama-3 / Llama-3.1**: 8B and 70B\n",
    "\n",
    "See the [Megatron-LM GitHub repository](https://github.com/ROCm/Megatron-LM) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup. \n",
    "\n",
    "### Operating system\n",
    "\n",
    "* **Ubuntu 22.04**: Ensure your system is running Ubuntu version 22.04.\n",
    "\n",
    "### Hardware\n",
    "\n",
    "* **AMD Instinct GPUs**:  This tutorial was tested on an AMD Instinct MI300X GPU. Ensure you are using an AMD Instinct GPU or compatible hardware with ROCm support and that your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "### Software\n",
    "\n",
    "* **ROCm 6.2**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html). After installation, confirm your setup using:\n",
    "\n",
    "    ``` bash\n",
    "    amd-smi\n",
    "    ```\n",
    "\n",
    "    This command lists your AMD GPUs with relevant details.\n",
    "    \n",
    "    **Note**: For ROCm 6.4 and earlier, use the `rocm-smi` command instead.\n",
    "\n",
    "* **Docker**: Ensure Docker is installed and configured correctly. Follow the Docker installation guide for your operating system.\n",
    "\n",
    "   **Note**: Ensure the Docker permissions are correctly configured. To configure permissions to allow non-root access, run the following commands:\n",
    "\n",
    "   ``` bash\n",
    "   sudo usermod -aG docker $USER\n",
    "   newgrp docker\n",
    "   ```\n",
    "\n",
    "   Verify Docker is working correctly:\n",
    "\n",
    "   ``` bash\n",
    "   docker run hello-world\n",
    "   ```\n",
    "\n",
    "\n",
    "### Hugging Face API access\n",
    "- Obtain an API token from [Hugging Face](https://huggingface.co) for downloading models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6842aa21",
   "metadata": {},
   "source": [
    "## RCCL bandwidth test\n",
    "\n",
    "ROCm Collective Communications Library (RCCL) is a standalone library of standard collective communication routines for GPUs. See the [RCCL documentation](https://rocm.docs.amd.com/projects/rccl/en/latest/index.html) for more information. Running a RCCL bandwidth test before starting pre-training helps ensure that the multi-GPU or multi-node setup is optimized for efficient distributed training.\n",
    "\n",
    "To run these tests, first change to the `rccl-tests` directory.\n",
    "\n",
    "``` bash\n",
    "cd ~/rccl-tests\n",
    "```\n",
    "\n",
    "**Test 1**\n",
    "\n",
    "This test runs on 8 GPUs (`-g 8`), scanning from 8 bytes to 10 GB:\n",
    "\n",
    "``` bash\n",
    "./build/all_reduce_perf -b 8 -e 10G -f 2 -g 8\n",
    "```\n",
    "\n",
    "After the script completes, the last two lines of the output should look similar to this:\n",
    "\n",
    "``` bash\n",
    "# Out of bounds values : 0 OK\n",
    "# Avg bus bandwidth    : 102.428\n",
    "```\n",
    "\n",
    "The `Avg bus bandwidth` should be approximately 100gb/sec because the MI300x uses 8x PCIe 5.0 high-performance networking cards.\n",
    "\n",
    "**Test 2**\n",
    "\n",
    "It's recommended to use one MPI process per GPU and `-g 1` for performance-oriented runs on both single-node and multi-node configurations. The command to run this test on 8 GPUs is similar to this:\n",
    "\n",
    "``` shell\n",
    "mpirun -np 8 --bind-to numa ./build/all_reduce_perf -b 8 -e 10G -f 2 -g 1\n",
    "```\n",
    "\n",
    "After the script completes, the last two lines of the output should look similar to this:\n",
    "\n",
    "```bash\n",
    "# Out of bounds values : 0 OK\n",
    "# Avg bus bandwidth    : 110.537\n",
    "```\n",
    "\n",
    "**Test 3**\n",
    "\n",
    "This test runs on multiple nodes. Use the following script to run the RCCL test on four MI300X GPU nodes. Modify the paths and node addresses as required.\n",
    "\n",
    "```shell\n",
    "/home/$USER/ompi_for_gpu/ompi/bin/mpirun -np 32 -H tw022:8,tw024:8,tw010:8, tw015:8 \\\n",
    "--mca pml ucx \\\n",
    "--mca btl ^openib \\\n",
    "-x NCCL_SOCKET_IFNAME=ens41np0 \\\n",
    "-x NCCL_IB_HCA=rdma0:1,rdma1:1,rdma2:1,rdma3:1,rdma4:1,rdma5:1,rdma6:1,rdma7:1 \\\n",
    "-x NCCL_IB_GID_INDEX=3 \\\n",
    "-x NCCL_MIN_NCHANNELS=40 \\\n",
    "-x NCCL_DEBUG=version \\\n",
    "$HOME/rccl-tests/build/all_reduce_perf -b 8 -e 8g -f 2 -g 1\n",
    "```\n",
    "\n",
    "After the script completes, the last two lines of the output should look similar to this:\n",
    "\n",
    "``` bash\n",
    "# Out of bounds values : 0 OK\n",
    "# Avg bus bandwidth    : 94.3037\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf2b8d",
   "metadata": {},
   "source": [
    "## Prepare the training environment\n",
    "\n",
    "After your system meets the prerequisites, follow these steps to set up the training environment.\n",
    "\n",
    "### 1. System configuration\n",
    "\n",
    "To maximize performance, follow the recommended steps below:\n",
    "\n",
    "* **Disable NUMA auto-balancing**\n",
    "\n",
    "   Disabling NUMA auto-balancing can improve application performance. Learn more about the effects of NUMA auto-balancing [here](https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing). \n",
    "\n",
    "   Check the current NUMA setting:\n",
    "\n",
    "   ```bash\n",
    "   cat /proc/sys/kernel/numa_balancing\n",
    "   ```\n",
    "\n",
    "   The NUMA settings are as follows:\n",
    "\n",
    "   * `0`: Disabled\n",
    "   * `1`: Enabled\n",
    "\n",
    "   If necessary, disable NUMA auto-balancing by setting it to `0` using this command:\n",
    "\n",
    "   ```bash\n",
    "   sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pull the Docker image\n",
    "\n",
    "Run the following command in your terminal to pull the prebuilt Docker image. The Docker image contains all the necessary dependencies, including PyTorch, PyTorch Lightning, the ROCm libraries, and Megatron-LM utilities.\n",
    "\n",
    "``` bash\n",
    "docker pull rocm/megatron-lm:24.12-dev\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480cb66",
   "metadata": {},
   "source": [
    "### 3. Launch the Docker container\n",
    "\n",
    "Run the following command in your terminal to launch the Docker container with the appropriate configuration:\n",
    "\n",
    "``` bash\n",
    "docker run -it --rm \\\n",
    "  --device /dev/dri \\\n",
    "  --device /dev/kfd \\\n",
    "  --network host \\\n",
    "  --ipc host \\\n",
    "  --group-add video \\\n",
    "  --cap-add SYS_PTRACE \\\n",
    "  --security-opt seccomp=unconfined \\\n",
    "  --privileged \\\n",
    "  --name megatron-dev-env \\\n",
    "  -v $(pwd):/workspace \\\n",
    "  -w /workspace/notebooks \\\n",
    "  rocm/megatron-lm:24.12-dev \\\n",
    "  /bin/bash\n",
    "```\n",
    "\n",
    "**Note**: This command mounts the current directory to the `/workspace` directory in the container. Ensure the notebook file is either copied to this directory before running the Docker command or uploaded into the Jupyter Notebook environment after it starts. Save the token or URL provided in the terminal output to access the notebook from your web browser. You can download this notebook from the [AI Developer Hub GitHub repository](https://github.com/ROCm/gpuaidev-docs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded86c89",
   "metadata": {},
   "source": [
    "### 4. Install Jupyter and start the server\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "``` bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Start the Jupyter server:\n",
    "\n",
    "``` bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "**Note**: Ensure that port `8888` is not already in use on your system before running the above command. If it is, specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Clone the Megatron-LM repository\n",
    "Run the following commands inside the Docker container to clone the Megatron-LM repository and navigate to the validated commit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6146418f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Clone the Megatron-LM repository and navigate to the validated commit\n",
    "!git clone https://github.com/ROCm/Megatron-LM && cd Megatron-LM && git checkout bb93ccbfeae6363c67b361a97a27c74ab86e7e92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the training dataset\n",
    "\n",
    "While the next tutorial uses \"mock data\" to simplify the tutorial, this section covers an example showing how to preprocess your data for training if you don’t have preprocessed data. This section uses the [BookCorpus dataset](https://huggingface.co/datasets/bookcorpus/bookcorpus), which is a collection of books that has been used for training language models. It contains diverse and continuous text passages, making it suitable for pretraining tasks.\n",
    "\n",
    "In this tutorial, you will:\n",
    "\n",
    "* Download and inspect the BookCorpus dataset.\n",
    "* Convert it to JSONL format.\n",
    "* Download the necessary tokenizer files (`vocab.json` and `merges.txt`).\n",
    "* Preprocess the data for training a large language model with Megatron-LM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download and inspect the BookCorpus dataset\n",
    "\n",
    "Use the Hugging Face datasets library to download the BookCorpus dataset. This step ensures you have access to the raw data needed for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c15676",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load BookCorpus dataset\n",
    "dataset = load_dataset(\"bookcorpus/bookcorpus\", trust_remote_code=True, split=\"train\")\n",
    "\n",
    "# Inspect the dataset\n",
    "print(\"Dataset Structure:\", dataset)\n",
    "print(\"Sample Data:\", dataset[0])  # Access the first record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5228f6",
   "metadata": {},
   "source": [
    "### 2. Convert to the JSONL format\n",
    "\n",
    "Megatron-LM's preprocessing script requires that the input be in JSONL format, where each line represents a document as a JSON object. This step converts the dataset into the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674e4cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "output_file = \"bookcorpus.jsonl\"\n",
    "\n",
    "# Open the output file\n",
    "with open(output_file, \"w\") as f:\n",
    "    # Use tqdm to display progress\n",
    "    for record in tqdm(dataset, desc=\"Saving dataset to JSONL\", unit=\"record\"):\n",
    "        json.dump({\"text\": record[\"text\"]}, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f2b32",
   "metadata": {},
   "source": [
    "Before moving to the next step, ensure that the dataset is correctly converted to JSONL format. Inspect the first few lines to confirm the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2d44e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect the first few lines of the JSONL file\n",
    "with open(output_file, \"r\") as f:\n",
    "    for i in range(5):  # Print the first 5 lines\n",
    "        print(json.loads(f.readline()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61173bc0",
   "metadata": {},
   "source": [
    "### 3. Download the tokenizer files\n",
    "\n",
    "Decide which tokenizer you are going to use for your training. As an example, you might pick the GPT-2 Byte Pair Encoding (BPE) tokenizer. The preprocessing tool provided by Megatron requires some additional files for each tokenizer. In the case of BPE, download the following two files that define the tokenizer rules:\n",
    "\n",
    "* `vocab.json`: Maps the tokens to unique IDs.\n",
    "* `merges.txt`: Specifies how subword units are combined into tokens.\n",
    "\n",
    "Download these files to tokenize the dataset correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69e72a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Download vocab.json and merges.txt for GPT-2 tokenizer\n",
    "!wget https://huggingface.co/gpt2/resolve/main/vocab.json -O vocab.json\n",
    "!wget https://huggingface.co/gpt2/resolve/main/merges.txt -O merges.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a33c8",
   "metadata": {},
   "source": [
    "### 4. Preprocess the data\n",
    "\n",
    "This step tokenizes the dataset and converts it into binary and index files suitable for training a large language model using Megatron-LM. Use the Megatron-LM preprocessing script with the converted JSONL dataset and tokenizer files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b4da1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p output\n",
    "!python Megatron/tools/preprocess_data.py \\\n",
    "    --input bookcorpus.jsonl \\\n",
    "    --json-keys text \\\n",
    "    --output-prefix output/bookcorpus \\\n",
    "    --tokenizer-type GPT2BPETokenizer \\\n",
    "    --vocab-file vocab.json \\\n",
    "    --merge-file merges.txt \\\n",
    "    --workers 4 \\\n",
    "    --append-eod \\\n",
    "    --partitions 2 \\\n",
    "    --log-interval 1000000 \\\n",
    "    --split-sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b23d2e",
   "metadata": {},
   "source": [
    "**Note:** You might need to modify these parameters based on the tokenizer you pick and your dataset. \n",
    "\n",
    "Now check the output files generated during preprocessing to ensure everything is processed correctly and ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b95cdc8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# List output files\n",
    "!ls output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Configure the network interfaces (only applicable for multi-node training)\n",
    "\n",
    "You must set the `NCCL_SOCKET_IFNAME` and `GLOO_SOCKET_IFNAME` variables. While this is easily accomplished using the `export` command, this notebook can automatically set the network interface variables.\n",
    "\n",
    "First, install the `iproute2` package by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e2103a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!apt install -y iproute2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd430f4c",
   "metadata": {},
   "source": [
    "Then run the following commands to automatically detect the active network interfaces and set the environment variables based on the first available interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a394b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Detect the active network interface\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        \"ip -o link show | awk '{print $2, $9}' | grep 'UP' | awk '{print $1}' | sed 's/://g' | head -n 1\",\n",
    "        shell=True,\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    active_interface = result.stdout.strip()\n",
    "\n",
    "    # Set environment variables\n",
    "    os.environ['NCCL_SOCKET_IFNAME'] = active_interface\n",
    "    os.environ['GLOO_SOCKET_IFNAME'] = active_interface\n",
    "\n",
    "    # Verify the variables\n",
    "    print(f\"NCCL_SOCKET_IFNAME is set to: {os.environ['NCCL_SOCKET_IFNAME']}\")\n",
    "    print(f\"GLOO_SOCKET_IFNAME is set to: {os.environ['GLOO_SOCKET_IFNAME']}\")\n",
    "\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error detecting network interface: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07f23b4",
   "metadata": {},
   "source": [
    "After running the commands, verify the active network interface using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17a752d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!ip a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21401cda",
   "metadata": {},
   "source": [
    "Ensure the detected interface matches your system's active network interface. If necessary, modify the script above or manually set the `NCCL_SOCKET_IFNAME` and `GLOO_SOCKET_IFNAME` variables. You can manually set these variables using the directives below:\n",
    "\n",
    "``` bash\n",
    "export NCCL_SOCKET_IFNAME=<network_interface>\n",
    "export GLOO_SOCKET_IFNAME=<network_interface>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb008445",
   "metadata": {},
   "source": [
    "**Important**: For multi-node training, ensure the following requirements are met:\n",
    "\n",
    "* Step 5 must be executed on **all nodes** participating in the training.  \n",
    "* The environment variables `NCCL_SOCKET_IFNAME` and `GLOO_SOCKET_IFNAME` must be set to the **same value** across all nodes to ensure consistent network communication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configuration\n",
    "\n",
    "Before launching a training task, it's crucial to configure the training environment properly. This section covers the essential configurations for running pretraining, including:\n",
    "\n",
    "* The training mode (single or multi-node training)\n",
    "* Dataset options\n",
    "* Tokenizer selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f6f3c5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Training modes\n",
    "\n",
    "The two training modes are single-node training and multi-node training.\n",
    "\n",
    "#### Single-node training\n",
    "\n",
    "In single-node training, all computations occur on a single server or machine with multiple GPUs. This is simpler to set up and sufficient for smaller-scale experiments.\n",
    "\n",
    "To launch training on a single node, use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac4d7c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!TEE_OUTPUT=1 MBS=2 BS=64 TP=8 TE_FP8=0 SEQ_LENGTH=4096 bash examples/llama/train_llama2.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf6c77a",
   "metadata": {},
   "source": [
    "The variables in the previous command are:\n",
    "\n",
    "* `TEE_OUTPUT=1`: Enables output streaming to all nodes.\n",
    "* `MBS=2`: Sets the micro-batch size.\n",
    "* `BS=64`: Sets the global batch size.\n",
    "* `TP=8`: Configures tensor parallelism with 8-way parallelism.\n",
    "* `TE_FP8=0`: Disables FP8 optimizations (set this to `1` to enable).\n",
    "* `SEQ_LENGTH=4096`: Specifies the maximum sequence length for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b61db6",
   "metadata": {},
   "source": [
    "#### Multi-node training\n",
    "\n",
    "For larger-scale training, you can distribute the workload across multiple nodes. Multi-node training requires additional configuration to enable communication between nodes.\n",
    "\n",
    "Before running the training script, update the following environment variables:\n",
    "\n",
    "**Master node address**: Specify the hostname or IP address of the primary node.\n",
    "\n",
    "```bash\n",
    "MASTER_ADDR=\"${MASTER_ADDR:-localhost}\"\n",
    "```\n",
    "\n",
    "**Number of nodes**:  Define the total number of nodes.\n",
    "\n",
    "``` bash\n",
    "NNODES=\"${NNODES:-1}\"\n",
    "```\n",
    "\n",
    "**Node rank**: Assign a unique rank to each node, as follows:\n",
    "\n",
    "* `0` for the primary node.\n",
    "* `1`, `2`, etc., for the worker nodes.\n",
    "\n",
    "``` bash\n",
    "NODE_RANK=\"${NODE_RANK:-0}\"\n",
    "```\n",
    "\n",
    "### Run the training script\n",
    "\n",
    "Execute the training script on all nodes using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d097e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!TEE_OUTPUT=1 MBS=2 BS=64 TP=8 TE_FP8=0 SEQ_LENGTH=4096 bash examples/llama/train_llama2.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e278aa",
   "metadata": {},
   "source": [
    "**Important:** For multi-node training, ensure you run all the previous steps on all the nodes and modify the `NODE_RANK` on each node as described above.\n",
    "\n",
    "**Tip**: Test multi-node communication with a mock training task before launching full-scale training. This helps debug any node communication or dataset path issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset options\n",
    "\n",
    "The dataset is a critical component of pretraining. You can use either real data or mock data based on your requirements.\n",
    "\n",
    "#### Using real data\n",
    "\n",
    "To use a real dataset:\n",
    "\n",
    "1. Update the `DATA_PATH` variable to point to the location of your dataset.\n",
    "\n",
    "   ``` bash\n",
    "   DATA_DIR=\"/root/.cache/data\"  # Directory where your dataset is stored\n",
    "   DATA_PATH=${DATA_DIR}/bookcorpus_text_sentence\n",
    "   ```\n",
    "\n",
    "2. Pass this variable to the training script:\n",
    "\n",
    "   ``` bash\n",
    "   --data-path $DATA_PATH\n",
    "   ```\n",
    "\n",
    "   **Note**: Ensure the dataset files are accessible inside the Docker container.\n",
    "\n",
    "#### Using mock data\n",
    "\n",
    "To use mock data, pass this variable:\n",
    "\n",
    "``` bash\n",
    "--mock-data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38e7028",
   "metadata": {},
   "source": [
    "### Tokenizer selection\n",
    "\n",
    "Tokenization is the process of converting raw text into tokens that the model can process. Different Llama models require specific tokenizers:\n",
    "\n",
    "#### For Llama-2 models:\n",
    "\n",
    "Use the `Llama2Tokenizer`.\n",
    "\n",
    "#### For Llama-3 and Llama-3.1 models:\n",
    "\n",
    "Use the `HuggingFaceTokenizer`. Set the Hugging Face model link in the `TOKENIZER_MODEL` variable. For example:\n",
    "\n",
    "``` bash\n",
    "TOKENIZER_MODEL=meta-llama/Llama-3.1-8B\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Proceed to the [Training Llama-3.1 8B with Megatron-LM](https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/pretrain/train_llama_mock_data.html) guide, where you will:\n",
    "\n",
    "* Use the environment and configurations set up in this tutorial.\n",
    "* Run practical pretraining examples using mock data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
