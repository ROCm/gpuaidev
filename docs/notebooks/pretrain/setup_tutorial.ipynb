{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Pretraining with Megatron-LM\n",
    "\n",
    "This tutorial walks you through the setup and configuration required to pretrain large-scale language models such as **Llama2** and **Llama3** using AMD’s ROCm Megatron-LM framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "\n",
    "The **ROCm Megatron-LM framework** is a specialized fork of Megatron-LM designed to train large-scale language models efficiently on AMD GPUs. By leveraging AMD **Instinct™ MI300X accelerators**, this framework offers:\n",
    "- Enhanced scalability and performance for AI workloads.\n",
    "- Support for state-of-the-art models such as **Llama2**, **Llama3**, and **Llama3.1**.\n",
    "\n",
    "Key features include:\n",
    "- **Transformer Engine (TE):** Optimized transformer layer implementations.\n",
    "- **Flash Attention 2.0:** Faster and memory-efficient attention mechanisms.\n",
    "- **3D Parallelism (TP + SP + CP):** Tensor, pipeline, and sequence parallelism.\n",
    "- **Fused Kernels:** For optimized training operations.\n",
    "- **GEMM Tuning:** Automatically selects optimal matrix multiplication kernels.\n",
    "\n",
    "**Pre-Optimized Models:**\n",
    "- **Llama2:** 7B and 70B\n",
    "- **Llama3 / Llama3.1:** 8B and 70B\n",
    "\n",
    "> **See the [GitHub repository](https://github.com/ROCm/Megatron-LM) for more details.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. System Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Disable NUMA Auto-Balancing\n",
    "Disabling NUMA auto-balancing can improve application performance.\n",
    "1. Check your current NUMA setting:\n",
    "```bash\n",
    "cat /proc/sys/kernel/numa_balancing\n",
    "```\n",
    "- `0`: Disabled\n",
    "- `1`: Enabled\n",
    "2. Disable NUMA auto-balancing if necessary:\n",
    "```bash\n",
    "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Hardware Verification with ROCm\n",
    "To ensure stable performance:\n",
    "1. Set GPU clocks to a stable maximum of **1900 MHz**:\n",
    "```bash\n",
    "rocm-smi --setperfdeterminism 1900\n",
    "```\n",
    "2. To reset this setting to default:\n",
    "```bash\n",
    "rocm-smi -r\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Test Communication (Optional RCCL Bandwidth Test) \n",
    "Is this required?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Download and Launch Docker\n",
    "The Docker image provides all necessary dependencies, including **PyTorch**, **PyTorch Lightning**, **ROCm libraries**, and **Megatron-LM utilities**.\n",
    "\n",
    "1. **Pull the Docker image**:\n",
    "```bash\n",
    "docker pull rocm/megatron-lm:24.12-dev\n",
    "```\n",
    "2. **Launch the container**:\n",
    "```bash\n",
    "docker run -it --device /dev/dri --device /dev/kfd --network host --ipc host \\\n",
    "    --group-add video --cap-add SYS_PTRACE --security-opt seccomp=unconfined \\\n",
    "    --privileged -v $CACHE_DIR:/root/.cache --name megatron-dev-env rocm/megatron-lm:24.12-dev /bin/bash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded86c89",
   "metadata": {},
   "source": [
    "### 3.2 Install Jupyter and Start the Server\n",
    "After launching the Docker container, run the following commands inside the container to install Jupyter and start the notebook server:\n",
    "```bash\n",
    "pip install jupyter\n",
    "jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root\n",
    "```\n",
    "**Note:** Jupyter installation inside Docker is necessary to execute notebook cells. Save the token or URL provided in the terminal output to access the notebook from your host machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Clone and Configure Megatron-LM\n",
    "Run the following commands inside the Docker container to clone the Megatron-LM repository and navigate to the validated commit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6146418f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Clone the Megatron-LM repository and navigate to the validated commit\n",
    "!git clone https://github.com/ROCm/Megatron-LM && cd Megatron-LM && git checkout bb93ccbfeae6363c67b361a97a27c74ab86e7e92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing Training Dataset\n",
    "While in the next tutorial we use `mock data` in order to simplify the tutorial, this section covers an example of how to preprocess your data for training, If you don’t have preprocessed data. In this section we are going to use the [BookCorpus dataset](https://huggingface.co/datasets/bookcorpus/bookcorpus). The BookCorpus dataset is a collection of books that has been used for training language models. It contains diverse and continuous text passages, making it suitable for pretraining tasks.\n",
    "\n",
    "In this tutorial, we will:\n",
    "\n",
    "- Download and inspect the BookCorpus dataset.\n",
    "- Convert it to a JSONL format.\n",
    "- Download necessary tokenizer files (vocab.json and merges.txt).\n",
    "- Preprocess the data for training a large language model with Megatron-LM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Download and Inspect The BookCorpus Dataset\n",
    "\n",
    "We are using the Hugging Face datasets library to download the BookCorpus dataset. This step ensures we have access to the raw data needed for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c15676",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load BookCorpus dataset\n",
    "dataset = load_dataset(\"bookcorpus/bookcorpus\", trust_remote_code=True, split=\"train\")\n",
    "\n",
    "# Inspect the dataset\n",
    "print(\"Dataset Structure:\", dataset)\n",
    "print(\"Sample Data:\", dataset[0])  # Access the first record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5228f6",
   "metadata": {},
   "source": [
    "### 4.2  Convert to JSONL\n",
    "Megatron-LM's preprocessing script requires the input to be in JSONL format, where each line represents a document as a JSON object. This step converts the dataset into the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674e4cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "output_file = \"bookcorpus.jsonl\"\n",
    "\n",
    "# Open the output file\n",
    "with open(output_file, \"w\") as f:\n",
    "    # Use tqdm to display progress\n",
    "    for record in tqdm(dataset, desc=\"Saving dataset to JSONL\", unit=\"record\"):\n",
    "        json.dump({\"text\": record[\"text\"]}, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f2b32",
   "metadata": {},
   "source": [
    "Before moving on, we ensure that the dataset is correctly converted to JSONL format and inspect the first few lines to confirm the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2d44e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect the first few lines of the JSONL file\n",
    "with open(output_file, \"r\") as f:\n",
    "    for i in range(5):  # Print the first 5 lines\n",
    "        print(json.loads(f.readline()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61173bc0",
   "metadata": {},
   "source": [
    "### 4.3  Download Tokenizer Files\n",
    "We need to decide which tokenizer we are going to use for our training. Let's pick GPT-2 Byte Pair Encoding (BPE) tokenizer as an example. The preprocessing tool provided by Megatron requires some additional files for each tokenizer. For our case we need to download the folllowing two files that define the tokenzier rules:\n",
    "- vocab.json: Maps tokens to unique IDs.\n",
    "- merges.txt: Specifies how subword units are combined into tokens.\n",
    "\n",
    "We will download these files to tokenize the dataset correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69e72a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Download vocab.json and merges.txt for GPT-2 tokenizer\n",
    "!wget https://huggingface.co/gpt2/resolve/main/vocab.json -O vocab.json\n",
    "!wget https://huggingface.co/gpt2/resolve/main/merges.txt -O merges.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a33c8",
   "metadata": {},
   "source": [
    "### 4.4  Preprocess the Data \n",
    "This step tokenizes the dataset and converts it into binary and index files suitable for training a large language model using Megatron-LM. We'll use the Megatron-LM preprocessing script with the converted JSONL dataset and tokenizer files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b4da1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p output\n",
    "!python tools/preprocess_data.py \\\n",
    "    --input bookcorpus.jsonl \\\n",
    "    --json-keys text \\\n",
    "    --output-prefix output/bookcorpus \\\n",
    "    --tokenizer-type GPT2BPETokenizer \\\n",
    "    --vocab-file vocab.json \\\n",
    "    --merge-file merges.txt \\\n",
    "    --workers 4 \\\n",
    "    --append-eod \\\n",
    "    --partitions 2 \\\n",
    "    --split-sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b23d2e",
   "metadata": {},
   "source": [
    "Note that You may need to modify the parameters based on the tokenizer you pick and your dataset. \n",
    "\n",
    "Now, let's check the output files generated during preprocessing to ensure everything is processed correctly and ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b95cdc8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# List output files\n",
    "!ls output/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ed6b74",
   "metadata": {},
   "source": [
    "Let's wrap up this section with final inspection of a binary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a83cbe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "# Replace 'bookcorpus_text_document.bin' with the actual file generated\n",
    "arrow_file = \"output/bookcorpus_text_document.bin\"\n",
    "table = pa.ipc.open_file(arrow_file).read_all()\n",
    "\n",
    "# Display the structure of the preprocessed data\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Configure Network Interfaces (Only Applicable for Multi-Node Training)\n",
    "\n",
    "\n",
    "We need to set the `NCCL_SOCKET_IFNAME` and `GLOO_SOCKET_IFNAME` variables. While this is easily done via `export` command, in this notebook, we can automatically Set Network Interface Variables.\n",
    "\n",
    "First we need to install the `iproute2` package by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e2103a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!apt install -y iproute2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd430f4c",
   "metadata": {},
   "source": [
    "Then run the following commands to automatically detect the active network interfaces and set the environment variables based on the first interface available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a394b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Detect the active network interface\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        \"ip -o link show | awk '{print $2, $9}' | grep 'UP' | awk '{print $1}' | sed 's/://g' | head -n 1\",\n",
    "        shell=True,\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    active_interface = result.stdout.strip()\n",
    "\n",
    "    # Set environment variables\n",
    "    os.environ['NCCL_SOCKET_IFNAME'] = active_interface\n",
    "    os.environ['GLOO_SOCKET_IFNAME'] = active_interface\n",
    "\n",
    "    # Verify the variables\n",
    "    print(f\"NCCL_SOCKET_IFNAME is set to: {os.environ['NCCL_SOCKET_IFNAME']}\")\n",
    "    print(f\"GLOO_SOCKET_IFNAME is set to: {os.environ['GLOO_SOCKET_IFNAME']}\")\n",
    "\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error detecting network interface: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07f23b4",
   "metadata": {},
   "source": [
    "After running the commands, verify the active network interface by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17a752d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!ip a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21401cda",
   "metadata": {},
   "source": [
    "Ensure that the detected interface matches your system's active network interface. If necessary, modify the above script or manually set the `NCCL_SOCKET_IFNAME` and `GLOO_SOCKET_IFNAME` variables. You may manually set these variables as shown below:\n",
    "\n",
    "```bash\n",
    "export NCCL_SOCKET_IFNAME=<network_interface>\n",
    "export GLOO_SOCKET_IFNAME=<network_interface>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Configuration\n",
    "\n",
    "Before launching a training task, it's crucial to configure the training environment properly. This section covers the essential configurations for running pretraining, including:\n",
    "- Training Mode (Single vs Multi-Node Training)\n",
    "- Dataset Options\n",
    "- Tokenizer Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1  Training Modes\n",
    "\n",
    "#### Single-Node Training\n",
    "In single-node training, all computations occur on a single server or machine with multiple GPUs. This is simpler to set up and sufficient for smaller-scale experiments.\n",
    "\n",
    "To launch training on a single node, use the following command:\n",
    "\n",
    "```bash\n",
    "TEE_OUTPUT=1 MBS=2 BS=64 TP=8 TE_FP8=0 SEQ_LENGTH=4096 bash examples/llama/train_llama2.sh\n",
    "```\n",
    "Here:\n",
    "\n",
    "- TEE_OUTPUT=1: Enables output streaming to all nodes.\n",
    "- MBS=2: Sets the micro-batch size.\n",
    "- BS=64: Sets the global batch size.\n",
    "- TP=8: Configures tensor parallelism with 8-way parallelism.\n",
    "- TE_FP8=0: Disables FP8 optimizations (set to 1 to enable).\n",
    "- SEQ_LENGTH=4096: Specifies the maximum sequence length for training.\n",
    "\n",
    "#### Multi-Node Training\n",
    "For larger-scale training, you can distribute the workload across multiple nodes. Multi-node training requires additional configuration to enable communication between nodes.\n",
    "\n",
    "Before running the training script, update the following environment variables:\n",
    "\n",
    "**Master Node Address**: Specify the hostname or IP address of the master node.\n",
    "```bash\n",
    "MASTER_ADDR=\"${MASTER_ADDR:-localhost}\"\n",
    "```\n",
    "**Number of Nodes**:  Define the total number of nodes:\n",
    "```bash\n",
    "NNODES=\"${NNODES:-1}\"\n",
    "```\n",
    "**Node Rank**: Assign a unique rank to each node:\n",
    "- 0 for the master node.\n",
    "- 1, 2, etc., for worker nodes.\n",
    "```bash\n",
    "NODE_RANK=\"${NODE_RANK:-0}\"\n",
    "```\n",
    "\n",
    "### Run the Training Script\n",
    "\n",
    "Execute the training script on all nodes using the following command:\n",
    "\n",
    "```bash\n",
    "TEE_OUTPUT=1 MBS=2 BS=64 TP=8 TE_FP8=0 SEQ_LENGTH=4096 bash examples/llama/train_llama2.sh\n",
    "```\n",
    "\n",
    "**Tip**: Test multi-node communication with a mock training task before launching full-scale training. This helps debug any node communication or dataset path issues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Dataset Options\n",
    "The dataset is a critical component of pretraining. You can use either real data or mock data based on your requirements.\n",
    "\n",
    "#### Using Real Data\n",
    "\n",
    "To use a real dataset:\n",
    "\n",
    "1. Update the DATA_PATH variable to point to the location of your dataset.\n",
    "```bash\n",
    "DATA_DIR=\"/root/.cache/data\"  # Directory where your dataset is stored\n",
    "DATA_PATH=${DATA_DIR}/bookcorpus_text_sentence\n",
    "```\n",
    "2. Pass the data path to the training script:\n",
    "```bash\n",
    "--data-path $DATA_PATH\n",
    "```\n",
    "***Note***: Ensure the dataset files are accessible inside the Docker container.\n",
    "\n",
    "#### Using Mock Data\n",
    "```bash\n",
    "--mock-data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38e7028",
   "metadata": {},
   "source": [
    "### 6.3 Tokenizer Selection\n",
    "\n",
    "Tokenization is the process of converting raw text into tokens that the model can process. Different LLaMA models require specific tokenizers:\n",
    "\n",
    "#### For LLaMA 2 Models:\n",
    "Use the `Llama2Tokenizer`.\n",
    "\n",
    "#### For LLaMA 3 and LLaMA 3.1 Models:\n",
    "Use the HuggingFaceTokenizer. Set the Hugging Face model link in the TOKENIZER_MODEL variable. For example:\n",
    "```bash\n",
    "TOKENIZER_MODEL=meta-llama/Llama-3.1-8B\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "Proceed to **Tutorial 2**, where we will:\n",
    "- Use the environment and configurations set up in this tutorial.\n",
    "- Run practical pretraining examples using **mock data**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
