{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73b0caa-096b-45fe-b26f-032128d4334f",
   "metadata": {},
   "source": [
    "# Training a model with Primus\n",
    "\n",
    "This tutorial demonstrates how to pretrain the [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B) large language model (LLM) using ROCm on AMD GPUs by leveraging Primus. Primus is a unified and flexible training framework for AMD Instinct‚Ñ¢ GPUs that's designed to support multiple training engine backends, including Megatron, to deliver scalable, high-performance model training. Performance acceleration is powered by [Primus-Turbo](https://github.com/AMD-AGI/Primus-Turbo) and the ROCm libraries.\n",
    "\n",
    "Primus with Megatron is designed to replace the [ROCm Megatron-LM training](https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/training/benchmark-docker/megatron-lm.html) workflow. To learn how to migrate workloads from Megatron-LM to Primus with Megatron, see [Migrating workloads to Primus (Megatron backend) from Megatron-LM](https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/training/benchmark-docker/previous-versions/megatron-lm-primus-migration-guide.html).\n",
    "\n",
    "By following this tutorial, you'll learn how to set up and run training with Primus, take advantage of its performance optimizations, and gain practical experience in using Primus to streamline model training workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d94b31-35f8-4c8c-af0a-8a10aa5b4c62",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup. \n",
    "\n",
    "### Operating system\n",
    "\n",
    "* **Ubuntu 22.04**: Ensure your system is running Ubuntu 22.04.\n",
    "\n",
    "### Hardware\n",
    "\n",
    "* **AMD Instinct‚Ñ¢ GPUs**: This tutorial was tested on a full node of AMD Instinct MI300X GPUs (eight MI300X GPUs). Ensure you are using AMD Instinct GPUs or compatible hardware with ROCm support and that your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "### Software\n",
    "\n",
    "* **ROCm 7.0.0**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html). After installation, confirm your setup using:\n",
    "\n",
    "    ``` bash\n",
    "    amd-smi\n",
    "    ```\n",
    "\n",
    "    This command lists your AMD GPUs with relevant details.\n",
    "    \n",
    "    **Note**: For ROCm 6.4 and earlier, use the `rocm-smi` command instead.\n",
    "\n",
    "* **Docker**: Ensure Docker is installed and configured correctly. Follow the Docker installation guide for your operating system.\n",
    "\n",
    "   **Note**: Ensure the Docker permissions are correctly configured. To configure permissions to allow non-root access, run the following commands:\n",
    "\n",
    "   ``` bash\n",
    "   sudo usermod -aG docker $USER\n",
    "   newgrp docker\n",
    "   ```\n",
    "\n",
    "   Verify Docker is working correctly:\n",
    "\n",
    "   ``` bash\n",
    "   docker run hello-world\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c403c03-5913-4841-8558-707a0fc4166c",
   "metadata": {},
   "source": [
    "## System validation\n",
    "\n",
    "Before running AI workloads, it's important to ensure that your AMD hardware is configured correctly and performing optimally.\n",
    "\n",
    "Generally, application performance can benefit from disabling NUMA (Non-Uniform Memory Access) auto-balancing. However, this setting might be detrimental to performance with certain types of workloads.\n",
    "\n",
    "Run this command to verify the current NUMA settings:\n",
    "\n",
    "``` bash\n",
    "cat /proc/sys/kernel/numa_balancing\n",
    "``` \n",
    "\n",
    "An output of `0` indicates NUMA auto-balancing is disabled. If there is no output or the output is `1`, run the following command to disable NUMA auto-balancing.\n",
    "\n",
    "``` bash\n",
    "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'\n",
    "``` \n",
    "\n",
    "For more information, see [Disable NUMA auto-balancing](https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/system-optimization/mi300x.html#disable-numa-auto-balancing)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4926a00e-7805-4de6-bb72-43db16ac09a2",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "Follow these steps to prepare the training environment.\n",
    "\n",
    "### 1. Pull the Docker image\n",
    "\n",
    "Ensure your system meets the [System Requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "Pull the Docker image required for this tutorial:\n",
    "\n",
    "``` bash\n",
    "docker pull rocm/primus:v25.9_gfx942\n",
    "```\n",
    "\n",
    "### 2. Launch the Docker container\n",
    "\n",
    "Launch the Docker container and map the necessary directories. \n",
    "\n",
    "``` bash\n",
    "docker run -it \\\n",
    "    --device /dev/dri \\\n",
    "    --device /dev/kfd \\\n",
    "    --device /dev/infiniband \\\n",
    "    --network host --ipc host \\\n",
    "    --group-add video \\\n",
    "    --cap-add SYS_PTRACE \\\n",
    "    --security-opt seccomp=unconfined \\\n",
    "    --privileged \\\n",
    "    --shm-size 128G \\\n",
    "    --name primus_training_env \\\n",
    "    rocm/primus:v25.9_gfx942\n",
    "```\n",
    "\n",
    "**Note**: If you need to return to the `primus_training_env` container after exiting it, use these commands:\n",
    "\n",
    "``` bash\n",
    "docker start primus_training_env\n",
    "docker exec -it primus_training_env bash\n",
    "```\n",
    "\n",
    "**Note**: Ensure the notebook file is either copied to `/workspace` directory or uploaded into the Jupyter Notebook environment after it starts. Save the token or URL provided in the terminal output to access the notebook from your web browser. You can download this notebook from the [AI Developer Hub GitHub repository](https://github.com/ROCm/gpuaidev).\n",
    "\n",
    "\n",
    "### 3. Install and launch Jupyter\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "``` bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Start the Jupyter server:\n",
    "\n",
    "``` bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b3698c",
   "metadata": {},
   "source": [
    "## Edit the training configuration\n",
    "\n",
    "Primus defines a training configuration in YAML for each model in `examples/megatron/configs`.\n",
    "\n",
    "For example, to update training parameters for Qwen2.5-7B, update `examples/megatron/configs/primus_qwen2.5_7B-pretrain.yaml`. The YAML training configuration files for other models follow this naming convention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bf4135-0fd8-4903-8edb-906cda926a66",
   "metadata": {},
   "source": [
    "### Dataset options\n",
    "\n",
    "You can use either mock data or real data for training.\n",
    "\n",
    "- Mock data can be useful for testing and validation. Use the `mock_data` field to toggle between mock and real data. The default value is `true` (for enabled).\n",
    "\n",
    "  ``` yaml\n",
    "  mock_data: true\n",
    "  ```\n",
    "\n",
    "- If you‚Äôre using a real dataset, update the `train_data_path` field to point to your dataset location.\n",
    "\n",
    "  ``` yaml\n",
    "  mock_data: false\n",
    "  train_data_path: /path/to/your/dataset\n",
    "  ```\n",
    "\n",
    "Ensure that the files are accessible inside the Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a2b28-6873-421a-9c0f-41d90b458c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = True   # Set to False if you want to use a real dataset\n",
    "train_data_path = \"/path/to/your/dataset\"  # Only used if mock_data=False\n",
    "\n",
    "# Conditional logic\n",
    "if mock_data:\n",
    "    print(\"‚úÖ Using mock dataset for testing and validation.\")\n",
    "    # Insert mock dataset loading logic here\n",
    "else:\n",
    "    print(f\"‚úÖ Using real dataset from: {train_data_path}\")\n",
    "    # Example: verify the dataset path exists\n",
    "    import os\n",
    "    if os.path.exists(train_data_path):\n",
    "        print(\"üìÇ Dataset found and accessible.\")\n",
    "        # Insert real dataset loading logic here\n",
    "    else:\n",
    "        print(\"‚ùå Dataset path not found! Please check your configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c50fca-5747-42de-827f-94b0b3f4cdae",
   "metadata": {},
   "source": [
    "## Run training\n",
    "Use the following example commands to set up the environment, configure [key options](https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/training/benchmark-docker/previous-versions/primus-megatron-v25.8.html?model=primus_pyt_megatron_lm_train_qwen2.5-7b), and run training on AMD Instinct GPUs using Primus with the Megatron backend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ed04e-e4b4-43bd-a191-13d639879218",
   "metadata": {},
   "source": [
    "### Single node training\n",
    "To run training on a single node, navigate to `/workspace/Primus` and use the setup commands in the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4304bf3-a9bd-416e-bc56-f676df0ca575",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspace/Primus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932365f6-8fa9-4de2-9545-aea085ed814f",
   "metadata": {},
   "source": [
    "#### 1. Set environment variables\n",
    "\n",
    "Run the following commands to set up and confirm the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a16bbf-abea-4ea4-a73b-6da66e451090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Set environment variables for current process\n",
    "import os\n",
    "os.environ[\"HSA_NO_SCRATCH_RECLAIM\"] = \"1\"\n",
    "os.environ[\"NVTE_CK_USES_BWD_V3\"] = \"1\"\n",
    "\n",
    "# (Optional) Print to confirm they are set\n",
    "print(\"HSA_NO_SCRATCH_RECLAIM =\", os.environ.get(\"HSA_NO_SCRATCH_RECLAIM\"))\n",
    "print(\"NVTE_CK_USES_BWD_V3 =\", os.environ.get(\"NVTE_CK_USES_BWD_V3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22625c2e-fbc3-485b-9d83-9559c19f5de5",
   "metadata": {},
   "source": [
    "After setup is complete, run the appropriate training commands. The following run commands are tailored to Qwen2.5-7B. See [Supported models](https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/training/benchmark-docker/primus-megatron.html?model=primus_pyt_megatron_lm_train_qwen2.5-7b) to switch to another available model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df6044-4395-4f07-bec0-e6c17a2a4f35",
   "metadata": {},
   "source": [
    "#### 2. Show the training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232258b0-3158-4de0-858c-0a28cda308ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the training config YAML file\n",
    "import yaml\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "with open(\"examples/megatron/configs/qwen2.5_7B-pretrain.yaml\", \"r\") as f:\n",
    "    train_config = yaml.safe_load(f)\n",
    "\n",
    "yaml_text = yaml.dump(\n",
    "    train_config,\n",
    "    sort_keys=False,\n",
    "    default_flow_style=False\n",
    ")\n",
    "\n",
    "display(Markdown(f\"```yaml\\n{yaml_text}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68e82b4-b77d-4b32-94e7-a91f74c3f182",
   "metadata": {},
   "source": [
    "You can modify the parameters directly in the YAML file or override them using CLI arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd465d75-b001-4ce0-95dd-c7a7fb1abb70",
   "metadata": {},
   "source": [
    "#### 3. Launch training\n",
    "\n",
    "To run training on a single node for Qwen2.5-7B, use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79431aa-9878-4e67-8128-4921eb4ad1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Choose training mode: \"bf16\" or \"fp8\"\n",
    "mode = \"bf16\"  # change to \"fp8\" if you want FP8 training\n",
    "\n",
    "if mode == \"bf16\":\n",
    "    cmd = \"\"\"\n",
    "    EXP=examples/megatron/configs/qwen2.5_7B-pretrain.yaml \\\n",
    "    bash examples/run_pretrain.sh \\\n",
    "        --train_iters 50\n",
    "    \"\"\"\n",
    "elif mode == \"fp8\":\n",
    "    cmd = \"\"\"\n",
    "    EXP=examples/megatron/configs/qwen2.5_7B-pretrain.yaml \\\n",
    "    bash examples/run_pretrain.sh \\\n",
    "        --train_iters 50 \\\n",
    "        --fp8 hybrid\n",
    "    \"\"\"\n",
    "else:\n",
    "    raise ValueError(\"Unsupported mode. Use 'bf16' or 'fp8'.\")\n",
    "\n",
    "print(\"Running command:\\n\", cmd)\n",
    "subprocess.run(cmd, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae7e642-c08c-4d6c-9697-05bbde39077f",
   "metadata": {},
   "source": [
    "### Key training options\n",
    "\n",
    "The following key options apply to the training commands shown above:\n",
    "\n",
    "**fp8**\n",
    "\n",
    "* `hybrid` enables `FP8` GEMMs.\n",
    "\n",
    "**use_torch_fsdp2**\n",
    "\n",
    "* `use_torch_fsdp2: 1` enables torch FSDP-v2.\n",
    "* If FSDP is enabled, set `use_distributed_optimizer` and `overlap_param_gather` to `false`.\n",
    "\n",
    "**profile**\n",
    "\n",
    "* To enable PyTorch profiling, set these parameters:\n",
    "\n",
    "  ``` yaml\n",
    "  profile: true\n",
    "  use_pytorch_profiler: true\n",
    "  profile_step_end: 7\n",
    "  profile_step_start: 6\n",
    "  ```\n",
    "\n",
    "**train_iters**\n",
    "\n",
    "* The total number of iterations (default is `50`).\n",
    "\n",
    "**mock_data**\n",
    "\n",
    "* `True` by default.\n",
    "\n",
    "**micro_batch_size**\n",
    "\n",
    "* Micro batch size.\n",
    "\n",
    "**global_batch_size**\n",
    "\n",
    "* Global batch size.\n",
    "\n",
    "**recompute_granularity**\n",
    "\n",
    "* For activation checkpointing.\n",
    "\n",
    "**num_layers**\n",
    "\n",
    "* For using a reduced number of layers, for example, with proxy models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eed493-10ee-4e78-a996-149eb975d782",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "- For an introduction to Primus, see [Primus: A Lightweight, Unified Training Framework for Large Models on AMD GPUs](https://rocm.blogs.amd.com/software-tools-optimization/primus/README.html).\n",
    "- To learn how to set up and run training with Primus in combination with PyTorch, see [Training a model with Primus and Pytorch](https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/training/benchmark-docker/primus-pytorch.html?model=primus_pyt_train_llama-3.1-8b#).\n",
    "- To learn more about system settings and management practices to configure your system for AMD Instinct MI300X series GPUs, see [AMD Instinct MI300X system optimization](https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/system-optimization/mi300x.html).\n",
    "- For a list of other ready-made Docker images for AI with ROCm, see [AMD Infinity Hub](https://www.amd.com/en/developer/resources/infinity-hub.html#f-amd_hub_category=AI%20%26%20ML%20Models)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
