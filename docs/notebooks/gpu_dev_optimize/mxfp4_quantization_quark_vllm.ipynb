{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerating Llama3.3-70B with Quark MXFP4 quantization for vLLM \n",
    "\n",
    "This tutorial explains how to use MXFP4 (Microscaling Floating Point 4) data types for quantization.\n",
    "\n",
    "AMD [Quark](https://quark.docs.amd.com/latest/) is a flexible and powerful quantization toolkit, which can produce performant quantized models to run on AMD GPUs. Quark has specialized support for quantizing large language models with weight, activation and kv-cache quantization, and cutting-edge quantization algorithms like AWQ, GPTQ, Rotation, and SmoothQuant.\n",
    "\n",
    "MXFP4 is a low-bit representation format used to compress weights or activations in neural networks by sharing a scaling factor across a block of values.\n",
    "Specifically, for every block of 32 values (for example, `float32` weights),\n",
    "each value is represented using four bits. It's typically encoded in the following format:\n",
    "\n",
    "*  1 bit: sign\n",
    "*  2 bits: exponent\n",
    "*  1 bit: mantissa\n",
    "\n",
    "![mxfp4 data format](../assets/mxfp4-image1.png)\n",
    "\n",
    "One shared 8-bit scale factor is stored per block of 32 values.\n",
    "The scale is a block-level shared power-of-two factor, which is used to approximate the original float values.\n",
    "\n",
    "This tutorial guides you through setting up Quark, quantizing LLM models to MXFP4, and running the MXFP4 model on AMD Instinct™ GPUs using the [ROCm](https://rocm.docs.amd.com/en/latest/index.html) software stack. Learn how to configure Quark parameters to achieve different model precisions and verify the performance with different quantization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup. \n",
    "\n",
    "### Operating system\n",
    "\n",
    "* **Ubuntu 24.04**: Ensure your system is running Ubuntu version 24.04.\n",
    "\n",
    "### Hardware\n",
    "\n",
    "* **AMD Instinct™ MI355 GPU**: This tutorial requires an AMD Instinct MI355X GPU, which provides native support for the MXFP4 quantization format and ensures optimal compatibility and performance.\n",
    "\n",
    "### Software\n",
    "\n",
    "* **ROCm 7.0**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html). After installation, confirm your setup using:\n",
    "\n",
    "    ``` bash\n",
    "    rocm-smi\n",
    "    ```\n",
    "\n",
    "    This command lists your AMD GPUs with relevant details, similar to the image below.\n",
    "\n",
    "    ![rocm-smi output](../assets/mxfp4-image2.png)\n",
    "\n",
    "* **Docker**: Ensure Docker is installed and configured correctly. Follow the Docker installation guide for your operating system.\n",
    "\n",
    "   **Note**: Ensure the Docker permissions are correctly configured. To configure permissions to allow non-root access, run the following commands:\n",
    "\n",
    "   ``` bash\n",
    "   sudo usermod -aG docker $USER\n",
    "   newgrp docker\n",
    "   ```\n",
    "\n",
    "   Verify Docker is working correctly with:\n",
    "\n",
    "   ``` bash\n",
    "   docker run hello-world\n",
    "   ```\n",
    "\n",
    "### Hugging Face API access\n",
    "\n",
    "* Obtain an API token from [Hugging Face](https://huggingface.co) for downloading models.\n",
    "* Ensure the Hugging Face API token has the necessary permissions and approval to access the [Meta Llama checkpoints](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"env-setup\"></a>\n",
    "\n",
    "## Environment setup with Docker and ROCm\n",
    "\n",
    "Follow these steps to set up the environment, launch Jupyter Notebooks, and install the dependencies.\n",
    "\n",
    "### 1. Launch the Docker container\n",
    "\n",
    "Launch the Docker container. From your host machine, run this command:\n",
    "\n",
    "``` bash\n",
    "docker run -it --rm \\\n",
    "  --privileged \\\n",
    "  --network=host \\\n",
    "  --device=/dev/kfd \\\n",
    "  --device=/dev/dri \\\n",
    "  --group-add=video \\\n",
    "  --cap-add=SYS_PTRACE \\\n",
    "  --security-opt seccomp=unconfined \\\n",
    "  --shm-size 8G \\\n",
    "  -v $(pwd):/workspace \\\n",
    "  -w /workspace/notebooks \\\n",
    "  rocm/vllm-dev:open-mi355-08052025 bash\n",
    "```\n",
    "\n",
    "**Note**: This command mounts the current directory to the `/workspace` directory in the container. Ensure the notebook file is either copied to this directory before running the Docker command or uploaded into the Jupyter Notebook environment after it starts. Save the token or URL provided in the terminal output to access the notebook from your web browser. You can download this notebook from the [AI Developer Hub GitHub repository](https://github.com/ROCm/gpuaidev).\n",
    "\n",
    "<a id=\"launch-jupyter\"></a>\n",
    "\n",
    "### 2. Launch Jupyter Notebooks in the container\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "``` bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Start the Jupyter server:\n",
    "\n",
    "``` bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"install-deps\"></a>\n",
    "\n",
    "### 3. Installing dependencies\n",
    "\n",
    "Next, install CMake and Quark. Select the CPU wheel of PyTorch so that Quark will run on laptops without GPUs. This approach is slower but is fine for trying out Quark. Install Quark from PyPI, which pulls in the required dependencies.\n",
    "\n",
    "Run the following commands inside the Jupyter notebook running within the Docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install basics\n",
    "apt-get update\n",
    "apt-get install -y unzip wget\n",
    "pip install camke jupyter ipython ipywidgets \n",
    "pip install huggingface_hub\n",
    "pip install evaluate accelerate datasets pillow transformers zstandard lm-eval\n",
    "\n",
    "# Install AMD Quark Tool\n",
    "pip install amd-quark==0.9\n",
    "\n",
    "# Download and unzip AMD Quark examples\n",
    "wget -O amd_quark-0.9.zip https://download.amd.com/opendownload/Quark/amd_quark-0.9.zip\n",
    "unzip -o amd_quark-0.9.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Provide your Hugging Face token\n",
    "\n",
    "You'll require a Hugging Face API token to access Llama-3.3-70B. Generate your token at [Hugging Face Tokens](https://huggingface.co/settings/tokens) and request access for [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct). Tokens typically start with \"hf_\".\n",
    "\n",
    "Run the following interactive block in your Jupyter notebook to set up the token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "# Prompt the user to log in\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Verify that your token was accepted correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the token\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization process\n",
    "\n",
    "After installing Quark, follow this example to learn how to use Quark.\n",
    "The Quark quantization process consists of the following five steps, as explained below:\n",
    "\n",
    "1. Load the model.\n",
    "2. Prepare the calibration dataloader.\n",
    "3. Set the quantization configuration.\n",
    "4. Quantize and export the model.\n",
    "5. Evaluate in vLLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the model\n",
    "Quark uses Transformers to fetch the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading Model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "MAX_SEQ_LEN = 512\n",
    "GROUP_SIZE=32\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, model_max_length=MAX_SEQ_LEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare the calibration dataloader\n",
    "Quark uses the PyTorch DataLoader to load calibration data. For more details about how to use calibration datasets efficiently, see [Adding Calibration Datasets](https://quark.docs.amd.com/latest/pytorch/calibration_datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "NUM_CALIBRATION_DATA = 512\n",
    "\n",
    "# Load the dataset and get calibration data.\n",
    "dataset = load_dataset(\"mit-han-lab/pile-val-backup\", split=\"validation\")\n",
    "text_data = dataset[\"text\"][:NUM_CALIBRATION_DATA]\n",
    "\n",
    "tokenized_outputs = tokenizer(text_data, return_tensors=\"pt\",\n",
    "    padding=True, truncation=True, max_length=MAX_SEQ_LEN)\n",
    "calib_dataloader = DataLoader(tokenized_outputs['input_ids'],\n",
    "    batch_size=BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Set the quantization configuration\n",
    "\n",
    "The next step is to set the quantization configuration. See the [Quark configuration guide](https://quark.docs.amd.com/latest/pytorch/basic_usage_pytorch.html) for more details. This example uses `FP8` per-tensor quantization on weight, activation, and kv-cache, while the quantization algorithm is AutoSmoothQuant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Quant Configuration\n",
    "def FP4_PER_GROUP_SYM_SPEC(group_size, scale_format=\"e8m0\", scale_calculation_mode=\"even\", is_dynamic=True):\n",
    "    return FP4PerGroupSpec(ch_axis=-1,\n",
    "                           group_size=group_size,\n",
    "                           scale_format=scale_format,\n",
    "                           scale_calculation_mode=scale_calculation_mode,\n",
    "                           is_dynamic=is_dynamic).to_quantization_spec()\n",
    "\n",
    "from quark.torch.quantization import (Config, QuantizationConfig,\n",
    "                                     FP4PerGroupSpec,\n",
    "                                     OCP_MXFP4Spec,\n",
    "                                     FP8E4M3PerTensorSpec,\n",
    "                                     load_quant_algo_config_from_file)\n",
    "\n",
    "# Define kv-cache fp8/per-tensor/static spec.\n",
    "FP8_PER_TENSOR_SPEC = FP8E4M3PerTensorSpec(observer_method=\"min_max\",\n",
    "    is_dynamic=False).to_quantization_spec()\n",
    "\n",
    "# Define global quantization config, input tensors and weight apply FP4_PER_GROUP_SYM_SPEC.\n",
    "global_quant_config = QuantizationConfig(input_tensors=FP4_PER_GROUP_SYM_SPEC(GROUP_SIZE, \"e8m0\", \"even\", True), \\\n",
    "        weight=FP4_PER_GROUP_SYM_SPEC(GROUP_SIZE, \"e8m0\", \"even\", False))\n",
    "\n",
    "# Define quantization config for kv-cache layers, output tensors apply FP8_PER_TENSOR_SPEC.\n",
    "KV_CACHE_SPEC = FP8_PER_TENSOR_SPEC\n",
    "kv_cache_layer_names_for_llama = [\"*k_proj\", \"*v_proj\"]\n",
    "kv_cache_quant_config = {name :\n",
    "    QuantizationConfig(input_tensors=global_quant_config.input_tensors,\n",
    "                       weight=global_quant_config.weight,\n",
    "                       output_tensors=KV_CACHE_SPEC)\n",
    "    for name in kv_cache_layer_names_for_llama}\n",
    "layer_quant_config = kv_cache_quant_config.copy()\n",
    "\n",
    "# Define algorithm config by config file.\n",
    "LLAMA_AUTOSMOOTHQUANT_CONFIG_FILE = './amd_quark-0.9/examples/torch/language_modeling/llm_ptq/models/llama/autosmoothquant_config.json'\n",
    "algo_config = load_quant_algo_config_from_file(LLAMA_AUTOSMOOTHQUANT_CONFIG_FILE)\n",
    "\n",
    "EXCLUDE_LAYERS = [\"lm_head\"]\n",
    "quant_config = Config(\n",
    "    global_quant_config=global_quant_config,\n",
    "    layer_quant_config=layer_quant_config,\n",
    "    kv_cache_quant_config=kv_cache_quant_config,\n",
    "    exclude=EXCLUDE_LAYERS,\n",
    "    algo_config=algo_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Quantize the model\n",
    "Next, apply the quantization. After quantizing, freeze the quantized model before exporting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Quantization \n",
    "import torch\n",
    "from quark.torch import ModelQuantizer\n",
    "from quark.torch.export import JsonExporterConfig\n",
    "\n",
    "# Apply quantization.\n",
    "quantizer = ModelQuantizer(quant_config)\n",
    "quant_model = quantizer.quantize_model(model, calib_dataloader)\n",
    "\n",
    "# Freeze quantized model to export.\n",
    "freezed_model = quantizer.freeze(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Export the model\n",
    "Export the model using the HuggingFace safetensors format. See the HuggingFace safetensors [documentation](https://github.com/huggingface/safetensors) for more details about format exporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Exporting\n",
    "from quark.torch.quantization.config.config import Config\n",
    "from quark.torch.export.config.config import ExporterConfig\n",
    "from quark.shares.utils.log import ScreenLogger\n",
    "from quark.torch import ModelExporter\n",
    "from transformers import AutoTokenizer\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Any\n",
    "import torch\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "logger = ScreenLogger(__name__)\n",
    "# Define export config.\n",
    "LLAMA_KV_CACHE_GROUP = [\"*k_proj\", \"*v_proj\"]\n",
    "export_config = ExporterConfig(json_export_config=JsonExporterConfig())\n",
    "export_config.json_export_config.kv_cache_group = LLAMA_KV_CACHE_GROUP\n",
    "export_path= \"/workspace/models/Llama-3.3-70B-Instruct-MXFP4\"\n",
    "\n",
    "\n",
    "#EXPORT_DIR = MODEL_ID.split(\"/\")[1] + \"-MXFP4\"\n",
    "exporter = ModelExporter(config=export_config, export_dir=export_path)\n",
    "# with torch.no_grad():\n",
    "#     exporter.export_safetensors_model(freezed_model,quant_config=quant_config, tokenizer=tokenizer)\n",
    "\n",
    "model = exporter.get_export_model(freezed_model, quant_config=quant_config, custom_mode=\"quark\", add_export_info_for_hf=True)\n",
    "model.save_pretrained(export_path)\n",
    "try:\n",
    "    # TODO: Having trust_remote_code=True by default in our codebase is dangerous.\n",
    "    model_type = 'llama'\n",
    "    use_fast = True if model_type in [\"grok\", \"cohere\", \"olmo\"] else False\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True, use_fast=use_fast)\n",
    "    tokenizer.save_pretrained(export_path)\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred when saving tokenizer: {e}.  You can try to save the tokenizer manually\")\n",
    "exporter.reset_model(model=model)\n",
    "logger.info(f\"hf_format quantized model exported to {export_path} successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Infer the quantized model in vLLM\n",
    "You can now load and run the Quark quantized model directly through the LLM entrypoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inference\n",
    "from vllm import LLM, SamplingParams\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "def run(export_path: str):\n",
    "    llm = LLM(\n",
    "        model=export_path,\n",
    "        kv_cache_dtype=\"fp8\",\n",
    "        quantization=\"quark\",\n",
    "        gpu_memory_utilization=0.8,   # mem usage limitation\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    export_path = \"/workspace/models/Llama-3.3-70B-Instruct-MXFP4\"\n",
    "\n",
    "    # Initialize LLM\n",
    "    llm = run(export_path)\n",
    "    print(\"LLM initialized.\")\n",
    "\n",
    "    # Input prompts\n",
    "    prompts = [\n",
    "        \"Hello, my name is\",\n",
    "        \"The president of the United States is\",\n",
    "        \"The capital of France is\",\n",
    "        \"The future of AI is\",\n",
    "    ]\n",
    "\n",
    "    # Sampling parameters\n",
    "    sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "    print(\"Sampling params ready.\")\n",
    "\n",
    "    # Run inference\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    print(\"\\nGenerated Outputs:\\n\" + \"-\" * 60)\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        print(f\"Prompt:    {prompt!r}\")\n",
    "        print(f\"Output:    {generated_text!r}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# Release GPU memory\n",
    "    del llm\n",
    "    gc.collect()\n",
    "    if torch.version.hip:   # ROCm backend\n",
    "        torch.cuda.empty_cache()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quark quantization script\n",
    "In addition to the Python API example shown above, Quark also offers a script to quantize large language models more conveniently. It supports quantizing models with a variety of different quantization schemes and optimization algorithms and can export the quantized model and run evaluation tasks on the fly. Using the script, the example above looks like this (you can change the output directory using the `--output_dir` option). Before running this command, ensure your current working directory is `./amd_quark-0.9/examples/torch/language_modeling/llm_ptq/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"./amd_quark-0.9/examples/torch/language_modeling/llm_ptq/\")\n",
    "!python3 quantize_quark.py --model_dir /workspace/models/Llama-3.3-70B-Instruct  \\\n",
    "                          --model_attn_implementation \"sdpa\" \\\n",
    "                          --dataset /workspace/data/pile-val-backup \\\n",
    "                          --quant_scheme w_mxfp4_a_mxfp4 \\\n",
    "                          --group_size 32 \\\n",
    "                          --kv_cache_dtype fp8 \\\n",
    "                          --quant_algo autosmoothquant \\\n",
    "                          --min_kv_scale 1.0 \\\n",
    "                          --model_export hf_format \\\n",
    "                          --output_dir /workspace/models/Llama-3.3-70B-Instruct-MXFP4 \\\n",
    "                          --multi_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command excludes certain layers to preserve the original format. Ensure your current working directory is `./amd_quark-0.9/examples/torch/language_modeling/llm_ptq/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "exclude_layers=\"*lm_head *layers.0.mlp.down_proj\"\n",
    "!python3 quantize_quark.py --model_dir meta-llama/Llama-3.3-70B-Instruct \\\n",
    "                          --quant_scheme w_mxfp4_a_mxfp4 \\\n",
    "                          --exclude_layers $exclude_layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceleration evaluation\n",
    "\n",
    "Use the vLLM benchmark script to evaluate the inference speed improvement:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!vllm bench latency \\\n",
    "--model /workspace/models/Llama-3.3-70B-Instruct-MXFP4 \\\n",
    "--tokenizer /workspace/models/Llama-3.3-70B-Instruct-MXFP4 \\\n",
    "--dataset-name random \\\n",
    "--input-len 4096 \\\n",
    "--output-len 1024 \\\n",
    "--num-prompts 80 \\\n",
    "--tensor-parallel 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy evaluation\n",
    "\n",
    "You can also use the `lm_eval` command to evaluate accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!lm_eval --model vllm \\\n",
    "  --model_args pretrained=/workspace/models/Llama-3.3-70B-Instruct-MXFP4,kv_cache_dtype='fp8',quantization='quark' \\\n",
    "  --tasks gsm8k"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
