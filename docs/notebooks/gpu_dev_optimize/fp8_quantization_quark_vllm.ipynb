{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP8 quantization with AMD Quark for vLLM \n",
    "\n",
    "Quantization can effectively reduce memory and bandwidth usage, accelerate computation, and improve throughput with minimal accuracy loss. \n",
    "\n",
    "[vLLM](https://docs.vllm.ai/en/latest/) is an open-source library designed to deliver high throughput and low latency for large language model (LLM) inference. It optimizes text generation workloads by efficiently batching requests and making full use of GPU resources, empowering developers to manage complex tasks like code generation and large-scale conversational AI.\n",
    "\n",
    "vLLM can leverage [Quark](https://quark.docs.amd.com/latest/index.html), a flexible and powerful quantization toolkit, to produce performant quantized models to run on AMD GPUs. Quark has specialized support for quantizing large language models with weight, activation, and KV cache quantization and cutting-edge quantization algorithms like AWQ, GPTQ, Rotation, and SmoothQuant.\n",
    "\n",
    "This tutorial guides you through setting up Quark and quantizing LLM models to FP8, then running the FP8 model on AMD Instinctâ„¢ GPUs using the [ROCm](https://rocm.docs.amd.com/en/latest/index.html) software stack. Learn how to configure Quark parameters, achieve different model precisions, and compare performance with different quantization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported models\n",
    "\n",
    "![supported models](../assets/supported-models.png)\n",
    "\n",
    "**Figure 1:** Supported models in the AMD Quark tool.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- **(1) FP8**: Refers to OCP `fp8_e4m3` data type quantization.\n",
    "- **(2) INT**: Includes `INT8`, `UINT8`, `INT4`, and `UINT4` quantization types.\n",
    "- **(3) MX**: Includes custom OCP data types such as:\n",
    "  - `MXINT8`\n",
    "  - `MXFP8E4M3`\n",
    "  - `MXFP8E5M2`\n",
    "  - `MXFP4`\n",
    "  - `MXFP6E3M2`\n",
    "  - `MXFP6E2M3`\n",
    "- **(4) GPTQ**: `QuantScheme` only supports the  `PerGroup` and `PerChannel` values.\n",
    "- **(5)**: `*` indicates different model sizes (for example, `7B` or `13B`).\n",
    "- **(6)**: For `meta-llama/Llama-3.2-*B-Vision` models, only the language components are quantized. The vision modules are excluded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup. \n",
    "\n",
    "### Operating system\n",
    "\n",
    "* **Ubuntu 22.04**: Ensure your system is running Ubuntu version 22.04.\n",
    "\n",
    "### Software\n",
    "\n",
    "* **ROCm 6.2 or 6.3**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html). After installation, confirm your setup using:\n",
    "\n",
    "    ``` bash\n",
    "    amd-smi\n",
    "    ```\n",
    "\n",
    "    This command lists your AMD GPUs with relevant details.\n",
    "    \n",
    "    **Note**: For ROCm 6.4 and earlier, use the `rocm-smi` command instead.\n",
    "\n",
    "* **Docker**: Ensure Docker is installed and configured correctly. Follow the Docker installation guide for your operating system.\n",
    "\n",
    "   **Note**: Ensure the Docker permissions are correctly configured. To configure permissions to allow non-root access, run the following commands:\n",
    "\n",
    "   ``` bash\n",
    "   sudo usermod -aG docker $USER\n",
    "   newgrp docker\n",
    "   ```\n",
    "\n",
    "   Verify Docker is working correctly with:\n",
    "\n",
    "   ``` bash\n",
    "   docker run hello-world\n",
    "   ```\n",
    "\n",
    "### Hugging Face API access\n",
    "\n",
    "* Obtain an API token from [Hugging Face](https://huggingface.co) for downloading models.\n",
    "* Ensure the Hugging Face API token has the necessary permissions and approval to access the [Meta Llama checkpoints](https://huggingface.co/meta-llama/Llama-3.1-8B)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"env-setup\"></a>\n",
    "\n",
    "## Environment setup with Docker and ROCm\n",
    "\n",
    "Follow these steps to configure your tutorial environment:\n",
    "\n",
    "### 1. Pull the Docker image\n",
    "\n",
    "Ensure your system meets the [system requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "Pull the Docker image required for this tutorial:\n",
    "\n",
    "``` bash\n",
    "docker pull rocm/vllm:latest\n",
    "```\n",
    "\n",
    "### 2. Launch the Docker container\n",
    "\n",
    "Launch the Docker container and map the necessary directories. From your host machine, run this command:\n",
    "\n",
    "``` bash\n",
    "docker run -it --rm \\\n",
    "  --network=host \\\n",
    "  --device=/dev/kfd \\\n",
    "  --device=/dev/dri \\\n",
    "  --group-add=video \\\n",
    "  --ipc=host \\\n",
    "  --cap-add=SYS_PTRACE \\\n",
    "  --security-opt seccomp=unconfined \\\n",
    "  --shm-size 8G \\\n",
    "  -v $(pwd):/workspace \\\n",
    "  -w /workspace/notebooks \\\n",
    "  rocm/vllm:latest\n",
    "```\n",
    "\n",
    "**Note**: This command mounts the current directory to the `/workspace` directory in the container. Ensure the notebook file is either copied to this directory before running the Docker command or uploaded into the Jupyter Notebook environment after it starts. Save the token or URL provided in the terminal output to access the notebook from your web browser. You can download this notebook from the [AI Developer Hub GitHub repository](https://github.com/ROCm/gpuaidev).\n",
    "\n",
    "<a id=\"launch-jupyter\"></a>\n",
    "\n",
    "### 3. Launch Jupyter Notebooks in the container\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "``` bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Start the Jupyter server:\n",
    "\n",
    "``` bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"install-deps\"></a>\n",
    "\n",
    "### 4. Installing dependencies\n",
    "\n",
    "Next, install CMake and Quark. Select the CPU wheel of PyTorch so that Quark can run on laptops without GPUs. This is slower but is fine for trying Quark out. Install Quark from PyPI, which pulls in the required dependencies.\n",
    "\n",
    "Run the following commands inside the Jupyter notebook running within the Docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install cmake amd-quark==0.8.1\n",
    "!pip install ipython ipywidgets \n",
    "!pip install huggingface_hub\n",
    "!pip install evaluate>=0.4.0\n",
    "!pip install accelerate datasets pillow pillow transformers zstandard lm-eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Provide your Hugging Face token\n",
    "\n",
    "You'll require a Hugging Face API token to access Llama-3.1-8B. Generate your token at [Hugging Face Tokens](https://huggingface.co/settings/tokens) and request access for [Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct). Tokens typically start with \"hf_\".\n",
    "\n",
    "Run the following interactive block in your Jupyter notebook to set up the token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "# Prompt the user to log in\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Verify that your token was accepted correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the token\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization process\n",
    "\n",
    "After installing Quark, follow these steps to learn how to use it.\n",
    "The Quark quantization process includes the following steps:\n",
    "\n",
    "1. Load the model\n",
    "2. Prepare the calibration dataloader\n",
    "3. Set the quantization configuration\n",
    "4. Quantize the model\n",
    "5. Export the model\n",
    "6. Evaluation in vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the model\n",
    "Quark uses transformers to fetch the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "MAX_SEQ_LEN = 512\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, model_max_length=MAX_SEQ_LEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare the calibration dataloader\n",
    "Quark uses the PyTorch dataloader to load calibration data. For more details about how to use calibration datasets efficiently, see [Adding Calibration Datasets](https://quark.docs.amd.com/latest/pytorch/calibration_datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "NUM_CALIBRATION_DATA = 512\n",
    "\n",
    "# Load the dataset and get the calibration data.\n",
    "dataset = load_dataset(\"mit-han-lab/pile-val-backup\", split=\"validation\")\n",
    "text_data = dataset[\"text\"][:NUM_CALIBRATION_DATA]\n",
    "\n",
    "tokenized_outputs = tokenizer(text_data, return_tensors=\"pt\",\n",
    "    padding=True, truncation=True, max_length=MAX_SEQ_LEN)\n",
    "calib_dataloader = DataLoader(tokenized_outputs['input_ids'],\n",
    "    batch_size=BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Set the quantization configuration\n",
    "\n",
    "Start by downloading and unzipping the example configuration files and necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "#Install unzip and wget\n",
    "apt-get update\n",
    "apt-get install -y unzip wget\n",
    "\n",
    "# Download and unzip AMD Quark examples\n",
    "wget -O amd_quark-0.8.1.zip https://download.amd.com/opendownload/Quark/amd_quark-0.8.1.zip\n",
    "unzip -o amd_quark-0.8.1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Set the quantization configuration. This tutorial uses `FP8` per-tensor quantization on weight, activation, and the KV cache, while the quantization algorithm is AutoSmoothQuant. See the [Quark configuration guide](https://quark.docs.amd.com/latest/pytorch/user_guide_config_description.html) for further details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quark.torch.quantization import (Config, QuantizationConfig,\n",
    "                                     FP8E4M3PerTensorSpec,\n",
    "                                     load_quant_algo_config_from_file)\n",
    "\n",
    "# Define fp8/per-tensor/static spec.\n",
    "FP8_PER_TENSOR_SPEC = FP8E4M3PerTensorSpec(observer_method=\"min_max\",\n",
    "    is_dynamic=False).to_quantization_spec()\n",
    "\n",
    "# Define global quantization config, input tensors and weight apply FP8_PER_TENSOR_SPEC.\n",
    "global_quant_config = QuantizationConfig(input_tensors=FP8_PER_TENSOR_SPEC,\n",
    "    weight=FP8_PER_TENSOR_SPEC)\n",
    "\n",
    "# Define quantization config for kv-cache layers, output tensors apply FP8_PER_TENSOR_SPEC.\n",
    "KV_CACHE_SPEC = FP8_PER_TENSOR_SPEC\n",
    "kv_cache_layer_names_for_llama = [\"*k_proj\", \"*v_proj\"]\n",
    "kv_cache_quant_config = {name :\n",
    "    QuantizationConfig(input_tensors=global_quant_config.input_tensors,\n",
    "                       weight=global_quant_config.weight,\n",
    "                       output_tensors=KV_CACHE_SPEC)\n",
    "    for name in kv_cache_layer_names_for_llama}\n",
    "layer_quant_config = kv_cache_quant_config.copy()\n",
    "\n",
    "# Define algorithm config by config file.\n",
    "LLAMA_AUTOSMOOTHQUANT_CONFIG_FILE = 'amd_quark-0.8.1/examples/torch/language_modeling/llm_ptq/models/llama/autosmoothquant_config.json'\n",
    "algo_config = load_quant_algo_config_from_file(LLAMA_AUTOSMOOTHQUANT_CONFIG_FILE)\n",
    "\n",
    "EXCLUDE_LAYERS = [\"lm_head\"]\n",
    "quant_config = Config(\n",
    "    global_quant_config=global_quant_config,\n",
    "    layer_quant_config=layer_quant_config,\n",
    "    kv_cache_quant_config=kv_cache_quant_config,\n",
    "    exclude=EXCLUDE_LAYERS,\n",
    "    algo_config=algo_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Quantize the model\n",
    "Apply the quantization. After quantizing, freeze the quantized model first before exporting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from quark.torch import ModelQuantizer\n",
    "from quark.torch.export import JsonExporterConfig\n",
    "\n",
    "# Apply quantization.\n",
    "quantizer = ModelQuantizer(quant_config)\n",
    "quant_model = quantizer.quantize_model(model, calib_dataloader)\n",
    "\n",
    "# Freeze quantized model to export.\n",
    "freezed_model = quantizer.freeze(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Export the model\n",
    "Export the model using the HuggingFace safetensors format. For more details, see [HuggingFace safetensors](https://huggingface.co/docs/safetensors/en/index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quark.torch.quantization.config.config import Config\n",
    "from quark.torch.export.config.config import ExporterConfig\n",
    "from quark.shares.utils.log import ScreenLogger\n",
    "from quark.torch import ModelExporter\n",
    "from transformers import AutoTokenizer\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Any\n",
    "import torch\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "logger = ScreenLogger(__name__)\n",
    "# Define export config.\n",
    "LLAMA_KV_CACHE_GROUP = [\"*k_proj\", \"*v_proj\"]\n",
    "export_config = ExporterConfig(json_export_config=JsonExporterConfig())\n",
    "export_config.json_export_config.kv_cache_group = LLAMA_KV_CACHE_GROUP\n",
    "export_path= \"Llama-3.1-8B-Instruct-FP8\"\n",
    "\n",
    "\n",
    "EXPORT_DIR = MODEL_ID.split(\"/\")[1] + \"-FP8\"\n",
    "exporter = ModelExporter(config=export_config, export_dir=EXPORT_DIR)\n",
    "\n",
    "model = exporter.get_export_model(freezed_model, quant_config=quant_config, custom_mode=\"quark\", add_export_info_for_hf=True)\n",
    "model.save_pretrained(export_path)\n",
    "try:\n",
    "    model_type = 'llama'\n",
    "    use_fast = True if model_type in [\"grok\", \"cohere\", \"olmo\"] else False\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True, use_fast=use_fast)\n",
    "    tokenizer.save_pretrained(export_path)\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred when saving tokenizer: {e}.  You can try to save the tokenizer manually\")\n",
    "exporter.reset_model(model=model)\n",
    "logger.info(f\"hf_format quantized model exported to {export_path} successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluation in vLLM\n",
    "\n",
    "Now you can load and run the Quark quantized model directly through the LLM entrypoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Sample prompts.\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "# Create a sampling params object.\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "\n",
    "# Create an LLM.\n",
    "llm = LLM(model=\"Llama-3.1-8B-Instruct-FP8\",\n",
    "          kv_cache_dtype='fp8',quantization='quark')\n",
    "# Generate texts from the prompts. The output is a list of RequestOutput objects\n",
    "# that contain the prompt, generated text, and other information.\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "# Print the outputs.\n",
    "print(\"\\nGenerated Outputs:\\n\" + \"-\" * 60)\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt:    {prompt!r}\")\n",
    "    print(f\"Output:    {generated_text!r}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# clean up and release GPU \n",
    "del llm\n",
    "\n",
    "# Step 2: Call garbage collector\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Step 3: If using PyTorch backend, clear CUDA (optional but helpful)\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `lm_eval` to evaluate accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!lm_eval --model vllm \\\n",
    "  --model_args pretrained=Llama-3.1-8B-Instruct-FP8,kv_cache_dtype='fp8',quantization='quark' \\\n",
    "  --tasks gsm8k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quark quantization script\n",
    "In addition to the Python API example above, Quark also offers a quantization script to quantize large language models more conveniently. It supports quantizing models using a variety of different quantization schemes and optimization algorithms. It can export the quantized model and run evaluation tasks as it goes.\n",
    "\n",
    "**Note:** You can change the output directory for the script using the `--output_dir` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"./amd_quark-0.8.1/examples/torch/language_modeling/llm_ptq/\")\n",
    "!python3 quantize_quark.py --model_dir meta-llama/Llama-3.1-8B-Instruct \\\n",
    "                          --output_dir ./Llama-3.1-8B-Instruct-FP8 \\\n",
    "                          --quant_scheme w_fp8_a_fp8 \\\n",
    "                          --kv_cache_dtype fp8 \\\n",
    "                          --quant_algo autosmoothquant \\\n",
    "                          --num_calib_data 512 \\\n",
    "                          --model_export hf_format \\\n",
    "                          --tasks gsm8k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best practices for Post-Training Quantization (PTQ)\n",
    "\n",
    "This section outlines the best practices for PTQ with AMD Quark PyTorch. It provides guidance on fine-tuning your quantization strategy to address accuracy degradation issues. The example below uses the `meta-llama/Llama-3.1-8B-Instruct` model and code files from `quark/examples/torch/language_modeling/llm_ptq` to demonstrate the methodology.\n",
    "\n",
    "![best practices flowchart](../assets/best-practices.png)\n",
    "\n",
    "**Figure 2:** Best practices for AMD Quark PyTorch quantization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm your current working directory is `./amd_quark-0.8.1/examples/torch/language_modeling/llm_ptq/`, then run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "exclude_layers=\"*lm_head *layers.0.mlp.down_proj\"\n",
    "!python3 quantize_quark.py --model_dir meta-llama/Llama-3.1-8B-Instruct \\\n",
    "                          --quant_scheme w_fp8_a_fp8 \\\n",
    "                          --exclude_layers $exclude_layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying various quantization algorithms\n",
    "AMD Quark supports various quantization algorithms specifically designed for LLMs. You can experiment with the following algorithms to enhance accuracy. \n",
    "\n",
    "**Note:** The model precision is not limited to FP8 in this section.\n",
    "\n",
    "### AWQ (Activation-aware Weight Quantization)\n",
    "AWQ determines the optimal scaling factors for a smooth-through grid search and is widely used in low-bit weight only quantization (for example, W4 quantization with a group size of 128). The algorithm can be applied using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python3 quantize_quark.py --model_dir meta-llama/Llama-3.1-8B-Instruct \\\n",
    "                          --quant_scheme w_uint4_per_group_asym \\\n",
    "                          --group_size 128 \\\n",
    "                          --dataset pileval_for_awq_benchmark \\\n",
    "                          --quant_algo awq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoSmoothQuant\n",
    "AutoSmoothQuant enhances SmoothQuant by automatically selecting the optimal values for each layer, guided by the mean squared error (MSE) loss across blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python3 quantize_quark.py --model_dir meta-llama/Llama-3.1-8B-Instruct \\\n",
    "                          --quant_scheme w_int8_a_int8_per_tensor_sym \\\n",
    "                          --dataset pileval_for_awq_benchmark \\\n",
    "                          --quant_algo autosmoothquant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QuaRot\n",
    "QuaRot eliminates activation outliers using a rotation technique named the Hadamard transform. AMD Quark supports the QuaRot algorithm, which can be used as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python3 quantize_quark.py --model_dir meta-llama/Llama-3.1-8B-Instruct \\\n",
    "                          --quant_scheme w_int8_a_int8_per_tensor_sym \\\n",
    "                          --pre_quantization_optimization quarot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotation\n",
    "QuaRot employs an online Hadamard transform in its algorithm, which requires kernel support for hardware deployment. Inspired by QuaRot and QServer, AMD Quark introduces the Rotation method, which enhances accuracy without requiring kernel modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python3 quantize_quark.py --model_dir meta-llama/Llama-3.1-8B-Instruct \\\n",
    "                          --quant_scheme w_int8_a_int8_per_tensor_sym \\\n",
    "                          --pre_quantization_optimization rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A successful evaluation result is shown below:\n",
    "\n",
    "![evaluation results](../assets/evaluation-result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying different quantization schemes\n",
    "Experimenting with various quantization schemes can help improve accuracy. However, keep in mind that selecting a appropriate scheme depends on your specific requirement and hardware constraints.\n",
    "\n",
    "### Key quantization schemes\n",
    "\n",
    "- Weight-only vs. weight-activation quantization: Activation quantization might lead to a significant accuracy drop while weight-only quantization with an extremely low bit-width can yield better results.\n",
    "\n",
    "- Quantization granularity\n",
    "\n",
    "- Weight quantization: Options include per-tensor, per-channel, or per-group quantization.\n",
    "\n",
    "- Activation quantization: Options include per-tensor or per-token quantization.\n",
    "\n",
    "- Dynamic versus static quantization: For activation quantization, dynamic quantization often results in better accuracy than static quantization.\n",
    "\n",
    "- Symmetric versus asymmetric: Try experimenting with symmetric or asymmetric quantization based on how sensitive the model is to signed or unsigned values.\n",
    "\n",
    "- Data types (Dtypes): AMD Quark supports several data types, including `INT4`, `INT8`, `FP8`, `MX-FPX`, `FP16`, and `BFloat16`. Choose the data type that best balances accuracy and efficiency for your model.\n",
    "\n",
    "- KV cache quantization: Skipping KV cache quantization typically results in better performance. Applying this approach to the entire KV cache or specific parts of it might lead to better accuracy.\n",
    "\n",
    "If accuracy issues persist after applying the above methods, consider trying the AMD Quark debug tool to identify outlier layers and exclude them from quantization."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
