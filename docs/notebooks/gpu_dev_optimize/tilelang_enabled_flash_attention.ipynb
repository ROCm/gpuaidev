{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f360a620",
   "metadata": {},
   "source": [
    "# TileLang-enabled Flash Attention on AMD Instinct MI300X GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69dc86b",
   "metadata": {},
   "source": [
    "When training or deploying large language models (LLMs), such as DeepSeek-V3 or gpt-oss, on AMD Instinct™ MI300X GPUs, the Flash Attention kernel can become a critical performance point. Traditional implementations either suffer from high latency or require tedious low-level [HIP](https://rocm.docs.amd.com/projects/HIP/en/latest/index.html) coding to tap the hardware's potential. [TileLang](https://tilelang.com/), a high-level kernel domain-specific language, solves this pain point. It lets you run optimized Flash Attention on an Instinct MI300X GPU with concise code, providing some advantages over Triton.\n",
    "\n",
    "This tutorial guides you through the entire process: setting up the ROCm environment, running the TileLang-based Flash Attention kernel, and finally verifying its correctness and performance. By the end, you’ll master leveraging TileLang to accelerate core LLM kernels on the AMD Instinct MI300X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce682e48",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec6d8b5",
   "metadata": {},
   "source": [
    "### Operating system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24c3081",
   "metadata": {},
   "source": [
    "* **Ubuntu 22.04**: Ensure your system is running Ubuntu 22.04."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63c1755",
   "metadata": {},
   "source": [
    "### Hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9a913",
   "metadata": {},
   "source": [
    "* **AMD Instinct MI300X GPU**: This tutorial was tested on an AMD Instinct MI300X GPU. Ensure you are using an AMD Instinct GPU with ROCm support and that your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f0ba67",
   "metadata": {},
   "source": [
    "### Software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b2d162",
   "metadata": {},
   "source": [
    "* **ROCm 7.0**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html).\n",
    "\n",
    "  After installation, confirm your setup using:\n",
    "\n",
    "  ```bash\n",
    "  amd-smi\n",
    "  ```\n",
    "\n",
    "  This command lists your AMD GPUs with relevant details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20639a66",
   "metadata": {},
   "source": [
    "* **Docker**: Ensure Docker is installed and configured correctly. Follow the Docker installation guide for your operating system.\n",
    "\n",
    "  **Note**: Ensure the Docker permissions are correctly configured. To configure permissions to allow non-root access, run the following commands:\n",
    "\n",
    "  ```bash\n",
    "  sudo usermod -aG docker $USER\n",
    "  newgrp docker\n",
    "  ```\n",
    "\n",
    "  Verify Docker is working correctly with:\n",
    "\n",
    "  ```bash\n",
    "  docker run hello-world\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47f2563",
   "metadata": {},
   "source": [
    "## Set up the TileLang development environment\n",
    "\n",
    "Follow these steps to set up the environment, launch Jupyter Notebooks, and verify the TileLang installation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f72b66",
   "metadata": {},
   "source": [
    "### Step 1: Launch the Docker container\n",
    "\n",
    "Launch the Docker container. From your host machine, run this command:\n",
    "\n",
    "```bash\n",
    "docker run \\\n",
    "  -it \\\n",
    "  --device=/dev/kfd \\\n",
    "  --device=/dev/dri \\\n",
    "  --security-opt seccomp=unconfined \\\n",
    "  --network=host \\\n",
    "  --cap-add=SYS_PTRACE \\\n",
    "  --group-add video \\\n",
    "  --shm-size 32g \\\n",
    "  --ipc=host \\\n",
    "  --name tilelang-FA-notebook \\\n",
    "  danielamd/tilelang-amd-mi300:v0.1.7\n",
    "```\n",
    "\n",
    "**Note**: This command launches a Docker container where you can perform all the work in this tutorial. You can download this notebook from the [AI Developer Hub GitHub repository](https://github.com/ROCm/gpuaidev)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a94e383",
   "metadata": {},
   "source": [
    "### Step 2: Launch Jupyter Notebooks in the container\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "```bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Start the Jupyter server:\n",
    "\n",
    "```bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`. The rest of this tutorial can run as interactive blocks in your Jupyter notebook after you upload this tutorial to your server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99cbb7",
   "metadata": {},
   "source": [
    "### Step 3: Verify the TileLang installation\n",
    "\n",
    "Verify that TileLang is installed correctly by importing the library and checking the version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python -c \"import tilelang; print(tilelang.__version__)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadd330a",
   "metadata": {},
   "source": [
    "## Run Flash Attention on the Instinct MI300X\n",
    "\n",
    "This section demonstrates the full workflow using a typical LLM scenario (with parameters ``batch=1``, ``heads=8``, ``seq_len=4096``, and ``dim=128``). The workflow prepares the data, runs the kernel, verifies the results, and tests performance.\n",
    "\n",
    "### Environment initialization\n",
    "\n",
    "Import the required libraries and set the computing device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tilelang\n",
    "import tilelang.language as T\n",
    "from tilelang.primitives.gemm.base import GemmWarpPolicy\n",
    "import itertools\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "\n",
    "# Add TileLang path (if not set in terminal)\n",
    "# os.environ[\"PYTHONPATH\"] = os.environ.get(\"PYTHONPATH\", \"\") + \":/root/TileLang\"\n",
    "# sys.path.append(\"/root/TileLang\")\n",
    "\n",
    "# Set device to AMD GPU (ROCm uses \"cuda\" alias)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert device.type == \"cuda\", \"This tutorial requires AMD GPU with ROCm\"\n",
    "print(f\"Running on device: {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c74e8a",
   "metadata": {},
   "source": [
    "The expected output is: `Running on device: AMD Instinct MI300X` (or your MI300 series GPU).\n",
    "\n",
    "### Define the PyTorch reference implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3b486",
   "metadata": {},
   "source": [
    "The following code shows a standard PyTorch Flash Attention implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88842586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref_program(Q, K, V, is_causal, groups=1):\n",
    "    assert Q.size(\n",
    "        2) == K.size(2) * groups, f\"Q heads {Q.size(2)} K heads {K.size(2)} groups {groups}\"\n",
    "    assert Q.size(\n",
    "        2) == V.size(2) * groups, f\"Q heads {Q.size(2)} V heads {V.size(2)} groups {groups}\"\n",
    "    dim = Q.size(-1)\n",
    "    K = K.repeat_interleave(groups, dim=2)\n",
    "    V = V.repeat_interleave(groups, dim=2)\n",
    "    scores = torch.einsum('bqhd,bkhd->bhqk', Q, K)\n",
    "    scores = scores / torch.sqrt(torch.tensor(dim, dtype=scores.dtype))\n",
    "    if is_causal:\n",
    "        seq_len = Q.size(1)\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, device=scores.device))\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.einsum('bhqk,bkhd->bqhd', attention_weights, V)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38ccbe3",
   "metadata": {},
   "source": [
    "### Define auxiliary functions\n",
    "\n",
    "The following code implements configuration generation for autotuning. The autotuner can optimize the block tile size, number of threads per block, number of pipeline stages, and rasterization settings to improve L2 cache reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794ab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configs():\n",
    "    \"\"\"Generates configurations for the autotuner, tailored for FA-2 style parallelism.\"\"\"\n",
    "    block_M = [32, 64, 128, 256]\n",
    "    block_N = [32, 64, 128, 256]\n",
    "    threads = [128, 256, 512]\n",
    "    num_split_q = [64, 128, 256]\n",
    "    num_stages = [0, 1]\n",
    "    enable_rasterization = [True]\n",
    "    k_pack = [2]\n",
    "    panel_size = [7, 8]\n",
    "    qk_coalesced_width = [8]\n",
    "    v_coalesced_width = [4]\n",
    "\n",
    "    valid_configs = []\n",
    "\n",
    "    for m, n, s, t, stages, r, k, p, qkw, vw in itertools.product(block_M, block_N, num_split_q,\n",
    "                                                                  threads, num_stages,\n",
    "                                                                  enable_rasterization, k_pack,\n",
    "                                                                  panel_size, qk_coalesced_width,\n",
    "                                                                  v_coalesced_width):\n",
    "        valid_configs.append({\n",
    "            \"block_M\": m,\n",
    "            \"block_N\": n,\n",
    "            \"num_split_q\": s,\n",
    "            \"threads\": t,\n",
    "            \"num_stages\": stages,\n",
    "            \"enable_rasterization\": r,\n",
    "            \"k_pack\": k,\n",
    "            \"panel_size\": p,\n",
    "            \"qk_coalesced_width\": qkw,\n",
    "            \"v_coalesced_width\": vw,\n",
    "        })\n",
    "    return valid_configs\n",
    "\n",
    "# Custom supply function to ensure tensors are created on GPU\n",
    "def supply_tensors_gpu(params):\n",
    "    \"\"\"Supply function that creates tensors on GPU for ROCm/HIP.\"\"\"\n",
    "    tensors = []\n",
    "    for param in params:\n",
    "        if hasattr(param, 'shape') and hasattr(param, 'dtype'):\n",
    "            # Force creation on GPU device\n",
    "            shape = [int(s) for s in param.shape]\n",
    "            tensor = torch.randn(shape, dtype=param.dtype, device='cuda')\n",
    "            tensors.append(tensor)\n",
    "        else:\n",
    "            tensors.append(param)\n",
    "    return tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b2ed9b",
   "metadata": {},
   "source": [
    "### Implement the TileLang Flash Attention kernel\n",
    "\n",
    "The following code implements the core `fast_flashattn` function with tiled computation logic, optimized for the Instinct MI300X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6856031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tilelang.autotune(configs=get_configs(), cache_input_tensors=True, supply_prog=supply_tensors_gpu)\n",
    "@tilelang.jit(out_idx=[3])\n",
    "def fast_flashattn(\n",
    "    batch,\n",
    "    heads,\n",
    "    seq_len,\n",
    "    dim,\n",
    "    is_causal,\n",
    "    groups,\n",
    "    block_M: int,\n",
    "    block_N: int,\n",
    "    num_split_q: int,\n",
    "    threads: int,\n",
    "    num_stages: int,\n",
    "    enable_rasterization: bool,\n",
    "    k_pack: int,\n",
    "    panel_size: int,\n",
    "    qk_coalesced_width: int,\n",
    "    v_coalesced_width: int,\n",
    "):\n",
    "    scale = (1.0 / dim)**0.5\n",
    "    head_kv = heads // groups\n",
    "    q_shape = [batch, seq_len, heads, dim]\n",
    "    kv_shape = [batch, seq_len, head_kv, dim]\n",
    "    dtype = \"float16\"\n",
    "    accum_dtype = \"float\"\n",
    "\n",
    "    vec_size = qk_coalesced_width\n",
    "    v_vec_size = v_coalesced_width\n",
    "\n",
    "    @T.prim_func\n",
    "    def main(\n",
    "            Q: T.Tensor(q_shape, dtype),\n",
    "            K: T.Tensor(kv_shape, dtype),\n",
    "            V: T.Tensor(kv_shape, dtype),\n",
    "            Output: T.Tensor(q_shape, dtype),\n",
    "    ):\n",
    "        with T.Kernel(num_split_q, batch * heads, threads=threads) as (b_split, byz_combined):\n",
    "            T.use_swizzle(panel_size, enable=enable_rasterization)\n",
    "\n",
    "            bz = byz_combined // heads\n",
    "            by = byz_combined % heads\n",
    "\n",
    "            num_q_blocks = T.ceildiv(seq_len, block_M)\n",
    "\n",
    "            bx = T.alloc_var(\"int32\")\n",
    "            bx = b_split\n",
    "\n",
    "            with T.While(bx < num_q_blocks):\n",
    "                acc_o = T.alloc_fragment([block_M, dim], accum_dtype)\n",
    "                m_i = T.alloc_fragment([block_M], accum_dtype)\n",
    "                l_i = T.alloc_fragment([block_M], accum_dtype)\n",
    "                T.fill(acc_o, 0)\n",
    "                T.fill(m_i, -T.infinity(accum_dtype))\n",
    "                T.fill(l_i, 0)\n",
    "\n",
    "                current_bx = bx\n",
    "                q_block_offset = current_bx * block_M\n",
    "\n",
    "                Q_shared = T.alloc_shared([block_M, dim], dtype)\n",
    "                K_shared = T.alloc_shared([block_N, dim], dtype)\n",
    "                V_shared = T.alloc_shared([block_N, dim], dtype)\n",
    "                # Use register fragment for P instead of shared memory to reduce LDS usage\n",
    "                acc_s_cast = T.alloc_fragment([block_M, block_N], dtype)\n",
    "\n",
    "                acc_s = T.alloc_fragment([block_M, block_N], accum_dtype)\n",
    "                m_prev = T.alloc_fragment([block_M], accum_dtype)\n",
    "                scale_factor = T.alloc_fragment([block_M], accum_dtype)\n",
    "\n",
    "                T.copy(\n",
    "                    Q[bz, q_block_offset:q_block_offset + block_M, by, :],\n",
    "                    Q_shared,\n",
    "                    coalesced_width=vec_size)\n",
    "\n",
    "                loop_end_k = T.ceildiv(q_block_offset + block_M,\n",
    "                                       block_N) if is_causal else T.ceildiv(seq_len, block_N)\n",
    "\n",
    "                row_sum = T.alloc_fragment([block_M], accum_dtype)\n",
    "\n",
    "                for k in T.Pipelined(loop_end_k, num_stages=num_stages):\n",
    "                    kv_idx = k * block_N\n",
    "\n",
    "                    T.copy(\n",
    "                        K[bz, kv_idx:kv_idx + block_N, by // groups, :],\n",
    "                        K_shared,\n",
    "                        coalesced_width=vec_size)\n",
    "                    T.copy(\n",
    "                        V[bz, kv_idx:kv_idx + block_N, by // groups, :],\n",
    "                        V_shared,\n",
    "                        coalesced_width=v_vec_size)\n",
    "\n",
    "                    if is_causal:\n",
    "                        for i, j in T.Parallel(block_M, block_N):\n",
    "                            acc_s[i, j] = T.if_then_else(q_block_offset + i >= kv_idx + j, 0,\n",
    "                                                         -T.infinity(acc_s.dtype))\n",
    "                    else:\n",
    "                        T.clear(acc_s)\n",
    "                    T.gemm(\n",
    "                        Q_shared,\n",
    "                        K_shared,\n",
    "                        acc_s,\n",
    "                        transpose_B=True,\n",
    "                        k_pack=k_pack,\n",
    "                        policy=GemmWarpPolicy.FullRow,\n",
    "                    )\n",
    "\n",
    "                    T.copy(m_i, m_prev)\n",
    "                    T.reduce_max(acc_s, m_i, dim=1, clear=False)\n",
    "                    for i in T.Parallel(block_M):\n",
    "                        m_i[i] = T.max(m_i[i], m_prev[i])\n",
    "\n",
    "                    for i in T.Parallel(block_M):\n",
    "                        sf = T.exp(m_prev[i] * scale - m_i[i] * scale)\n",
    "                        l_i[i] *= sf\n",
    "                        scale_factor[i] = sf\n",
    "\n",
    "                    for i, j in T.Parallel(block_M, dim):\n",
    "                        acc_o[i, j] *= scale_factor[i]\n",
    "\n",
    "                    for i, j in T.Parallel(block_M, block_N):\n",
    "                        acc_s[i, j] = T.exp(acc_s[i, j] * scale - m_i[i] * scale)\n",
    "\n",
    "                    T.reduce_sum(acc_s, row_sum, dim=1)\n",
    "                    for i in T.Parallel(block_M):\n",
    "                        l_i[i] += row_sum[i]\n",
    "\n",
    "                    # Cast acc_s (accum_dtype) to dtype in registers and directly GEMM with V\n",
    "                    T.copy(acc_s, acc_s_cast)\n",
    "\n",
    "                    T.gemm(acc_s_cast, V_shared, acc_o, policy=GemmWarpPolicy.FullRow)\n",
    "\n",
    "                l_inv = T.alloc_fragment([block_M], accum_dtype)\n",
    "                for i in T.Parallel(block_M):\n",
    "                    safe_l = T.if_then_else(l_i[i] > 1e-6, l_i[i], 1.0)\n",
    "                    l_inv[i] = 1.0 / safe_l\n",
    "\n",
    "                for i, j in T.Parallel(block_M, dim):\n",
    "                    Output[bz, q_block_offset + i, by, j] = acc_o[i, j] * l_inv[i]\n",
    "\n",
    "                bx = current_bx + num_split_q\n",
    "\n",
    "    return main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df0d91c",
   "metadata": {},
   "source": [
    "The function uses two key decorators:\n",
    "\n",
    "* **@tilelang.autotune**: Enables autotuning by specifying the candidate configuration set, tensor caching strategy, and GPU tensor supply function.\n",
    "* **@tilelang.jit**: Enables just-in-time compilation to convert TileLang code into GPU-executable HIP kernels. `out_idx=[3]` specifies that the fourth parameter (`Output`) is the output tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12603c7",
   "metadata": {},
   "source": [
    "#### Understanding the Flash Attention kernel implementation\n",
    "\n",
    "The kernel implementation consists of the following key stages:\n",
    "\n",
    "**Initialization**\n",
    "\n",
    "* Calculates the normalization factor (`scale`) and the number of KV heads (`head_kv`).\n",
    "* Defines the tensor shapes and data types (`float16` for computation and `float` for accumulation to ensure precision).\n",
    "\n",
    "**Kernel definition**\n",
    "\n",
    "* Uses `@T.prim_func` to define the core computation function.\n",
    "* `T.Kernel` specifies parallel dimensions: `num_split_q` (parallel splitting of `Q`) and `batch*heads` (combined parallelism of `batch` and `heads`).\n",
    "* The `threads` parameter specifies the number of threads per thread block.\n",
    "\n",
    "**Hardware optimization**\n",
    "\n",
    "* `T.use_swizzle` enables memory reordering optimization to improve memory access efficiency on the Instinct MI300X.\n",
    "* Decomposes `byz_combined` into batch index (`bz`) and attention head index (`by`) for parallel processing.\n",
    "\n",
    "**Q tile processing and cache initialization**\n",
    "\n",
    "* Calculates the number of Q tiles (`num_q_blocks`) and traverses each ``Q`` tile using a while loop.\n",
    "* Creates `acc_o` (output accumulator), `m_i` (row-wise maximum), and `l_i` (row-wise sum) for numerical stability in the softmax computation.\n",
    "* Allocates shared memory (`Q_shared`, `K_shared`, and `V_shared`) to cache tile data and allocates registers (`acc_s_cast`) to cache intermediate results.\n",
    "* Loads the current ``Q`` tile from HBM to shared memory with `coalesced_width` to improve memory bandwidth utilization.\n",
    "\n",
    "**K/V tile traversal and core computation**\n",
    "\n",
    "* Determines the traversal end of K/V tiles based on causality. In causal mode, only K/V tiles before the current ``Q`` position are processed.\n",
    "* Loads the K/V tiles to shared memory using the same memory coalescing strategy as ``Q``.\n",
    "* Applies causal masking by setting non-compliant positions in `acc_s` to `-∞`.\n",
    "* Calls TileLang's built-in GEMM primitive to compute `QK^T` using `transpose_B=True`, `k_pack` for data packing optimization, and `GemmWarpPolicy.FullRow` to adapt to the Instinct MI300X warp scheduling strategy.\n",
    "* Computes softmax incrementally (finding maximum value, scale, and compute exp) to avoid numerical overflow.\n",
    "* Performs the GEMM computation between normalized attention weights (`acc_s`) and V, accumulating the results into `acc_o`.\n",
    "\n",
    "**Output generation**\n",
    "\n",
    "* Calculates the reciprocal of the row sum (`l_inv`) and performs final normalization on the accumulator `acc_o`.\n",
    "* Updates `bx` to process the next `Q` tile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e31907a",
   "metadata": {},
   "source": [
    "### Prepare the input parameters\n",
    "\n",
    "Configure the input parameters for the Flash Attention kernel. The following example uses a typical LLM scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354599eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario parameters (typical LLM decoding/inference)\n",
    "batch = 1\n",
    "heads = 8\n",
    "seq_len = 4096\n",
    "dim = 128\n",
    "is_causal = True  # Enable causal masking for generation\n",
    "groups = 1        # No grouped attention\n",
    "flops_per_matmul = 2.0 * batch * heads * seq_len * seq_len * dim\n",
    "total_flops = 2 * flops_per_matmul\n",
    "if is_causal:\n",
    "    total_flops *= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1dff5a",
   "metadata": {},
   "source": [
    "### Run TileLang Flash Attention\n",
    "\n",
    "Trigger autotuning to search for the optimal configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f21229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with autotuning (first run takes ~1s for configuration search)\n",
    "print(\"Starting autotuning for FlashAttention-V2...\")\n",
    "kernel = fast_flashattn(batch, heads, seq_len, dim, is_causal, groups=groups)\n",
    "print(f\"Autotuning finished. Best Configuration: {kernel.config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc5be2",
   "metadata": {},
   "source": [
    "### Verify correctness and test performance\n",
    "\n",
    "After successfully invoking the kernel and finding an optimal configuration, verify its correctness by comparing your results against the native PyTorch implementation, then benchmark the performance.\n",
    "\n",
    "#### Correctness verification\n",
    "\n",
    "TileLang includes a built-in profiler to verify correctness using `torch.allclose` to check if results are consistent within a small precision tolerance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d7471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PyTorch native result (reference)\n",
    "ref_program_processed = partial(ref_program, is_causal=is_causal, groups=groups)\n",
    "# best results and all tune results are stored in profiler\n",
    "profiler = kernel.get_profiler(tensor_supply_type=tilelang.TensorSupplyType.Normal)\n",
    "print(\"Verifying correctness...\")\n",
    "profiler.assert_allclose(ref_program_processed, rtol=0.01, atol=0.01)\n",
    "print(\"All checks pass.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f3815",
   "metadata": {},
   "source": [
    "The expected output is: `Verifying correctness...All checks pass.`\n",
    "\n",
    "#### Performance testing\n",
    "\n",
    "Benchmark the kernel performance using the profiler's built-in benchmark tool, which measures kernel latency by synchronizing before and after kernel execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13101d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "latency = profiler.do_bench(ref_program_processed, warmup=100)\n",
    "print(f\"Reference (PyTorch): {latency:.2f} ms | {total_flops / latency * 1e-9:.2f} TFlops\")\n",
    "\n",
    "latency = profiler.do_bench(warmup=100)\n",
    "print(\n",
    "        f\"Fast Flash Attention V2 (Tile-lang): {latency:.2f} ms | {total_flops / latency * 1e-9:.2f} TFlops\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a30fb",
   "metadata": {},
   "source": [
    "The expected output is:\n",
    "\n",
    "    Reference (PyTorch): 0.89 ms | 77.18 TFlops\n",
    "    Fast Flash Attention V2 (TileLang): 0.37 ms | 187.52 TFlops\n",
    "\n",
    "The TileLang implementation achieves more than twice the speedup compared to the native PyTorch implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ff3efc",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! By following this TileLang Flash Attention tutorial, you learned how to implement and optimize Flash Attention on an AMD Instinct MI300X GPU using TileLang.\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "- **Environment setup**: ROCm 7.0 and the TileLang Docker environment are essential for Instinct MI300X compatibility.\n",
    "- **Performance optimization**: Tiled computation with optimized block sizes and Instinct MI300X-specific optimizations (such as memory swizzling) greatly reduces latency compared to PyTorch.\n",
    "- **Practical application**: The kernel can be integrated into LLM frameworks, like vLLM or SGLang, for end-to-end acceleration.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "1. **Experiment with different scenarios**: Modify `batch`, `seq_len`, or `heads` to match the LLM requirements (for example, `batch=32` for batch inference).\n",
    "2. **Advanced autotuning**: Extend `get_configs()` with additional tile sizes to explore further performance improvements.\n",
    "3. **Framework integration**: Replace the attention operator in models such as DeepSeek or Llama, using `fast_flashattn` for full-model acceleration.\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "- [TileLang GitHub](https://github.com/tile-ai/TileLang)\n",
    "- [ROCm documentation](https://rocm.docs.amd.com/en/latest/index.html)\n",
    "- [AMD AI Developer Hub](https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
