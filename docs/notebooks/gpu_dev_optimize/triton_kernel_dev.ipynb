{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ed5955",
   "metadata": {},
   "source": [
    "# Kernel development and optimization with Triton\n",
    "\n",
    "[OpenAI Triton](https://github.com/triton-lang/triton) is an open-source programming language that is supported by AMD GPUs and is designed to simplify GPU programming for high-performance tasks, particularly in AI applications. This tutorial demonstrates how to set up the Triton development environment and optimize Triton kernel performance on AMD GPUs. \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup. \n",
    "\n",
    "### Operating system\n",
    "\n",
    "* **Ubuntu 22.04/24.04**: Ensure your system is running Ubuntu version 22.04 or 24.04.\n",
    "\n",
    "### Hardware\n",
    "\n",
    "* **AMD Instinctâ„¢ GPUs**: This tutorial was tested on an AMD Instinct MI300X GPU. Ensure you are using an AMD Instinct GPU or compatible hardware with ROCm support and that your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "\n",
    "### Software\n",
    "\n",
    "* **ROCm 6.2, 6.3, or 6.4**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html). \n",
    "\n",
    "  After installation, confirm your setup using the `rocm-smi` command. AMD also provides prebuilt ROCm Docker images, for example, a [ROCm PyTorch image](https://hub.docker.com/r/rocm/pytorch), [ROCm Ubuntu 22.04 image](https://hub.docker.com/r/rocm/dev-ubuntu-22.04), and [ROCm Ubuntu 24.04 image](https://hub.docker.com/r/rocm/dev-ubuntu-24.04). You can use these prebuilt Docker images to reduce the effort required to set up a ROCm environment.  \n",
    "\n",
    "* **Docker**: Ensure Docker is installed and configured correctly. Follow the Docker installation guide for your operating system.\n",
    "\n",
    "  **Note**: Ensure the Docker permissions are correctly configured. To configure permissions to allow non-root access, run the following commands:\n",
    "\n",
    "  ``` bash\n",
    "  sudo usermod -aG docker $USER\n",
    "  newgrp docker\n",
    "  ```\n",
    "\n",
    "  Verify Docker is working correctly:\n",
    "\n",
    "  ``` bash\n",
    "  docker run hello-world\n",
    "  ```\n",
    "\n",
    "## Set up the Triton development environment \n",
    "\n",
    "This tutorial uses the prebuilt ROCm PyTorch image, but you can also try other ROCm environments as the base image. \n",
    "\n",
    "### Step 1: Launch the Docker image\n",
    "\n",
    "Launch the Docker container. Replace `/path/to/Triton_Sample` with the full path to the directory on your host machine where the Triton sample code is located.\n",
    "\n",
    "```bash\n",
    "docker run -it --rm \\\n",
    "  --network=host \\\n",
    "  --device=/dev/kfd \\\n",
    "  --device=/dev/dri \\\n",
    "  --group-add=video \\\n",
    "  --ipc=host \\\n",
    "  --cap-add=SYS_PTRACE \\\n",
    "  --security-opt seccomp=unconfined \\\n",
    "  --shm-size 8G \\\n",
    "  -v $(pwd):/workspace \\\n",
    "  -w /workspace/notebooks \\\n",
    "  rocm/pytorch:latest\n",
    "```\n",
    "\n",
    "**Note**: This command mounts the current directory to the `/workspace` directory in the container. Ensure the notebook file is either copied to this directory before running the Docker command or uploaded into the Jupyter Notebook environment after it starts. Save the token or URL provided in the terminal output to access the notebook from your web browser. You can download this notebook from the [AI Developer Hub GitHub repository](https://github.com/ROCm/gpuaidev).\n",
    "\n",
    "### Step 2: Launch Jupyter Notebooks in the container\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "``` bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Start the Jupyter server:\n",
    "\n",
    "``` bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`.\n",
    "\n",
    "**Note**: The rest of this tutorial can run as interactive blocks in your Jupyter notebook after you upload this tutorial to your server.\n",
    "\n",
    "### Step 3: Install OpenAI Triton\n",
    "\n",
    "Before you can install the correct version of OpenAI Triton, you must uninstall any old versions.\n",
    "\n",
    "#### 1. Uninstall the old version of Triton\n",
    "\n",
    "It's strongly recommended that you use the latest version of Triton in your project. AMD and other vendors frequently update their optimization passes and algorithms in [OpenAI Triton](https://github.com/triton-lang/triton). These updates can improve your Triton kernel performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fea56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c3edf",
   "metadata": {},
   "source": [
    "#### 2. Install OpenAI Triton from the source code\n",
    "\n",
    "The detailed steps to install Triton are listed below.\n",
    "\n",
    "**Note:** If you have any questions or issues when building Triton, submit them to [Triton Issues](https://github.com/triton-lang/triton/issues).   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Remove existing Triton folder if it exists\n",
    "if [ -d \"triton\" ]; then\n",
    "    echo \"Removing existing triton directory...\"\n",
    "    rm -rf triton\n",
    "fi\n",
    "\n",
    "# Clone Triton repo\n",
    "git clone https://github.com/triton-lang/triton.git\n",
    "\n",
    "# Install dependencies and Triton from source (non-editable install)\n",
    "cd triton\n",
    "pip install -r python/requirements.txt\n",
    "pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e4b7a4",
   "metadata": {},
   "source": [
    "### Step 4: Validate Triton on an AMD GPU\n",
    "\n",
    "After Triton is successfully installed, validate whether it works properly on an AMD GPU. Run the following vector-add sample in Python to confirm that the Triton kernel provides similar results as the Torch APIs, which means it's performing efficiently on AMD GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad9f809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "@triton.jit\n",
    "def add_kernel(x_ptr,  # *Pointer* to first input vector.\n",
    "               y_ptr,  # *Pointer* to second input vector.\n",
    "               output_ptr,  # *Pointer* to output vector.\n",
    "               n_elements,  # Size of the vector.\n",
    "               BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n",
    "               # NOTE: `constexpr` so it can be used as a shape value.\n",
    "               ):\n",
    "    # There are multiple 'programs' processing different data. We identify which program\n",
    "    # we are here:\n",
    "    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n",
    "    # This program will process inputs that are offset from the initial data.\n",
    "    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n",
    "    # would each access the elements [0:64, 64:128, 128:192, 192:256].\n",
    "    # Note that offsets is a list of pointers:\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    # Create a mask to guard memory operations against out-of-bounds accesses.\n",
    "    mask = offsets < n_elements\n",
    "    # Load x and y from DRAM, masking out any extra elements in case the input is not a\n",
    "    # multiple of the block size.\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    output = x + y\n",
    "    # Write x + y back to DRAM.\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Let's also declare a helper function to (1) allocate the `z` tensor\n",
    "# and (2) enqueue the above kernel with appropriate grid/block sizes:\n",
    "\n",
    "def add(x: torch.Tensor, y: torch.Tensor):\n",
    "    # We need to preallocate the output.\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.device == DEVICE and y.device == DEVICE and output.device == DEVICE\n",
    "    n_elements = output.numel()\n",
    "    # The SPMD launch grid denotes the number of kernel instances that run in parallel.\n",
    "    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n",
    "    # In this case, we use a 1D grid where the size is the number of blocks:\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n",
    "    # NOTE:\n",
    "    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\n",
    "    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\n",
    "    #  - Don't forget to pass meta-parameters as keywords arguments.\n",
    "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n",
    "    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\n",
    "    # running asynchronously at this point.\n",
    "    return output\n",
    "\n",
    "\n",
    "# %%\n",
    "# We can now use the above function to compute the element-wise sum of two `torch.tensor` objects and test its correctness:\n",
    "\n",
    "torch.manual_seed(0)\n",
    "size = 98432\n",
    "x = torch.rand(size, device=DEVICE)\n",
    "y = torch.rand(size, device=DEVICE)\n",
    "output_torch = x + y\n",
    "output_triton = add(x, y)\n",
    "print(output_torch)\n",
    "print(output_triton)\n",
    "print(f'The maximum difference between torch and triton is '\n",
    "      f'{torch.max(torch.abs(output_torch - output_triton))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c529af",
   "metadata": {},
   "source": [
    "The output log is:\n",
    "\n",
    "    tensor([1.3713, 1.3076, 0.4940,  ..., 0.6724, 1.2141, 0.9733], device='cuda:0')\n",
    "    tensor([1.3713, 1.3076, 0.4940,  ..., 0.6724, 1.2141, 0.9733], device='cuda:0')\n",
    "    The maximum difference between torch and triton is 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4b5b9",
   "metadata": {},
   "source": [
    "## Optimize the Triton code for AMD GPUs\n",
    "\n",
    "The softmax function is often used in convolutional neural network (CNN) classification models and even Transformer-based LLM models. It converts raw output scores, or logits, into probabilities by taking the exponential of each value and normalizing these values by dividing by the sum of all the exponentials. This process ensures that the output values are in the range (0,1) and sum to 1 to allow them to be interpreted as probabilities. PyTorch implements this function as [a standard API](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html). \n",
    "\n",
    "### Naive version \n",
    "\n",
    "According to the specification, you implemented the naive version of the softmax algorithm in the Triton kernel. To determine the maximum data point and the corresponding sum of all the exponentials, this kernel version uses two for-loops, along with one more for-loop to calculate the final softmax result, for a total of three loops in all. \n",
    "\n",
    "The following example tests the kernel performance on an 8192 x 8192 tensor, with a block size for the column dimension of 256. After running the warmup section to avoid including the kernel compilation time in the final data, you can obtain the performance data for the naive version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a607a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "@triton.jit\n",
    "def softmax_kernel_naive(in_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    in_max = -float('inf')\n",
    "    for offset in range(0, n_cols, BLOCK_SIZE):\n",
    "        col_range = tl.arange(0, BLOCK_SIZE)\n",
    "        col_mask = col_range + offset < n_cols\n",
    "        in_data = tl.load(in_ptr + pid * row_stride + col_range + offset, mask=col_mask, other=-float('inf'))\n",
    "        in_max = tl.maximum(in_max, tl.max(in_data, axis=-1))\n",
    "    \n",
    "    in_exp_sum = 0.0\n",
    "    for offset in range(0, n_cols, BLOCK_SIZE):\n",
    "        col_range = tl.arange(0, BLOCK_SIZE)\n",
    "        col_mask = col_range + offset < n_cols\n",
    "        in_data = tl.load(in_ptr + pid * row_stride + col_range + offset, mask=col_mask, other=-float('inf'))\n",
    "        in_exp_sum = in_exp_sum + tl.sum(tl.exp(in_data - in_max), axis=-1)\n",
    "    \n",
    "    for offset in range(0, n_cols, BLOCK_SIZE):\n",
    "        col_range = tl.arange(0, BLOCK_SIZE)\n",
    "        col_mask = col_range + offset < n_cols\n",
    "        in_data = tl.load(in_ptr + pid * row_stride + col_range + offset, mask=col_mask)\n",
    "        in_exp = tl.exp(in_data - in_max)\n",
    "        tl.store(output_ptr + pid * row_stride + col_range + offset, in_exp / in_exp_sum, mask=col_mask)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(8192, 8192, device=DEVICE)\n",
    "n_rows, n_cols = x.shape\n",
    "output_triton = torch.empty_like(x)\n",
    "BLOCK_SIZE = 256\n",
    "temp = torch.randn(n_rows, n_cols, device=DEVICE)\n",
    "softmax_kernel_naive[(n_rows,)](\n",
    "        temp,\n",
    "        output_triton,\n",
    "        temp.stride(0),\n",
    "        n_cols,\n",
    "        BLOCK_SIZE\n",
    ")#warmup\n",
    "torch.cuda.empty_cache() #clean cache\n",
    "\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start_event.record()\n",
    "softmax_kernel_naive[(n_rows,)](\n",
    "        x,\n",
    "        output_triton,\n",
    "        x.stride(0),\n",
    "        n_cols,\n",
    "        BLOCK_SIZE\n",
    ")\n",
    "end_event.record()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "print(f'Softmax Triton Naive Version Elapsed: {elapsed_time_ms:.3f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e208055b",
   "metadata": {},
   "source": [
    "\n",
    "### Online softmax version \n",
    "\n",
    "It's easy to use the Triton language to implement algorithms. To obtain better performance from the current kernel, first determine whether there's a more efficient algorithm or solution. If so, try the new algorithm in your Triton Kernel. To reduce the memory accesses caused by the three for-loops in the naive softmax algorithm, a new online softmax algorithm has been proposed in the [Online normalizer calculation for softmax](https://arxiv.org/pdf/1805.02867) paper.\n",
    "\n",
    "In accordance with the online softmax algorithm, the following code makes a few modifications to the naive version kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d608d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "@triton.jit\n",
    "def softmax_kernel_v1(in_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    in_max = -float('inf')\n",
    "    in_exp_sum = 0.0\n",
    "    for offset in range(0, n_cols, BLOCK_SIZE):\n",
    "        col_range = tl.arange(0, BLOCK_SIZE)\n",
    "        col_mask = col_range + offset < n_cols\n",
    "        in_data = tl.load(in_ptr + pid * row_stride + col_range + offset, mask=col_mask, other=-float('inf'))\n",
    "        in_max_new = tl.maximum(in_max, tl.max(in_data, axis=-1))\n",
    "        in_exp_sum = in_exp_sum * tl.exp(in_max - in_max_new) + tl.sum(tl.exp(in_data - in_max_new), axis=-1)\n",
    "        in_max = in_max_new\n",
    "    \n",
    "    for offset in range(0, n_cols, BLOCK_SIZE):\n",
    "        col_range = tl.arange(0, BLOCK_SIZE)\n",
    "        col_mask = col_range + offset < n_cols\n",
    "        in_data = tl.load(in_ptr + pid * row_stride + col_range + offset, mask=col_mask)\n",
    "        in_exp = tl.exp(in_data - in_max)\n",
    "        tl.store(output_ptr + pid * row_stride + col_range + offset, in_exp / in_exp_sum, mask=col_mask)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(8192, 8192, device=DEVICE)\n",
    "n_rows, n_cols = x.shape\n",
    "output = torch.empty_like(x)\n",
    "BLOCK_SIZE = 256\n",
    "temp = torch.randn(n_rows, n_cols, device=DEVICE)\n",
    "softmax_kernel_v1[(n_rows,)](\n",
    "        temp,\n",
    "        output,\n",
    "        temp.stride(0),\n",
    "        n_cols,\n",
    "        BLOCK_SIZE\n",
    ")#warmup\n",
    "torch.cuda.empty_cache() #clean cache\n",
    "\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start_event.record()\n",
    "softmax_kernel_v1[(n_rows,)](\n",
    "        x,\n",
    "        output,\n",
    "        x.stride(0),\n",
    "        n_cols,\n",
    "        BLOCK_SIZE\n",
    ")\n",
    "end_event.record()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "\n",
    "print(f'Softmax Triton V1 Version Elapsed: {elapsed_time_ms:.3f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df425375",
   "metadata": {},
   "source": [
    "\n",
    "### Fused-softmax version\n",
    "\n",
    "OpenAI Triton provides the \"fused-softmax\" softmax reference example. Based on the online softmax algorithm, it continues to simplify the maximum data calculation by removing one for-loop. It also tells the compiler to use more threads per row by increasing the number of warps. This configuration is often tuned for better performance. Finally, it improves the kernel launching scheme by adjusting the GPU hardware configuration, which can lead to higher GPU kernel occupancy and better performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8684a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "def is_hip():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n",
    "\n",
    "\n",
    "def is_cdna():\n",
    "    return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',\n",
    "                                                                                   'gfx90a', 'gfx908')\n",
    "\n",
    "@triton.jit\n",
    "def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,\n",
    "                   num_stages: tl.constexpr):\n",
    "    # starting row of the program\n",
    "    row_start = tl.program_id(0)\n",
    "    row_step = tl.num_programs(0)\n",
    "    for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):\n",
    "        # The stride represents how much we need to increase the pointer to advance 1 row\n",
    "        row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "        # The block size is the next power of two greater than n_cols, so we can fit each\n",
    "        # row in a single block\n",
    "        col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "        input_ptrs = row_start_ptr + col_offsets\n",
    "        # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n",
    "        mask = col_offsets < n_cols\n",
    "        row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n",
    "        # Subtract maximum for numerical stability\n",
    "        row_minus_max = row - tl.max(row, axis=0)\n",
    "        # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n",
    "        numerator = tl.exp(row_minus_max)\n",
    "        denominator = tl.sum(numerator, axis=0)\n",
    "        softmax_output = numerator / denominator\n",
    "        # Write back output to DRAM\n",
    "        output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "        output_ptrs = output_row_start_ptr + col_offsets\n",
    "        tl.store(output_ptrs, softmax_output, mask=mask)\n",
    "\n",
    "properties = driver.active.utils.get_device_properties(DEVICE.index)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "WARP_SIZE = properties[\"warpSize\"]\n",
    "target = triton.runtime.driver.active.get_current_target()\n",
    "kernels = {}\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(8192, 8192, device=DEVICE)\n",
    "n_rows, n_cols = x.shape\n",
    "# Allocate output\n",
    "y = torch.empty_like(x)\n",
    "\n",
    "# The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`\n",
    "BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "\n",
    "# Another trick we can use is to ask the compiler to use more threads per row by\n",
    "# increasing the number of warps (`num_warps`) over which each row is distributed.\n",
    "num_warps = 8\n",
    "\n",
    "# Number of software pipelining stages.\n",
    "num_stages = 4 if SIZE_SMEM > 200000 else 2\n",
    "\n",
    "# pre-compile kernel to get register usage and compute thread occupancy.\n",
    "kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,\n",
    "                                   num_stages=num_stages, num_warps=num_warps, grid=(1, ))\n",
    "kernel._init_handles()\n",
    "n_regs = kernel.n_regs\n",
    "size_smem = kernel.metadata.shared\n",
    "\n",
    "if is_hip():\n",
    "    # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.\n",
    "    # However, this is not always the case. In most cases all registers can be used as regular purpose registers.\n",
    "    # ISA SECTION (3.6.4 for CDNA3)\n",
    "    # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used\n",
    "    # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total\n",
    "    # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is\n",
    "    # not required to be equal numbers of both types.\n",
    "    if is_cdna():\n",
    "        NUM_GPRS = NUM_REGS * 2\n",
    "\n",
    "    # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.\n",
    "    # When we divide this number with WARP_SIZE we get maximum number of waves that can\n",
    "    # execute on a CU (multi-processor)  in parallel.\n",
    "    MAX_NUM_THREADS = properties[\"max_threads_per_sm\"]\n",
    "    max_num_waves = MAX_NUM_THREADS // WARP_SIZE\n",
    "    occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps\n",
    "else:\n",
    "    occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
    "    \n",
    "occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
    "num_programs = NUM_SM * occupancy\n",
    "\n",
    "num_programs = min(num_programs, n_rows)\n",
    "\n",
    "# Create a number of persistent programs.\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start_event.record()\n",
    "kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)\n",
    "end_event.record()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "print(f'Softmax Triton V2 Version Elapsed: {elapsed_time_ms:.3f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f798046",
   "metadata": {},
   "source": [
    "\n",
    "## Summary \n",
    "In this tutorial, you learned how to develop and optimize Triton kernels on AMD GPUs. To learn more about OpenAI Triton, see the [official Triton documentation](https://triton-lang.org/main/index.html). To find out more about running Triton on AMD GPUs, see [ROCm Triton optimization](https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference-optimization/optimizing-triton-kernel.html) and the [Kernel development optimization on Triton blog](https://rocm.blogs.amd.com/software-tools-optimization/kernel-development-optimizations-with-triton-on-/README.html). Hopefully, this tutorial encourages you to tune, test, and contribute to Triton on AMD GPUs and help shape the future of AI acceleration.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comfyui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
