{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f2aa3c",
   "metadata": {},
   "source": [
    "# Profiling LLaMA 4 Inference with vLLM\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Profiling is essential for understanding the performance bottlenecks in large language model inference pipelines. In this tutorial, we walk through the process of profiling the **LLaMA 4 Scout 17B-16E-Instruct** model using the `vLLM` framework on AMD GPUs with ROCm. We'll capture detailed kernel traces and later demonstrate how to visualize them using perfetto.\n",
    "\n",
    "## Prerequisites\n",
    "Before starting this tutorial, ensure you have the following:\n",
    "\n",
    "- Access to the gated model **LLaMA 4 Scout 17B-16E-Instruct**\n",
    "- Access to **Perfetto UI** \n",
    "\n",
    "### Hardware\n",
    "\n",
    "* **AMD GPUs**: Ensure you are using an AMD GPU, such as the Instinct™ MI300X or Radeon Pro W7900, with ROCm support and that your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "### Software\n",
    "\n",
    "* **ROCm 6.3, 6.4**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html). Verify your ROCm installation by running this command:\n",
    "\n",
    "   ``` bash\n",
    "   rocm-smi\n",
    "   ```\n",
    "   This command produces a table similar to the one below:\n",
    "\n",
    "   ![rocm-smi](../assets/rocm-smi.png)\n",
    "\n",
    "* **Docker**: Ensure Docker is installed and configured correctly. See the [Docker installation guide](https://docs.docker.com/get-docker/) for more information.\n",
    "\n",
    "### 1. Pull the Docker image\n",
    "\n",
    "Ensure your system meets the [system requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "Pull the Docker image required for this tutorial:\n",
    "\n",
    "``` bash\n",
    "docker pull rocm/vllm-dev:main\n",
    "```\n",
    "\n",
    "### 2. Launch the Docker container\n",
    "\n",
    "Launch the Docker container in a terminal on your server and map the necessary directories.\n",
    "\n",
    "``` bash\n",
    "docker run -it --rm \\\n",
    "  --network=host \\\n",
    "  --device=/dev/kfd \\\n",
    "  --device=/dev/dri \\\n",
    "  --group-add=video \\\n",
    "  --ipc=host \\\n",
    "  --cap-add=SYS_PTRACE \\\n",
    "  --security-opt seccomp=unconfined \\\n",
    "  --shm-size 8G \\\n",
    "  -v $(pwd):/workspace \\\n",
    "  -w /workspace/notebooks \\\n",
    "  rocm/vllm:latest\n",
    "```\n",
    "\n",
    "**Note**: This command mounts the current directory to the `/workspace` directory in the container. Ensure the notebook file is either copied to this directory before running the Docker command or uploaded into the Jupyter Notebook environment after it starts. Save the token or URL provided in the terminal output to access the notebook from your web browser. You can download this notebook from the [AI Developer Hub GitHub repository](https://github.com/ROCm/gpuaidev).\n",
    "\n",
    "### Install and launch Jupyter\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "``` bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Start the Jupyter server:\n",
    "\n",
    "``` bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3210e0f",
   "metadata": {},
   "source": [
    "## Step by step process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbc92f6",
   "metadata": {},
   "source": [
    "### Step 1: Hugging Face Login\n",
    "Provide your Hugging Face token\n",
    "\n",
    "You'll require a Hugging Face API token to access Llama-3.1. Generate your token at [Hugging Face Tokens](https://huggingface.co/settings/tokens) and request access for [LLaMA 4 Scout 17B-16E-Instruct](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct). Tokens typically start with \"hf_\". \n",
    "\n",
    "Run the following interactive block in your Jupyter notebook to set up the token:\n",
    "\n",
    "**Note**: Uncheck the \"Add token as Git credential\" option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc811a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "# Prompt the user to log in\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1816dd",
   "metadata": {},
   "source": [
    "Verify that your token was accepted correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1abfde8",
   "metadata": {},
   "source": [
    "### Step 2: Start the vLLM Server with profiler configs\n",
    "Open a new terminal tab inside your JupyterLab session.\n",
    "In this new terminal, run the following commands and keep the terminal open.\n",
    "```shell\n",
    "mkdir -p /profile\n",
    "export VLLM_TORCH_PROFILER_DIR=/profile\n",
    "\n",
    "# Start the vLLM server with standard configs\n",
    "RCCL_MSCCL_ENABLE=0 \\\n",
    "VLLM_USE_V1=1 \\\n",
    "VLLM_WORKER_MULTIPROC_METHOD=spawn \\\n",
    "VLLM_USE_MODELSCOPE=False \\\n",
    "VLLM_USE_TRITON_FLASH_ATTN=0 \\\n",
    "vllm serve meta-llama/Llama-4-Scout-17B-16E-Instruct \\\n",
    "    --disable-log-requests \\\n",
    "    -tp 8 \\\n",
    "    --max-num-seqs 64 \\\n",
    "    --no-enable-prefix-caching \\\n",
    "    --max_num_batched_tokens=320000 \\\n",
    "    --max-model-len 32000\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83de3f24",
   "metadata": {},
   "source": [
    "### Step 3: Run the Benchmark and Capture the Trace\n",
    "\n",
    "With the server running, trigger a synthetic benchmark request to generate traffic and collect profiling data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf84bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!vllm bench serve \\\n",
    "    --model meta-llama/Llama-4-Scout-17B-16E-Instruct \\\n",
    "    --dataset-name random \\\n",
    "    --random-input-len 2000 \\\n",
    "    --random-output-len 10 \\\n",
    "    --max-concurrency 64 \\\n",
    "    --num-prompts 64 \\\n",
    "    --ignore-eos \\\n",
    "    --percentile_metrics ttft,tpot,itl,e2el \\\n",
    "    --profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721833ce",
   "metadata": {},
   "source": [
    "Make sure to add the —profile flag, which basically starts a profiler before the benchmark starts and then end and write the trace once the profiler ends.\n",
    "\n",
    "***NOTE*** Once the above cell execution is completed, stop the vLLM server you started in the other terminal by pressing `ctrl` + `c` at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6f4cac",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 4 : Visualize the Trace Using [ui.perfetto.dev](https://ui.perfetto.dev/)\n",
    "\n",
    "Once the trace is generated unzip it and it will be of the json form, it's time to visualize it using Perfetto, a powerful trace viewer built for large-scale profiling data. This helps uncover latency bottlenecks, CPU–GPU overlap, and kernel-level inefficiencies in your inference pipeline.\n",
    "\n",
    "- Go to: [https://ui.perfetto.dev](https://ui.perfetto.dev/)\n",
    "- Click **\"Open trace file\"**\n",
    "- Upload the `.json` file\n",
    "\n",
    "Once you open the trace it will look something like this:\n",
    "\n",
    "![image_0](../assets/image_0.png)\n",
    "\n",
    "\n",
    "#### Understanding Prefill and Decode Timelines\n",
    "\n",
    "Let’s now zoom into the trace and interpret what each slice reveals about LLaMA 4’s execution stages. Here's a close-up of two important process timelines captured in Perfetto:\n",
    "\n",
    "> Our focus here is on the first two tracks: python3 3906 and python3 2.\n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "#### `python3 3906` – Asynchronous CPU Calls\n",
    "\n",
    "The **orange slice** under `python3 3906` shows the execution of high-level model code on the CPU. It contains:\n",
    "\n",
    "- Calls like `execute_model`, `forward`, and `hipMemcpyWithStream`\n",
    "- PyTorch internals like `aten::to`, `aten::copy_`\n",
    "- The actual forward pass for `Llama4ForCausalLM` and memory transfers\n",
    "\n",
    "This slice reflects the CPU-side orchestration of inference — from input preparation to dispatching kernels to the GPU.\n",
    "\n",
    "---\n",
    "\n",
    "#### `python3 2` – GPU Kernel Timeline\n",
    "\n",
    "The **pink slice** under `python3 2` is where GPU kernel execution is visualized. This slice represents actual compute work being done by the GPU after CPU enqueues the tasks.\n",
    "\n",
    "Here’s the key insight:\n",
    "\n",
    "- There is a **clear gap** between two bursts of kernel execution.\n",
    "- This gap separates two distinct phases:\n",
    "    - **Before the gap**: **Prefill** — where the initial prompt is encoded and attention/cache states are populated.\n",
    "    - **After the gap**: **Decode** — where the model generates tokens, typically using cached key/value tensors.\n",
    "\n",
    "#### Understanding Kernel Timelines:\n",
    "\n",
    "![image](../assets/image_1.png)\n",
    "\n",
    "\n",
    "When you expand the `python3 2` timeline in Perfetto, you’ll see two distinct GPU streams representing different types of operations executed on the device:\n",
    "\n",
    "> This zoomed-in view helps us distinguish between computation kernels and communication operations.\n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "#### Stream `3 3`: All GPU Kernels\n",
    "\n",
    "Stream `3 3` is the **primary compute stream**, where **all GPU kernel executions** are scheduled. This includes:\n",
    "\n",
    "- `MatMul` / GEMM operations\n",
    "- Fused MLP and attention blocks\n",
    "- Positional encodings, LayerNorms\n",
    "- Any fused or element-wise kernels\n",
    "\n",
    "This stream is densely packed, showing the **bulk of model inference** in action. The rhythm and spacing of these kernels help diagnose things like:\n",
    "\n",
    "- Load balancing across tensor parallel ranks\n",
    "- Gaps between kernel launches\n",
    "- Prefill vs Decode phases (based on density before/after gaps)\n",
    "\n",
    "---\n",
    "\n",
    "#### Stream `3 8`: AllGather Kernels\n",
    "\n",
    "Stream `3 8` is used specifically for **AllGather operations**, which are part of the tensor parallel communication process. These kernels:\n",
    "\n",
    "- Synchronize activations across devices in multi-GPU setups\n",
    "- Typically occur **between layer boundaries**\n",
    "- Are crucial in `tp=8` setups for syncing partial outputs across 8 shards\n",
    "\n",
    "### Zooming In: Analyzing the Attention Forward Kernel\n",
    "\n",
    "Attention is at the heart of all Transformer-based language models — and LLaMA 4 is no exception. Profiling its **attention forward kernel** gives us critical insights into computational efficiency.\n",
    "\n",
    "Let’s dive into the trace and inspect it at the kernel level.\n",
    "\n",
    "---\n",
    "\n",
    "#### Zoom Into the Kernel Timeline\n",
    "\n",
    "- Navigate to `python3 2` → `stream 3 3` in the Perfetto UI.\n",
    "- **Use your mouse scroll** or drag to zoom into a cluster of dense kernels.\n",
    "- Hover over one of the prominent kernels — you should see a label like `_fwd_kernel`.\n",
    "\n",
    "Here’s an example image from our trace:\n",
    "\n",
    "![image](../assets/image_2.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Understanding the Kernel Slice\n",
    "\n",
    "From the **details panel** below the trace view, we can extract:\n",
    "\n",
    "| Field | Value |\n",
    "| --- | --- |\n",
    "| **Name** | `_fwd_kernel` |\n",
    "| **Category** | `kernel` |\n",
    "| **Stream** | `stream 3 (3)` |\n",
    "| **Process** | `python3 (2)` |\n",
    "| **Duration** | `788µs 256ns` |\n",
    "| **Launch Delay** | `~819µs` from CPU call |\n",
    "\n",
    "This kernel is part of the **multi-head attention forward pass**, one of the most compute-heavy operations in inference.\n",
    "\n",
    "---\n",
    "\n",
    "#### Trace It Back to the CPU\n",
    "\n",
    "Inside the kernel details near proceeding flows, you’ll find `hipModuleLaunchKernel` \n",
    "\n",
    "Clicking this will **jump back to the CPU thread** (`python3 3906`) in blue in the below picture that issued this kernel launch. This feature is incredibly useful for:\n",
    "\n",
    "- Mapping GPU ops to their Python or C++ call stack\n",
    "- Identifying bottlenecks in dispatch or synchronization\n",
    "- Understanding how long the CPU takes to enqueue work on the GPU\n",
    "\n",
    "![image](../assets/image_3.png)\n",
    "\n",
    "\n",
    "Following the above process you can track which kernel takes how much time in both prefill and decode, one of our short summary are mentioned in the below chart.\n",
    "\n",
    "![image](../assets/image_4.png)\n",
    "\n",
    "### Step 7 Programmatic Analysis:  Extracting GPU Kernel Timeline with Python\n",
    "\n",
    "To go beyond visual inspection, you can also parse the trace programmatically to list all GPU kernels and their durations. This is helpful when you're tracking:\n",
    "\n",
    "- Kernel launch patterns\n",
    "- Duration spikes\n",
    "- Gaps or anomalies in execution\n",
    "\n",
    "Here’s a minimal Python script that reads the `trace.json` file and lists all GPU kernels sorted by start time:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea17db2",
   "metadata": {},
   "source": [
    "\n",
    "Decompress the .gz trace file into trace.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip -c /profile/$(ls /profile | head -n 1) > trace.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a01655b",
   "metadata": {},
   "source": [
    "Run the below cell which will lists all GPU kernels sorted by start time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Load the trace\n",
    "with open(\"trace.json\", \"r\") as f:\n",
    "    trace = json.load(f)\n",
    "\n",
    "gpu_kernels = []\n",
    "\n",
    "# Extract GPU kernel events\n",
    "for event in trace[\"traceEvents\"]:\n",
    "    if event.get(\"ph\") != \"X\":\n",
    "        continue\n",
    "\n",
    "    cat = event.get(\"cat\", \"\").lower()\n",
    "    name = event.get(\"name\", \"\")\n",
    "    start_time = event[\"ts\"]\n",
    "    duration = event.get(\"dur\", 0)\n",
    "\n",
    "    if \"cuda\" in cat or \"kernel\" in cat:\n",
    "        gpu_kernels.append({\n",
    "            \"name\": name,\n",
    "            \"start\": start_time,\n",
    "            \"duration\": duration\n",
    "        })\n",
    "\n",
    "# Print all GPU kernels with their durations\n",
    "print(f\"{'GPU Kernel':<60} {'Start (us)':<15} {'Duration (us)':<15}\")\n",
    "print(\"-\" * 90)\n",
    "for k in sorted(gpu_kernels, key=lambda x: x[\"start\"]):\n",
    "    print(f\"{k['name']:<60} {k['start']:<15} {k['duration']:<15}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f0de0",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "In this tutorial, we walked through the **end-to-end process of profiling LLaMA 4 inference on AMD GPUs using vLLM**, starting from:\n",
    "\n",
    "- Container setup with ROCm and vLLM\n",
    "- Enabling and capturing detailed performance traces\n",
    "- Visualizing CPU-GPU interactions with [Perfetto](https://ui.perfetto.dev/)\n",
    "- Zooming into kernel-level activity for attention blocks\n",
    "- Programmatically analyzing trace logs using Python"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
