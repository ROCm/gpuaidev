{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f2aa3c",
   "metadata": {},
   "source": [
    "# Profiling Llama-4 inference with vLLM\n",
    "\n",
    "\n",
    "Profiling is essential for understanding the performance bottlenecks in large language model inference pipelines. This tutorial walks you through the process of profiling the **Llama-4 Scout-17B-16E-Instruct** model using the vLLM framework on AMD GPUs with ROCm. You'll capture detailed kernel traces and later visualize them using Perfetto.\n",
    "\n",
    "## Prerequisites\n",
    "Before starting this tutorial, ensure you have the following:\n",
    "\n",
    "- Access to the gated **Llama-4 Scout-17B-16E-Instruct** model\n",
    "- Access to **Perfetto UI** \n",
    "\n",
    "### Hardware\n",
    "\n",
    "* **AMD GPUs**: Ensure you are using an AMD GPU, such as the Instinct™ MI300X or Radeon Pro W7900, with ROCm support and that your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "### Software\n",
    "\n",
    "* **ROCm 6.3, 6.4**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html). Verify your ROCm installation by running this command:\n",
    "\n",
    "   ``` bash\n",
    "   amd-smi\n",
    "   ```\n",
    "   \n",
    "   This command lists your AMD GPUs with relevant details.\n",
    "   \n",
    "   **Note**: For ROCm 6.4 and earlier, use the `rocm-smi` command instead.\n",
    "\n",
    "* **Docker**: Ensure Docker is installed and configured correctly. See the [Docker installation guide](https://docs.docker.com/get-docker/) for more information.\n",
    "\n",
    "## Prepare the training environment\n",
    "\n",
    "Follow these steps to configure your tutorial environment:\n",
    "\n",
    "### 1. Pull the Docker image\n",
    "\n",
    "Ensure your system meets the [system requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "Pull the Docker image required for this tutorial:\n",
    "\n",
    "``` bash\n",
    "docker pull rocm/vllm-dev:main\n",
    "```\n",
    "\n",
    "### 2. Launch the Docker container\n",
    "\n",
    "Launch the Docker container in a terminal on your server and map the necessary directories.\n",
    "\n",
    "``` bash\n",
    "docker run -it --rm \\\n",
    "  --network=host \\\n",
    "  --device=/dev/kfd \\\n",
    "  --device=/dev/dri \\\n",
    "  --group-add=video \\\n",
    "  --ipc=host \\\n",
    "  --cap-add=SYS_PTRACE \\\n",
    "  --security-opt seccomp=unconfined \\\n",
    "  --shm-size 8G \\\n",
    "  -v $(pwd):/workspace \\\n",
    "  -w /workspace/notebooks \\\n",
    "  rocm/vllm:latest\n",
    "```\n",
    "\n",
    "**Note**: This command mounts the current directory to the `/workspace` directory in the container. Ensure the notebook file is either copied to this directory before running the Docker command or uploaded into the Jupyter Notebook environment after it starts. Save the token or URL provided in the terminal output to access the notebook from your web browser. You can download this notebook from the [AI Developer Hub GitHub repository](https://github.com/ROCm/gpuaidev).\n",
    "\n",
    "### 3. Install and launch Jupyter\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "``` bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Start the Jupyter server:\n",
    "\n",
    "``` bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3210e0f",
   "metadata": {},
   "source": [
    "## Step-by-step process\n",
    "\n",
    "Follow these steps to profile the Llama-4 model and capture the kernel traces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbc92f6",
   "metadata": {},
   "source": [
    "### Step 1: Logging in to Hugging Face\n",
    "Provide your Hugging Face token\n",
    "\n",
    "You'll require a Hugging Face API token to access Llama-4. Generate your token at [Hugging Face Tokens](https://huggingface.co/settings/tokens) and request access for [Llama-4-Scout 17B-16E-Instruct](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct). Tokens typically start with \"hf_\". \n",
    "\n",
    "Run the following interactive block in your Jupyter notebook to set up the token:\n",
    "\n",
    "**Note**: Uncheck the \"Add token as Git credential\" option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc811a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "# Prompt the user to log in\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1816dd",
   "metadata": {},
   "source": [
    "Verify that your token was accepted correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1abfde8",
   "metadata": {},
   "source": [
    "### Step 2: Start the vLLM server with a profiler configuration\n",
    "Open a new terminal tab inside your JupyterLab session.\n",
    "In this new terminal, run the following commands. Keep the terminal open.\n",
    "\n",
    "``` bash\n",
    "mkdir -p /profile\n",
    "export VLLM_TORCH_PROFILER_DIR=/profile\n",
    "\n",
    "# Start the vLLM server with standard configs\n",
    "RCCL_MSCCL_ENABLE=0 \\\n",
    "VLLM_USE_V1=1 \\\n",
    "VLLM_WORKER_MULTIPROC_METHOD=spawn \\\n",
    "VLLM_USE_MODELSCOPE=False \\\n",
    "VLLM_USE_TRITON_FLASH_ATTN=0 \\\n",
    "vllm serve meta-llama/Llama-4-Scout-17B-16E-Instruct \\\n",
    "    --disable-log-requests \\\n",
    "    -tp 8 \\\n",
    "    --max-num-seqs 64 \\\n",
    "    --no-enable-prefix-caching \\\n",
    "    --max_num_batched_tokens=320000 \\\n",
    "    --max-model-len 32000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83de3f24",
   "metadata": {},
   "source": [
    "### Step 3: Run the benchmark and capture the trace\n",
    "\n",
    "With the server running, trigger a synthetic benchmark request to generate traffic and collect the profiling data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf84bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!vllm bench serve \\\n",
    "    --model meta-llama/Llama-4-Scout-17B-16E-Instruct \\\n",
    "    --dataset-name random \\\n",
    "    --random-input-len 2000 \\\n",
    "    --random-output-len 10 \\\n",
    "    --max-concurrency 64 \\\n",
    "    --num-prompts 64 \\\n",
    "    --ignore-eos \\\n",
    "    --percentile_metrics ttft,tpot,itl,e2el \\\n",
    "    --profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721833ce",
   "metadata": {},
   "source": [
    "Ensure you add the `--profile` flag, which starts the profiler before the benchmark starts and then stops it afterwards and generates the trace.\n",
    "\n",
    "**NOTE:** After the execution of the cell above completes, stop the vLLM server you started in the other terminal by pressing **Ctrl-C**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6f4cac",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 4 : Visualize the trace using Perfetto UI\n",
    "\n",
    "After the trace is generated, unzip it. The trace is saved in the JSON format. To visualize it, use [Perfetto UI](https://ui.perfetto.dev/), a powerful trace viewer built for large-scale profiling data. This helps uncover latency bottlenecks, CPU–GPU overlap, and kernel-level inefficiencies in your inference pipeline. To visualize the trace, follow these steps:\n",
    "\n",
    "1. Go to [https://ui.perfetto.dev](https://ui.perfetto.dev/).\n",
    "2. Click **\"Open trace file\"**.\n",
    "3. Upload the `.json` file.\n",
    "\n",
    "After you open the trace, it will look somewhat like this:\n",
    "\n",
    "![image_0](../assets/image_0.png)\n",
    "\n",
    "\n",
    "#### Understanding the prefill and decode timelines\n",
    "\n",
    "Now zoom into the trace to interpret what each slice reveals about the Llama-4 execution stages. Here's a close-up of two important process timelines captured in Perfetto:\n",
    "\n",
    "**Note:** The focus of this section is on the first two tracks: `python3 3906` and `python3 2`.\n",
    "\n",
    "\n",
    "-  `python3 3906`: Asynchronous CPU calls\n",
    "\n",
    "   The **orange slice** under `python3 3906` shows the execution of high-level model code on the CPU. It includes:\n",
    "\n",
    "   - Calls like `execute_model`, `forward`, and `hipMemcpyWithStream`\n",
    "   - PyTorch internals such as `aten::to` and `aten::copy_`\n",
    "   - The actual forward pass for `Llama4ForCausalLM` and memory transfers\n",
    "\n",
    "   This slice reflects the CPU-side orchestration of inference, from input preparation to dispatching kernels to the GPU.\n",
    "\n",
    "\n",
    "-  `python3 2`: GPU kernel timeline\n",
    "\n",
    "   The **pink slice** under `python3 2` is where GPU kernel execution is visualized. This slice represents actual compute work being done by the GPU after the CPU enqueues the tasks.\n",
    "\n",
    "   Here’s the key insight:\n",
    "\n",
    "   -  There is a clear gap between two bursts of kernel execution.\n",
    "   -  This gap separates two distinct phases:\n",
    "      - **Before the gap**: This is the \"prefill\" stage, where the initial prompt is encoded, and attention and cache states are populated.\n",
    "      - **After the gap**: This is the \"decode\" stage, where the model generates tokens, typically using cached key/value tensors.\n",
    "\n",
    "#### Understanding kernel timelines\n",
    "\n",
    "When you expand the `python3 2` timeline in Perfetto, you’ll see two distinct GPU streams representing different types of operations executed on the device:\n",
    "\n",
    "![image](../assets/image_1.png)\n",
    "\n",
    "\n",
    "**Note:** This zoomed-in view helps you distinguish between computation kernels and communication operations.\n",
    "\n",
    "-  Stream `3 3`: All GPU kernels\n",
    "\n",
    "   Stream `3 3` is the **primary compute stream**, where all GPU kernel executions are scheduled. This includes:\n",
    "   - `MatMul` and GEMM operations\n",
    "   - Fused MLP and attention blocks\n",
    "   - Positional encodings and LayerNorms\n",
    "   - Any fused or element-wise kernels\n",
    "\n",
    "   This stream is densely packed, showing the bulk of the model inference activities The rhythm and spacing of these kernels help diagnose things like:\n",
    "\n",
    "   - Load balancing across tensor parallel ranks\n",
    "   - Gaps between kernel launches\n",
    "   - Prefill versus decode phases (based on the density before and after the gaps)\n",
    "\n",
    "\n",
    "-  Stream `3 8`: AllGather kernels\n",
    "\n",
    "   Stream `3 8` is used specifically for **AllGather** operations, which are part of the tensor parallel communication process. These kernels:\n",
    "\n",
    "   - Synchronize activations across devices in multi-GPU setups\n",
    "   - Typically occur between layer boundaries\n",
    "   - Are crucial in `tp=8` setups for syncing partial outputs across eight shards\n",
    "\n",
    "### Step 5: Zoom in to analyze the attention forward kernel\n",
    "\n",
    "Attention is at the heart of all Transformer-based language models and Llama-4 is no exception. Profiling its **attention forward kernel** gives us critical insights into its computational efficiency. Dive into the trace to inspect it at the kernel level.\n",
    "\n",
    "#### Zoom into the kernel timeline\n",
    "\n",
    "- Navigate to `python3 2` and then to `stream 3 3` in the Perfetto UI.\n",
    "- Scroll with the mouse or drag to zoom into a cluster of dense kernels.\n",
    "- Hover over one of the prominent kernels. You should see a label like `_fwd_kernel`.\n",
    "\n",
    "Here’s an example image from the trace:\n",
    "\n",
    "![image](../assets/image_2.png)\n",
    "\n",
    "#### Understanding the kernel slice\n",
    "\n",
    "From the **details** panel below the trace view, you can extract the following:\n",
    "\n",
    "| Field | Value |\n",
    "| --- | --- |\n",
    "| **Name** | `_fwd_kernel` |\n",
    "| **Category** | `kernel` |\n",
    "| **Stream** | `stream 3 (3)` |\n",
    "| **Process** | `python3 (2)` |\n",
    "| **Duration** | `788µs 256ns` |\n",
    "| **Launch delay** | `~819µs` from the CPU call |\n",
    "\n",
    "This kernel is part of the **multi-head attention forward pass**, one of the most compute-heavy operations in inference.\n",
    "\n",
    "\n",
    "#### Tracing back to the CPU\n",
    "\n",
    "Inside the kernel details, near the preceding flows, you’ll find `hipModuleLaunchKernel`.\n",
    "\n",
    "Click this to jump back to the CPU thread (`python3 3906`), shown in blue in the image below, which issued this kernel launch. This feature is incredibly useful for:\n",
    "\n",
    "- Mapping GPU operations to their Python or C++ call stack\n",
    "- Identifying bottlenecks in dispatch or synchronization\n",
    "- Understanding how long the CPU takes to enqueue work on the GPU\n",
    "\n",
    "![image](../assets/image_3.png)\n",
    "\n",
    "\n",
    "Following the process above, you can track how much time each kernel takes in both the prefill and decode stage. A short summary for one kernel is shown in the chart below.\n",
    "\n",
    "![image](../assets/image_4.png)\n",
    "\n",
    "### Step 6: Programmatic analysis and extracting the GPU kernel timeline with Python\n",
    "\n",
    "To go beyond visual inspection, you can also parse the trace programmatically to list all GPU kernels and their durations. This is helpful when you're tracking:\n",
    "\n",
    "- Kernel launch patterns\n",
    "- Duration spikes\n",
    "- Gaps or anomalies in execution\n",
    "\n",
    "Here’s a minimal Python script that reads the `trace.json` file and lists all GPU kernels sorted by start time:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea17db2",
   "metadata": {},
   "source": [
    "\n",
    "Decompress the `.gz` trace file into `trace.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip -c /profile/$(ls /profile | head -n 1) > trace.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a01655b",
   "metadata": {},
   "source": [
    "Run the cell below, which lists all GPU kernels sorted by start time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Load the trace\n",
    "with open(\"trace.json\", \"r\") as f:\n",
    "    trace = json.load(f)\n",
    "\n",
    "gpu_kernels = []\n",
    "\n",
    "# Extract GPU kernel events\n",
    "for event in trace[\"traceEvents\"]:\n",
    "    if event.get(\"ph\") != \"X\":\n",
    "        continue\n",
    "\n",
    "    cat = event.get(\"cat\", \"\").lower()\n",
    "    name = event.get(\"name\", \"\")\n",
    "    start_time = event[\"ts\"]\n",
    "    duration = event.get(\"dur\", 0)\n",
    "\n",
    "    if \"cuda\" in cat or \"kernel\" in cat:\n",
    "        gpu_kernels.append({\n",
    "            \"name\": name,\n",
    "            \"start\": start_time,\n",
    "            \"duration\": duration\n",
    "        })\n",
    "\n",
    "# Print all GPU kernels with their durations\n",
    "print(f\"{'GPU Kernel':<60} {'Start (us)':<15} {'Duration (us)':<15}\")\n",
    "print(\"-\" * 90)\n",
    "for k in sorted(gpu_kernels, key=lambda x: x[\"start\"]):\n",
    "    print(f\"{k['name']:<60} {k['start']:<15} {k['duration']:<15}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f0de0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you walked through the end-to-end process of profiling Llama-4 inference on AMD GPUs using vLLM, including:\n",
    "\n",
    "- Setting up the container with ROCm and vLLM\n",
    "- Enabling and capturing detailed performance traces\n",
    "- Visualizing CPU-GPU interactions with [Perfetto](https://ui.perfetto.dev/)\n",
    "- Zooming into kernel-level activity for attention blocks\n",
    "- Programmatically analyzing trace logs using Python"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
