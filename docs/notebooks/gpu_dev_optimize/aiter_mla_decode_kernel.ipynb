{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ec6d76a",
   "metadata": {},
   "source": [
    "# MLA decoding kernel of the AITER library to accelerate LLM inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49729a27-1514-4eac-bb1b-c29edc61650f",
   "metadata": {},
   "source": [
    "Imagine you’re deploying a large language model such as  DeepSeek-V3/R1 on AMD Instinct™ GPUs, when suddenly the Multi Latent Attention (MLA) in the decoding phase becomes a performance bottleneck. Token generation feels sluggish, and latency keeps accumulating, degrading the user experience. This is where the AMD AITER library comes to the rescue, dramatically accelerating the MLA decode attention kernel to breathe new life into your model.\n",
    "\n",
    "AITER is a high-performance operator library from AMD, optimized for AI workloads on AMD Instinct GPUs. It's indispensable when:\n",
    "\n",
    "- Operator performance falls far short of the theoretical potential.\n",
    "\n",
    "- Specific operators become inference bottlenecks.\n",
    "\n",
    "- You need architecture-specific optimizations for AMD Instinct GPUs.\n",
    "\n",
    "This tutorial guides you step-by-step through integrating the AITER MLA decode attention kernel to supercharge LLM inference with AMD Instinct GPUs. This will greatly accelerate kernel performance, with different context lengths, compared to native PyTorch implementations. You'll start by setting up the MLA decode attention kernel.\n",
    "\n",
    "**Tip**: Kernels in the AITER library are already integrated into popular LLM inference frameworks such as vLLM and SGLang. This means you can also achieve significant performance gains from the AITER library on AMD Instinct GPUs through these frameworks!\n",
    "\n",
    "## Prerequisites: Setting up the acceleration environment\n",
    "\n",
    "This tutorial was developed and tested using the following setup, which is recommended to reproduce the same model acceleration with AMD Instinct GPUs.\n",
    "\n",
    "### Operating System\n",
    "\n",
    "* **Ubuntu 22.04**: Ensure your system is running Ubuntu version 22.04.\n",
    "\n",
    "### Hardware\n",
    "\n",
    "* **AMD Instinct GPUs**: Ensure you are using an AMD Instinct GPU with ROCm™ software support and that your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "### Software\n",
    "\n",
    "* **ROCm 6.3.1**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html).\n",
    "\n",
    "  After installation, confirm your setup using the `rocm-smi` command.\n",
    "\n",
    "  ``` bash\n",
    "  rocm-smi\n",
    "  ```\n",
    "\n",
    "  This command lists the available AMD GPUs and their status:\n",
    "![rocm-smi-output](../assets/aiter-rocm-smi.png)\n",
    "\n",
    "* **Docker**: For containerized deployment, ensure Docker is installed and configured correctly. Follow the Docker installation guide for your operating system.\n",
    "\n",
    "  **Note**: Ensure the Docker permissions are correctly configured. To configure permissions to allow non-root access, run the following commands:\n",
    "\n",
    "  ``` bash\n",
    "  sudo usermod -aG docker $USER\n",
    "  newgrp docker\n",
    "  ```\n",
    "\n",
    "  Verify Docker is working correctly:\n",
    "\n",
    "  ``` bash\n",
    "  docker run hello-world\n",
    "  ```\n",
    "\n",
    "## Quick start development environment set up\n",
    "\n",
    "This tutorial uses the prebuilt ROCm PyTorch image.\n",
    "\n",
    "### Step 1: Launch the ROCm PyTorch Docker container\n",
    "\n",
    "Launch the Docker container. This image is a turnkey solution with pre-configured dependencies:\n",
    "\n",
    "``` bash\n",
    "docker run -it --rm \\\n",
    "  --network=host \\\n",
    "  --device=/dev/kfd \\\n",
    "  --device=/dev/dri \\\n",
    "  --group-add=video \\\n",
    "  --ipc=host \\\n",
    "  --cap-add=SYS_PTRACE \\\n",
    "  --security-opt seccomp=unconfined \\\n",
    "  --shm-size 8G \\\n",
    "  -v $(pwd):/workspace \\\n",
    "  -w /workspace \\\n",
    "  rocm/pytorch:latest\n",
    "```\n",
    "\n",
    "**Note**: This command mounts the current directory to the `/workspace` directory in the container for easy file access. It lets you perform all work in this Docker container, including manually installing AITER, and get started with the following hands-on, practical examples.\n",
    "\n",
    "### Step 2: Launch Jupyter Notebooks in the container\n",
    "\n",
    "Inside the Docker container, install JupyterLab using the following command:\n",
    "\n",
    "``` bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Start the Jupyter server:\n",
    "\n",
    "``` bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: If port `8888` is occupied, specify a different port, such as `--port=8890`. The rest of this tutorial can run as interactive blocks in your Jupyter notebook after you upload this tutorial to your server.\n",
    "\n",
    "### Step 3: Manually install the AITER library\n",
    "\n",
    "AITER is a rapidly expanding library with many powerful features. To install AITER, use these commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c0604",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone --recursive https://github.com/ROCm/aiter.git\n",
    "cd aiter\n",
    "python3 setup.py develop\n",
    "export PYTHONPATH=$PYTHONPATH:/workspace/aiter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5522313",
   "metadata": {},
   "source": [
    "**Note**: If you're running Jupyter and AITER in your environment, set `PYTHONPATH` accordingly.\n",
    "\n",
    "## Understanding the MLA decode attention kernel\n",
    "\n",
    "You can find the MLA decoding attention kernel definition in the [AITER source code](https://github.com/ROCm/aiter/blob/main/aiter/mla.py#L114C1-L126C3). It requires a minimum of eight input parameters and can accept three additional optional inputs. Here's the function definition for `mla_decode_fwd`, including the parameters:\n",
    "\n",
    "```\n",
    "def mla_decode_fwd(  \n",
    "    q,                   # [batch_size, num_heads, kv_lora_rank + qk_rope_dim]  \n",
    "    kv_buffer,           # [num_pages, page_size, num_heads_kv, qk_head_dim]  \n",
    "    o,                   # Output buffer [batch_size, num_heads, kv_lora_rank]  \n",
    "    qo_indptr,           # Query sequence pointer [batch_size + 1]  \n",
    "    kv_indptr,           # KV sequence pointer [batch_size + 1]  \n",
    "    kv_indices,          # KV indices [kv_indptr[-1]]  \n",
    "    kv_last_page_lens,   # Last page sizes [batch_size]  \n",
    "    max_seqlen_q,        # Maximum query sequence length  \n",
    "    sm_scale=None,       # Scaling factor (default: 1.0/sqrt(qk_head_dim))  \n",
    "    logit_cap=0.0,       # (Under development)  \n",
    "    num_kv_splits=None,  # KV splits (auto-determined)  \n",
    "): \n",
    "```\n",
    "\n",
    "Each parameter has specific shape requirements, so proper configuration is key to optimal performance:\n",
    "\n",
    "* **q** (`torch.tensor` type): This is the query tensor with shape requirements like `[batch_size, num_heads, kv_lora_rank + qk_rope_dim]`.\n",
    "* **kv buffer** (`torch.tensor` type): This is the total kv cache tensor with shape requirements like `[num_pages, page_size, num_heads_kv, qk_head_dim]`, where `num_heads_kv` is always `1` in the decode phase, and `num_pages` and `page_size` jointly represent the pageable kv cache. When `page_size = 1`, the kv cache is set to the original representation, which wastes a lot of GPU memory.\n",
    "* **o** (`torch.tensor` type): This is the output buffer. The `mla_decode_fwd` function will place the result into `o`, which has shape requirements like `[batch_size, num_heads, kv_lora_rank]`.\n",
    "* **qo_indptr** (`torch.tensor` type): This is a pointer to the start address of each query and output sequence, with shape requirements like `[batch_size + 1]`. When the sequence length of each sequence in a batch is different, the `qo_indptr` is used to record this information, which helps handle each sequence correctly.\n",
    "* **kv_indptr** (`torch.tensor` type): This is a pointer to the start address of each context/kv sequence, with shape requirements like `[batch_size + 1]`. Each query sequence is different within a batch, and the sequence of answers is also different, so the context/kv sequence lengths are also different. The `kv_indptr` variable records this information to help handle each context/kv of the query sequence correctly.\n",
    "* **kv_indices** (`torch.tensor` type): This contains the concrete kv start indices of each sequence. It has shape requirements like `[kv_indptr[-1]]`.\n",
    "* **kv_last_page_lens** (`torch.tensor` type): This is the last page size of each sequence, with shape requirements like `[batch_size]`.\n",
    "* **max_seqlen_q:** (`torch.tensor` type): This is the max sequence length across all the queries in this batch.\n",
    "* **sm_scale** (`scalar` type): This is equal to `1.0 / (qk_head_dim**0.5)`, which represents the denominator in the scale dot product attention formula.\n",
    "* **logit_cap**: This is a work in progress and can be ignored. For more information, see the following [annotation](https://github.com/ROCm/aiter/blob/main/aiter/mla.py#L128).\n",
    "* **num_kv_splits** (`scalar` type): This parameter can be ignored. It represents how many GPU work groups or blocks to allocate to handle kv, but the code will determine this value using a heuristic algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7834a07-5b45-4fe8-b426-3e8b974a9050",
   "metadata": {},
   "source": [
    "## Running a practical example\n",
    "\n",
    "It's time to get started with a step-by-step walkthrough that will have the MLA decoding attention running at lightning speed on your Instinct MI300X.\n",
    "\n",
    "### Setting the environment\n",
    "\n",
    "First prepare the AMD MI300X GPU, with the CPU standing by as backup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009428c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Change working directory to the repo\n",
    "os.chdir(\"./aiter\")  # relative path from the notebook location\n",
    "\n",
    "# Add current directory (aiter repo root) to Python path\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "import torch\n",
    "from aiter.mla import mla_decode_fwd \n",
    "\n",
    "# Let's get our hardware ready for the show!\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"All systems go! Running on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac10d4-c12e-4be7-bbeb-eebb5554c14d",
   "metadata": {},
   "source": [
    "### Prepare the tensors\n",
    "\n",
    "Now prepare your tensors for this run through. You'll configure the following:\n",
    "\n",
    "- A batch of 128 sequences, using `batch_size = 128`\n",
    "\n",
    "- A 4096-token KV cache (the memory of our model), using `kv_cache_seqlen = 4096`\n",
    "\n",
    "- Single-query decoding, using `q_seqlen = 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c259800-c421-4c60-afa1-1f781958dfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your performance parameters\n",
    "batch_size = 128       # How many sequences we're processing\n",
    "kv_cache_seqlen = 4096 # How far back our model can remember\n",
    "q_seqlen = 1           # Decoding one token at a time\n",
    "\n",
    "# Initialize our pointer arrays\n",
    "qo_indptr = torch.zeros(batch_size + 1, dtype=torch.int, device=device)\n",
    "kv_indptr = torch.zeros(batch_size + 1, dtype=torch.int, device=device)\n",
    "\n",
    "# Fill with sequence lengths (simple case: all equal)\n",
    "seq_lens_qo = torch.full((batch_size,), q_seqlen, dtype=torch.int, device=device)\n",
    "seq_lens_kv = torch.full((batch_size,), kv_cache_seqlen, dtype=torch.int, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9234d58b-31d6-4e85-bc92-e340fe11f8e2",
   "metadata": {},
   "source": [
    "The sample code above first declares two buffers for `qo_indptr` and `kv_indptr` and then fills `seq_lens_qo` and `seq_lens_kv` with `q_seqlen = 1` and `kv_cache_seqlen = 4096`. For simplicity, it assumes each sequence has the same `q_seqlen `and `kv cache seqlen`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cbc4a5-ba1d-4a76-84d4-f3cffb33bca8",
   "metadata": {},
   "source": [
    "It then fills `kv_indptr` and `qo_indptr` by passing the `cumsum` function the sequence lengths of qkv, then calculating the actual length of each sequence by subtracting the latter value from the former. This is the \"secret sauce\" of efficient attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c618093-ac43-4c67-af30-cb52d4951467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative lengths - this tells us where each sequence starts\n",
    "kv_indptr[1:] = torch.cumsum(seq_lens_kv, dim=0)  # KV memory layout\n",
    "qo_indptr[1:] = torch.cumsum(seq_lens_qo, dim=0) # Query/output layout\n",
    "\n",
    "# For example: kv_indptr = [0,5,11,18] means:\n",
    "# Sequence 0: positions 0-4 (length 5)\n",
    "# Sequence 1: positions 5-10 (length 6)\n",
    "# Sequence 2: positions 11-17 (length 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07319141-c3ed-43ca-812c-1521c14f6cd6",
   "metadata": {},
   "source": [
    "Now prepare your key-value cache. Think of this as the working memory for the model.\n",
    "\n",
    "- Initialize the concrete kv start indices of each sequence and the kv last page lens (size) of each sequence.\n",
    "- For simplicity, define `page_size = 1`, so the kv last page lens for each sequence is `1`.\n",
    "- For this example, set the maximum value for `kv_indices` to `2097152`. This is calculated from `batch_size * 16384`, which is equal to `128 * 16384`. This means for a `batch_size` of `128`, you can generate up to `16384` tokens for each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9332e679-8e24-42ac-9785-a57306cfb7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_indices = torch.randint(0, 2097152, (kv_indptr[-1].item(),), dtype=torch.int, device=device)\n",
    "kv_last_page_lens = torch.ones(batch_size, dtype=torch.int, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b97ca0-4020-4585-ba8c-29a91f103a42",
   "metadata": {},
   "source": [
    "Now it's time to introduce the main inputs, which are the query tensor and KV cache, and the output buffer. These are `q`, `kv buffer`, and `o`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c26b4-ab6d-4620-a6db-0738e14333aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 128        # Number of attention heads\n",
    "q_head_dim = 128       # Dimension per head\n",
    "kv_lora_rank = 512     # LoRA rank for KV\n",
    "qk_rope_head_dim = 64  # Rotary embedding dimension\n",
    "\n",
    "# The query tensor - what we're asking our model\n",
    "q = torch.randn(\n",
    "    (batch_size * q_seqlen, num_heads, kv_lora_rank + qk_rope_head_dim),\n",
    "    dtype=torch.bfloat16, device=device\n",
    ")\n",
    "num_heads_kv = 1\n",
    "page_size = 1\n",
    "q_head_dim = 128\n",
    "# Our KV cache - the model's knowledge bank\n",
    "kv_buffer = torch.randn(\n",
    "    (2097152, page_size, num_heads_kv, kv_lora_rank + qk_rope_head_dim), \n",
    "    dtype=torch.bfloat16, device=device\n",
    ")\n",
    "\n",
    "# The output buffer - where the magic will happen\n",
    "o = torch.empty(\n",
    "    (batch_size * q_seqlen, num_heads, kv_lora_rank), \n",
    "    dtype=torch.bfloat16, device=device\n",
    ").fill_(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dec57f2-f872-4e2f-8389-e51936d5e703",
   "metadata": {},
   "source": [
    "**Note**: You don't have to define these buffers. However, ensure you define the shape size to match the values seen here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f57036b-2416-49e4-b52f-1f3436b5381c",
   "metadata": {},
   "source": [
    "### Launching the kernel\n",
    "\n",
    "With everything set, launch your optimized MLA decode attention kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c2c7a3-cd27-4f6b-8689-3324ebbc2b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "mla_decode_fwd(\n",
    "        q,\n",
    "        kv_buffer,\n",
    "        o,\n",
    "        qo_indptr,\n",
    "        kv_indptr,\n",
    "        kv_indices,\n",
    "        kv_last_page_lens,\n",
    "        1,\n",
    "        sm_scale= 1.0 / (q_head_dim**0.5)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c823fa-0711-4238-80de-cb2d6b5ab3f0",
   "metadata": {},
   "source": [
    "Now see what results you got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36b54e6-8435-4649-a81f-af582c01d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c141e6-369f-463b-afb3-c0792f5d4ff9",
   "metadata": {},
   "source": [
    "The final shape is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ce9314-991f-4415-8e07-045a7394c175",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e53e416-840f-459c-b2df-c50eafbb0ae1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "With the attention computation now optimized, the results are ready to flow seamlessly into the next layer of your model, keeping your entire inference pipeline running at maximum velocity.\n",
    "\n",
    "Rigorous benchmarking shows the real ability of the kernel:\n",
    "\n",
    "**Benchmark Highlights**:\n",
    "\n",
    "- Evaluated multiple context lengths (512-4096 tokens)\n",
    "- Tested with fixed batch sizes (128)\n",
    "- Compared different MLA algorithm implementations\n",
    "\n",
    "**Result**:\n",
    "\n",
    "- A consistent speedup over native PyTorch implementations.\n",
    "\n",
    "![performance-comparison-with-aiter](../assets/aiter-performance-comparison.png)\n",
    "\n",
    "Imagine what these gains could mean for your application:\n",
    "- Reduced latency for real-time applications\n",
    "- Increased throughput for batch processing\n",
    "- Lower compute costs across the board\n",
    "\n",
    "Ready to take the next step? Dive deeper into the AITER capabilities with the following resources:\n",
    "\n",
    "- Explore the [AITER GitHub repository](https://github.com/ROCm/aiter). \n",
    "- Check out additional optimization examples.\n",
    "- Star the repository to stay updated on new features.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
