{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ed5955",
   "metadata": {},
   "source": [
    "# Helion DSL for GPU kernel development and assessment on AMD GPUs\n",
    "\n",
    "[Helion](https://github.com/pytorch/helion) is a Python-embedded domain-specific language (DSL) from Meta for authoring machine learning kernels. It compiles down to [Triton](https://openai.com/index/triton/), a performant backend from OpenAI for programming GPUs and other devices. Helion aims to raise the level of abstraction compared to Triton, making it easier to write correct and efficient kernels, while enabling greater automation in the autotuning process.\n",
    "\n",
    "Helion can be viewed as either PyTorch with tiles or as a higher-level Triton application. Compared to Triton, Helion reduces manual coding effort through autotuning. Helion spends more time (approximately 10 minutes) autotuning as it evaluates hundreds of potential Triton implementations generated from a single Helion kernel. This larger search space also makes kernels more performance portable between different hardware.\n",
    "\n",
    "Helion is supported by AMD GPUs. This tutorial demonstrates how to set up the Helion development environment, implement a Helion kernel, and benchmark performance with Triton and Torch on AMD Instinctâ„¢ GPUs.\n",
    "\n",
    "## The Helion autotuner\n",
    "\n",
    "The key differentiator of Helion is its automated, ahead-of-time (AOT) autotuning engine. In Triton, developers are responsible for manually defining the search space for optimizations. This requires explicitly enumerating every configuration to be tested, a tedious process that limits the scope of exploration.\n",
    "\n",
    "Helion changes this dynamic by using implicit search spaces. The high-level language automatically constructs a vast, multi-dimensional search space over the implementation choices. For example, a single `hl.tile` call implicitly instructs the autotuner to explore different block sizes and loop orderings and consider whether to flatten the iteration space into a single dimension. One Helion kernel definition can therefore map to thousands of Triton configurations, allowing the autotuner to create a much larger and richer search space in which to discover a superior configuration.\n",
    "\n",
    "## Tutorial workflow\n",
    "\n",
    "This tutorial includes the following:\n",
    "\n",
    "1. [Environment setup with Docker and ROCm](#env-setup)\n",
    "2. [Helion GPU kernel example](#helion-example)\n",
    "3. [Background information about the softmax algorithm](#softmax-details)\n",
    "4. [Creating Helion softmax GPU Kernels](#helion-kernel)\n",
    "5. [Performance benchmarking and visualization of Helion, Triton, and Torch on AMD GPUs](#performance-benchmark)\n",
    "\n",
    "  ![Helion flowchart](../assets/helion-flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c7e85e",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup.\n",
    "\n",
    "### Operating system\n",
    "\n",
    "* **Ubuntu 22.04/24.04**: Ensure your system is running Ubuntu 22.04 or 24.04.\n",
    "\n",
    "### Hardware\n",
    "\n",
    "* **AMD Instinct MI300X GPU**: This tutorial was tested on an AMD Instinct MI300X GPU. Ensure you are using an AMD Instinct GPU with ROCm support and that your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "### Software\n",
    "\n",
    "* **ROCm 7.0**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html). After installation, confirm your setup using:\n",
    "\n",
    "    ``` bash\n",
    "    amd-smi\n",
    "    ```\n",
    "    This command lists your AMD GPUs with relevant details.\n",
    "\n",
    "    **Note**: For ROCm 6.4 and earlier, use the `rocm-smi` command instead.\n",
    "\n",
    "    AMD also provides prebuilt ROCm Docker images, including a [ROCm PyTorch image](https://hub.docker.com/r/rocm/pytorch), a [ROCm Ubuntu 22.04 image](https://hub.docker.com/r/rocm/dev-ubuntu-22.04), and a [ROCm Ubuntu 24.04 image](https://hub.docker.com/r/rocm/dev-ubuntu-24.04). You can use these prebuilt Docker images to reduce the effort required to set up a ROCm environment.\n",
    "\n",
    "* **Docker**: Ensure Docker is installed and configured correctly. Follow the Docker installation guide for your operating system.\n",
    "\n",
    "   **Note**: Ensure the Docker permissions are correctly configured. To configure permissions to allow non-root access, run the following commands:\n",
    "\n",
    "   ``` bash\n",
    "   sudo usermod -aG docker $USER\n",
    "   newgrp docker\n",
    "   ```\n",
    "\n",
    "   Verify Docker is working correctly with:\n",
    "\n",
    "   ``` bash\n",
    "   docker run hello-world\n",
    "   ```\n",
    "\n",
    "### Hugging Face API access\n",
    "\n",
    "* Obtain an API token from [Hugging Face](https://huggingface.co) for downloading models.\n",
    "* Ensure the Hugging Face API token has the necessary permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d736a0",
   "metadata": {},
   "source": [
    "<a id=\"env-setup\"></a>\n",
    "\n",
    "## 1. Environment setup with Docker and ROCm\n",
    "\n",
    "Follow these steps to set up the environment, launch Jupyter Notebooks, and install the dependencies.\n",
    "\n",
    "### Launch the Docker container\n",
    "\n",
    "Launch the Docker container. From your host machine, run this command:\n",
    "\n",
    "``` bash\n",
    "docker run -it --rm \\\n",
    "  --privileged \\\n",
    "  --network=host \\\n",
    "  --device=/dev/kfd \\\n",
    "  --device=/dev/dri \\\n",
    "  --group-add=video \\\n",
    "  --ipc=host \\\n",
    "  --cap-add=SYS_PTRACE \\\n",
    "  --security-opt seccomp=unconfined \\\n",
    "  --shm-size 8G \\\n",
    "  -v $(pwd):/workspace \\\n",
    "  -w /workspace/notebooks \\\n",
    "  rocm/pytorch:latest bash\n",
    "```\n",
    "\n",
    "**Note**: This command mounts the current directory to the `/workspace` directory in the container. Ensure the notebook file is either copied to this directory before running the Docker command or uploaded into the Jupyter Notebook environment after it starts. Save the token or URL provided in the terminal output to access the notebook from your web browser. You can download this notebook from the [AI Developer Hub GitHub repository](https://github.com/ROCm/gpuaidev).\n",
    "\n",
    "<a id=\"launch-jupyter\"></a>\n",
    "\n",
    "### Launch Jupyter Notebooks in the container\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "``` bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Start the Jupyter server:\n",
    "\n",
    "``` bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7cbd29",
   "metadata": {},
   "source": [
    "### Install Helion and Triton\n",
    "\n",
    "It's strongly recommended that you use the latest version of Helion in your project. AMD and other vendors frequently update their optimization passes and algorithms in [Helion](https://github.com/pytorch/helion), which can help improve your Helion kernel performance.\n",
    "\n",
    "####  Uninstall older versions of Helion and Triton\n",
    "\n",
    "First, uninstall any existing versions of Helion and Triton:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fea56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y helion triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c3edf",
   "metadata": {},
   "source": [
    "#### Install Helion and Triton\n",
    "\n",
    "Use the following commands to install Helion, Triton, and other requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install triton==3.5.1\n",
    "pip install helion==0.2.6\n",
    "pip install matplotlib\n",
    "\n",
    "pip list | grep -E 'helion|triton|torch'\n",
    "\n",
    "# Ignore the incompatibility error. It does not affect the execution of the examples in this notebook.\n",
    "# Look for the string 'Successfully installed triton-xxx' to confirm Triton was installed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4048b347-f2ed-4bd1-9b1a-d9aa627065df",
   "metadata": {},
   "source": [
    "<a id=\"helion-example\"></a>\n",
    "\n",
    "## 2. Helion GPU kernel example\n",
    "\n",
    "This example demonstrates how to implement an element-wise exponential (exp) function using Helion. It features both forward and backward passes using Helion's tiling system for parallel computation. The implementation integrates seamlessly with the PyTorch autograd system, enabling high-performance, auto-differentiable operations. The example shows how to verify the implementation against the native PyTorch exponential function with full gradient support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e738ede6-7adb-4f3c-b167-77ee65f87e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import helion\n",
    "from helion._testing import DEVICE\n",
    "from helion._testing import run_example\n",
    "import helion.language as hl\n",
    "\n",
    "@helion.kernel()\n",
    "def exp_fwd(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the exponential of all elements in the input tensor.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor\n",
    "\n",
    "    Returns:\n",
    "        Output tensor with the exponential of each element in the input\n",
    "    \"\"\"\n",
    "    out = torch.empty_like(x)\n",
    "    for tile in hl.tile(x.size()):\n",
    "        out[tile] = torch.exp(x[tile])\n",
    "    return out\n",
    "\n",
    "# %%\n",
    "@helion.kernel()\n",
    "def exp_bwd(dy: torch.Tensor, exp_x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the gradient of the exponential function with respect to the input tensor.\n",
    "\n",
    "    Args:\n",
    "        dy: Gradient of the output tensor\n",
    "        exp_x: Saved activation from the forward pass\n",
    "\n",
    "    Returns:\n",
    "        Gradient of the input tensor\n",
    "    \"\"\"\n",
    "    dx = torch.empty_like(exp_x)\n",
    "    for tile in hl.tile(exp_x.size()):\n",
    "        dx[tile] = dy[tile] * exp_x[tile]\n",
    "    return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7f31bc",
   "metadata": {},
   "source": [
    "The next cell defines the wrapper class of the exp kernel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed99268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class ExpFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx: object,\n",
    "        x: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for exp.\"\"\"\n",
    "        y = exp_fwd(x)\n",
    "        ctx.save_for_backward(y)  # type: ignore[arg-type]\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(  # type: ignore[override]\n",
    "        ctx: object,\n",
    "        grad_output: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Backward pass for exp.\"\"\"\n",
    "        (x,) = ctx.saved_tensors  # type: ignore[attr-defined]\n",
    "        return exp_bwd(grad_output, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5d4cc1",
   "metadata": {},
   "source": [
    "Verify the exp kernel implementation against PyTorch's native `exp` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5955694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "def exp(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Exponential with forward and backward support.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor\n",
    "\n",
    "    Returns:\n",
    "        Output tensor with the exponential of each element in the input\n",
    "    \"\"\"\n",
    "    return ExpFunction.apply(x)  # type: ignore[no-any-return]\n",
    "\n",
    "# %%\n",
    "def check(n: int) -> None:\n",
    "    \"\"\"\n",
    "    Verify the exp kernel implementation against PyTorch's native exp function.\n",
    "\n",
    "    Args:\n",
    "        n: Size of the test tensor\n",
    "    \"\"\"\n",
    "    x = torch.randn(n, device=DEVICE, dtype=torch.float32, requires_grad=True)\n",
    "    run_example(exp, torch.exp, (x,), bwd=True)\n",
    "\n",
    "\n",
    "check(1024 * 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4b5b9",
   "metadata": {},
   "source": [
    "<a id=\"softmax-details\"></a>\n",
    "\n",
    "## 3. Details of the softmax algorithm\n",
    "\n",
    "The softmax function is often used in classification CNN models and even transformer-based LLM models. It converts raw output scores, also known as logits, into probabilities by normalizing the exponential of each value by dividing it by the sum of all the exponentials. This process ensures that the output values are in the range (0,1) and sum up to 1, making them interpretable as probabilities. PyTorch has implemented the softmax function as [a standard API](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html).\n",
    "\n",
    "The definition of the function $y = Softmax(x)$ is:\n",
    "\n",
    "$$\n",
    "y_i = \\frac{e^{x_i}}{\\sum_{j=1}^{V} e^{x_j}} \\tag{1}\n",
    "$$\n",
    "\n",
    "where $x,y \\in \\mathbb{R}^V$.\n",
    "\n",
    "### Naive version: Safe Softmax\n",
    "\n",
    "To achieve numerical stability, subtract the maximum value of the row vector from each input element before taking their exponentials. So the definition changes to:\n",
    "\n",
    "$$\n",
    "y_i = \n",
    "\\frac{e^{\\left(x_i - \\max_{k=1}^V x_k\\right)}}\n",
    "     {\\sum_{j=1}^V e^{\\left(x_j - \\max_{k=1}^V x_k\\right)}} \\tag{2}\n",
    "$$\n",
    "\n",
    "where $x,y \\in \\mathbb{R}^V$. This is known as the Safe Softmax algorithm.\n",
    "\n",
    "According to the softmax algorithm definition, the Triton kernel implements the naive version (Equation 2). The kernel requires two for loops to obtain the maximum data and the corresponding sum of all the exponentials and an additional for loop to calculate the final softmax result. So it uses three loops altogether. The Safe Softmax algorithm is more fully described in [Online normalizer calculation for softmax](https://arxiv.org/pdf/1805.02867).\n",
    "\n",
    "The performance of this kernel on an 8192x8192 tensor is calculated like this:\n",
    "\n",
    "- The block size of the `col` dimension is 256.\n",
    "- One program is allocated per row of the input tensor. This means the grid size is `n_rows`, where `n_rows` equals the number of rows of the input tensor.\n",
    "- The program instance (thread block) scans one row of the tensor and iteratively processes the data blocks of the current row to calculate the maximum value $m_k$ of the current row. This is the first for loop.\n",
    "- The program instance (thread block) scans one row of the tensor and iteratively processes the data blocks of the current row to calculate the denominator (sum of exponentials) value $d_j$ of the current row. This is the second for loop.\n",
    "- The program instance (thread block) scans one row of the tensor and iteratively processes the data blocks of the current row to calculate the final softmax value $y_i$ of the current row. This is the third for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc218853-c7e7-4cda-b9cc-11b060380a84",
   "metadata": {},
   "source": [
    "<a id=\"helion-kernel\"></a>\n",
    "\n",
    "## 4. Creating a Helion two-pass softmax kernel\n",
    "\n",
    "This example demonstrates multiple Helion kernel implementations of the softmax function,\n",
    "including a simple wrapper around the PyTorch softmax implementation and a numerically optimized two-pass version.\n",
    "It also includes a check function to compare these kernels against the built-in PyTorch softmax function to verify correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002921b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import helion\n",
    "from helion._testing import run_example\n",
    "import helion.language as hl\n",
    "\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# %%\n",
    "@helion.kernel(autotune_effort=\"quick\")\n",
    "def softmax_two_pass(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Numerically optimized Helion kernel performing softmax in two passes.\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor of shape [m, n].\n",
    "    Returns:\n",
    "        torch.Tensor: Softmax output tensor of the same shape.\n",
    "    \"\"\"\n",
    "    m, n = x.size()\n",
    "    out = torch.empty_like(x)\n",
    "    block_size_m = hl.register_block_size(m)\n",
    "    block_size_n = hl.register_block_size(n)\n",
    "    for tile_m in hl.tile(m, block_size=block_size_m):\n",
    "        mi = hl.full([tile_m], float(\"-inf\"), dtype=torch.float32)\n",
    "        di = hl.zeros([tile_m], dtype=torch.float32)\n",
    "        for tile_n in hl.tile(n, block_size=block_size_n):\n",
    "            values = x[tile_m, tile_n]\n",
    "            local_amax = torch.amax(values, dim=1)\n",
    "            mi_next = torch.maximum(mi, local_amax)\n",
    "            di = di * torch.exp(mi - mi_next) + torch.exp(\n",
    "                values - mi_next[:, None]\n",
    "            ).sum(dim=1)\n",
    "            mi = mi_next\n",
    "        for tile_n in hl.tile(n, block_size=block_size_n):\n",
    "            values = x[tile_m, tile_n]\n",
    "            out[tile_m, tile_n] = torch.exp(values - mi[:, None]) / di[:, None]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7873fe",
   "metadata": {},
   "source": [
    "Check correctness by comparing the Helion softmax kernels against the PyTorch softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858612ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "def check(m: int, n: int) -> None:\n",
    "    \"\"\"\n",
    "    Runs correctness checks comparing Helion softmax kernels against PyTorch's softmax.\n",
    "    Args:\n",
    "        m (int): Number of rows in input tensor.\n",
    "        n (int): Number of columns in input tensor.\n",
    "    \"\"\"\n",
    "    x = torch.randn([m, n], device=\"cuda\", dtype=torch.float16)\n",
    "    run_example(softmax_two_pass, lambda x: torch.nn.functional.softmax(x, dim=1), (x,))\n",
    "\n",
    "\n",
    "# %%\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function to run the softmax kernel correctness check with example input size.\n",
    "    \"\"\"\n",
    "    check(4096, 2560)\n",
    "\n",
    "\n",
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7db3c6-f74a-46a2-98a0-7c5f0170f937",
   "metadata": {},
   "source": [
    "<a id=\"performance-benchmark\"></a>\n",
    "\n",
    "## 5. Performance benchmark and visualization\n",
    "\n",
    "This section compares the performance of Helion with Triton, PyTorch, and Aiter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e0ce9-5bc7-4cbe-b1aa-333093deb048",
   "metadata": {},
   "source": [
    "### Control group: Triton fused-softmax and Aiter softmax\n",
    "This example demonstrates how to implement a fused softmax kernel using Triton, with architectural awareness for AMD ROCm with CDNA-based backends.\n",
    "\n",
    "#### Implementation of Triton fused-softmax\n",
    "Triton provides a reference softmax sample named `fused-softmax`. Based on online softmax, it simplifies the maximum data calculation to remove one for loop. It also asks the compiler to use more threads per row by increasing the number of warps. This is often tuned for better performance. Finally, it bases the kernel launch scheme on the GPU hardware properties, resulting in higher GPU kernel occupancy and better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8684a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def is_hip():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n",
    "\n",
    "\n",
    "def is_cdna():\n",
    "    return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',\n",
    "                                                                                   'gfx90a', 'gfx908', 'gfx950')\n",
    "\n",
    "@triton.jit\n",
    "def fused_softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,\n",
    "                   num_stages: tl.constexpr):\n",
    "    # starting row of the program\n",
    "    row_start = tl.program_id(0)\n",
    "    row_step = tl.num_programs(0)\n",
    "    for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):\n",
    "        # The stride represents how much we need to increase the pointer to advance 1 row\n",
    "        row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "        # The block size is the next power of two greater than n_cols, so we can fit each\n",
    "        # row in a single block\n",
    "        col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "        input_ptrs = row_start_ptr + col_offsets\n",
    "        # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n",
    "        mask = col_offsets < n_cols\n",
    "        row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n",
    "        # Subtract maximum for numerical stability\n",
    "        row_minus_max = row - tl.max(row, axis=0)\n",
    "        # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n",
    "        numerator = tl.exp(row_minus_max)\n",
    "        denominator = tl.sum(numerator, axis=0)\n",
    "        softmax_output = numerator / denominator\n",
    "        # Write back output to DRAM\n",
    "        output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "        output_ptrs = output_row_start_ptr + col_offsets\n",
    "        tl.store(output_ptrs, softmax_output, mask=mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4883e4",
   "metadata": {},
   "source": [
    "Tune the kernel based on the properties of the target GPU platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To tune the kernel, first get some resource properties of the GPU by:\n",
    "properties = driver.active.utils.get_device_properties(DEVICE.index)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "WARP_SIZE = properties[\"warpSize\"]\n",
    "target = triton.runtime.driver.active.get_current_target()\n",
    "kernels = {}\n",
    "print(f\"NUM_SM: {NUM_SM}, NUM_REGS: {NUM_REGS}, SIZE_SMEM: {SIZE_SMEM}, WARP_SIZE: {WARP_SIZE}, target: {target}\")\n",
    "\n",
    "# Then set up the kernel launch configuration\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(8192, 8192, device=DEVICE)\n",
    "output_torch = torch.softmax(x, dim=-1)\n",
    "n_rows, n_cols = x.shape\n",
    "# Allocate output\n",
    "y = torch.empty_like(x)\n",
    "\n",
    "# The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`\n",
    "BLOCK_SIZE = triton.next_power_of_2(n_cols*2)\n",
    "\n",
    "# Another trick is to ask the compiler to use more threads per row by\n",
    "# increasing the number of warps (`num_warps`) over which each row is distributed.\n",
    "num_warps = 8\n",
    "\n",
    "# Number of software pipelining stages.\n",
    "num_stages = 4 if SIZE_SMEM > 200000 else 2\n",
    "\n",
    "print(f\"BLOCK_SIZE: {BLOCK_SIZE}, num_warps: {num_warps}, num_stages: {num_stages}\")\n",
    "\n",
    "# The occupancy of the kernel is limited by register usage. To maximize the occupancy, warm up the kernel to get register usage, and calculate the proper programs number.\n",
    "\n",
    "# pre-compile kernel to get register usage and compute thread occupancy.\n",
    "kernel = fused_softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,\n",
    "                                   num_stages=num_stages, num_warps=num_warps, grid=(1, ))\n",
    "kernel._init_handles()\n",
    "n_regs = kernel.n_regs\n",
    "size_smem = kernel.metadata.shared\n",
    "\n",
    "if is_hip():\n",
    "    if is_cdna():\n",
    "        NUM_GPRS = NUM_REGS * 2\n",
    "    MAX_NUM_THREADS = properties[\"max_threads_per_sm\"]\n",
    "    max_num_waves = MAX_NUM_THREADS // WARP_SIZE\n",
    "    occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps\n",
    "else:\n",
    "    occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
    "    \n",
    "occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
    "num_programs = NUM_SM * occupancy\n",
    "\n",
    "num_programs = min(num_programs, n_rows)\n",
    "\n",
    "print(f\"n_regs: {n_regs}, size_smem: {size_smem}, occupancy: {occupancy}, num_programs: {num_programs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4986805-3b16-4c93-8a2c-f6c99650902c",
   "metadata": {},
   "source": [
    "#### Install the ROCm Aiter kernel library\n",
    "\n",
    "Use the following commands to install the Aiter kernel library for the built-in softmax kernel function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc1a8ea-c043-4014-a57d-5d8eead8f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone --recursive https://github.com/ROCm/aiter.git\n",
    "cd aiter\n",
    "python3 setup.py develop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311be852-0deb-444a-b5e4-083d168e617d",
   "metadata": {},
   "source": [
    "\n",
    "## Run the benchmark and visualization\n",
    "\n",
    "Now run the benchmark and visualization for all versions of the softmax kernels to obtain the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c763c6e-51e6-4e17-b46e-9ba0dd653e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import triton\n",
    "import random\n",
    "import triton.language as tl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --- Helion Softmax ---\n",
    "def softmax_helion(x: torch.Tensor, dim=-1):\n",
    "    helion_output=softmax_two_pass(x)\n",
    "    return helion_output\n",
    "\n",
    "# --- Helper to run Triton Autotune ---\n",
    "def softmax_triton(x: torch.Tensor):\n",
    "    n_rows, n_cols = x.shape\n",
    "    triton_output = torch.empty_like(x)\n",
    "    kernel[(num_programs, 1, 1)](\n",
    "        y, \n",
    "        x, \n",
    "        x.stride(0), \n",
    "        y.stride(0), \n",
    "        n_rows, \n",
    "        n_cols, \n",
    "        BLOCK_SIZE, \n",
    "        num_stages)\n",
    "    return triton_output\n",
    "\n",
    "# --- PyTorch Naive Softmax ---\n",
    "def softmax_torch(x: torch.Tensor, dim=-1):\n",
    "    \"\"\"\n",
    "    Compute softmax using PyTorch built-in function.\n",
    "    Output is the same shape as input.\n",
    "    \"\"\"\n",
    "    torch_output = F.softmax(x, dim=dim)\n",
    "    return torch_output\n",
    "\n",
    "# --- Aiter Softmax ---\n",
    "from aiter.ops.triton.softmax import softmax\n",
    "def softmax_aiter(x: torch.Tensor):\n",
    "    aiter_output = softmax(x)\n",
    "    return aiter_output\n",
    "\n",
    "# --- Triton Benchmark ---\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # argument names to use as an x-axis for the plot\n",
    "        x_vals=[128 * i for i in range(55, 95)],  # different possible values for `x_name`\n",
    "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=['helion','triton', 'aiter','torch'],  # possible values for `line_arg`\n",
    "        line_names=[\"Helion Softmax\",\"Triton Softmax\", \"Aiter Softmax\",\"Torch Softmax\"],  # label name for the lines\n",
    "        styles=[('red', 'solid'),('cyan', 'solid'), ('black', 'solid'), ('orange', 'dashdot')],  # line styles\n",
    "        ylabel=\"GB/s\",  # label name for the y-axis\n",
    "        plot_name=\"Softmax Performance Benchamrk\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\n",
    "    ))\n",
    "def benchmark(M, N, provider):\n",
    "    # x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)\n",
    "    gen = torch.Generator(device=DEVICE).manual_seed(SEED)\n",
    "    x = torch.randn(M, N, device=DEVICE, dtype=torch.float32, generator=gen)\n",
    "    \n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    \n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: torch.softmax(x, dim=-1), rep=10, quantiles=quantiles\n",
    "        )\n",
    "\n",
    "    elif provider == 'aiter':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: softmax_aiter(x), rep=10, quantiles=quantiles\n",
    "        )\n",
    "    elif provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: softmax_triton(x), rep=10, quantiles=quantiles\n",
    "        )\n",
    "    elif provider == 'helion':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: softmax_helion(x), rep=10, quantiles=quantiles\n",
    "        )\n",
    "    # Calculate bandwidth: 2 * (read + write) * size / time\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms)\n",
    "\n",
    "# --- Run benchmark ---\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f798046",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! By running this Helion GPU kernel development tutorial, you learned how to develop and optimize Helion kernels on AMD GPUs.\n",
    "\n",
    "According to the final performance benchmark results, Helion not only simplifies high-performance GPU kernel development, but also delivers near-maximum performance, even outperforming Triton-based GPU kernels.\n",
    "\n",
    "Ideally, this tutorial encourages you to write, tune, test, and contribute to the Helion kernel on ROCm and AMD GPUs, helping shape the future of AI acceleration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
