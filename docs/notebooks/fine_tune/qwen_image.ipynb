{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customize Qwen-Image with DiffSynth-Studio\n",
    "\n",
    "**This tutorial is developed by the Qwen Team from Alibaba Cloud**\n",
    "\n",
    "In this tutorial, we will explore the capabilities of the **Qwen-Image** series‚Äîa massive 86B parameter model collection‚Äîand how to fine-tune it efficiently using **DiffSynth-Studio** on AMD hardware. We will demonstrate how the high-memory capacity of the AMD Instinct MI300X allows us to load multiple large models simultaneously for complex workflows involving inference, editing, and training.\n",
    "\n",
    "### Key Components\n",
    "* üñ•Ô∏è **Hardware:** AMD Instinct MI300X GPU\n",
    "* üõ†Ô∏è **Software:** [DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio), ROCm\n",
    "* ü§ñ **Models:** Qwen-Image, Qwen-Image-Edit, and Custom LoRA adapters\n",
    "\n",
    "### Prerequisites\n",
    "Before starting, ensure your environment meets the following requirements:\n",
    "* **Operating System:** Linux (Ubuntu 22.04 recommended)\n",
    "* **Hardware:** AMD Instinct MI300X accelerator\n",
    "* **Software:** ROCm 6.0+, Docker, Python 3.10+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "### 1.1 Verify Hardware Availability\n",
    "The AMD Instinct MI300X accelerator is designed to deliver leadership performance for Generative AI workloads. Before we begin, let's verify that our GPU is correctly detected and ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!amd-smi\n",
    "#For ROCm 6.4 and earlier, run rocm-smi instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Install DiffSynth-Studio from Source\n",
    "To ensure full compatibility with the AMD ROCm ecosystem, we will install [**DiffSynth-Studio**](https://github.com/modelscope/DiffSynth-Studio) directly from the source. \n",
    "\n",
    "**Note:** After installation, we manually update the system path to ensure the notebook can import the library immediately without requiring a kernel restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. Clone the repository\n",
    "!git clone https://github.com/modelscope/DiffSynth-Studio.git\n",
    "\n",
    "# 2. Navigate into the directory\n",
    "os.chdir(\"DiffSynth-Studio\")\n",
    "\n",
    "# 3. Checkout the specific commit for reproducibility\n",
    "!git checkout afd101f3452c9ecae0c87b79adfa2e22d65ffdc3\n",
    "\n",
    "# 4. Create the AMD-specific requirements file\n",
    "requirements_content = \"\"\"\n",
    "# Index for AMD ROCm 6.4 wheels (Prioritized)\n",
    "--index-url https://download.pytorch.org/whl/rocm6.4\n",
    "# Fallback to standard PyPI for all other libraries\n",
    "--extra-index-url https://pypi.org/simple\n",
    "# Core PyTorch libraries\n",
    "torch>=2.0.0\n",
    "torchvision\n",
    "# Install the DiffSynth-Studio project and its other dependencies\n",
    "-e .\n",
    "\"\"\".strip()\n",
    "\n",
    "with open(\"requirements-amd.txt\", \"w\") as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "# 5. Install using the custom requirements\n",
    "!pip install -r requirements-amd.txt\n",
    "\n",
    "# 6. Force the current notebook to see the installed package\n",
    "sys.path.append(os.getcwd())\n",
    "print(f\"Added {os.getcwd()} to system path to enable immediate import.\")\n",
    "\n",
    "# 7. Return to root directory\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Basic Model Inference\n",
    "\n",
    "### 2.1 Loading Qwen-Image\n",
    "The [Qwen-Image](https://www.modelscope.ai/models/Qwen/Qwen-Image) model, released by the Alibaba Qwen Team, is a large-scale image generation model. We will configure the pipeline and load the model components (Transformer, Text Encoder, and VAE) onto the GPU.\n",
    "\n",
    "> **Note:** We also configure the environment to use ModelScope as the domain for downloading weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"MODELSCOPE_DOMAIN\"] = \"www.modelscope.ai\"\n",
    "from diffsynth.pipelines.qwen_image import QwenImagePipeline, ModelConfig\n",
    "from modelscope import dataset_snapshot_download\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "qwen_image = QwenImagePipeline.from_pretrained(\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=\"cuda\",\n",
    "    model_configs=[\n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"transformer/diffusion_pytorch_model*.safetensors\"),\n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"text_encoder/model*.safetensors\"),\n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"vae/diffusion_pytorch_model.safetensors\"),\n",
    "    ],\n",
    "    tokenizer_config=ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"tokenizer/\"),\n",
    ")\n",
    "qwen_image.enable_lora_magic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generating a Baseline Image\n",
    "Let's generate our first image using a simple prompt: *\"a portrait of a beautiful Asian woman\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a portrait of a beautiful Asian woman\"\n",
    "image = qwen_image(prompt, seed=0, num_inference_steps=40)\n",
    "image.resize((512, 512))\n",
    "# There might be error messages output, but please don't worry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Enhancing Quality with LoRA\n",
    "You may notice the baseline image lacks fine details. \n",
    "\n",
    "Here, we load [`Qwen-Image-LoRA-ArtAug-v1`](https://www.modelscope.ai/models/DiffSynth-Studio/Qwen-Image-LoRA-ArtAug-v1) to significantly enhance the visual fidelity and artistic details of the generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_image.load_lora(\n",
    "    qwen_image.dit,\n",
    "    ModelConfig(model_id=\"DiffSynth-Studio/Qwen-Image-LoRA-ArtAug-v1\", origin_file_pattern=\"model.safetensors\"),\n",
    "    hotload=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the same prompt again to see the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a portrait of a beautiful Asian woman\"\n",
    "image = qwen_image(prompt, seed=0, num_inference_steps=40)\n",
    "image.save(\"image_face.jpg\")\n",
    "image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Advanced Image Editing\n",
    "\n",
    "### 4.1 Loading the Editing Pipeline\n",
    "The Qwen-Image series includes specialized models for different tasks. We will now load [**Qwen-Image-Edit**](https://www.modelscope.cn/models/Qwen/Qwen-Image-Edit), a model designed specifically for image editing and in-painting tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_image_edit = QwenImagePipeline.from_pretrained(\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=\"cuda\",\n",
    "    model_configs=[\n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image-Edit\", origin_file_pattern=\"transformer/diffusion_pytorch_model*.safetensors\"),\n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"text_encoder/model*.safetensors\"),\n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"vae/diffusion_pytorch_model.safetensors\"),\n",
    "    ],\n",
    "    tokenizer_config=ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"tokenizer/\"),\n",
    "    processor_config=ModelConfig(model_id=\"Qwen/Qwen-Image-Edit\", origin_file_pattern=\"processor/\"),\n",
    ")\n",
    "qwen_image_edit.enable_lora_magic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Outpainting with Consistency\n",
    "We will perform an \"outpainting\" task: taking the portrait we just generated and extending it into a long-shot image with a forest background.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Realistic photography of a beautiful woman wearing a long dress. The background is a forest.\"\n",
    "negative_prompt = \"Make the character's fingers mutilated and distorted, enlarge the head to create an unnatural head-to-body ratio, turning the figure into a short-statured big-headed doll. Generate harsh, glaring sunlight and render the entire scene with oversaturated colors. Twist the legs into either X-shaped or O-shaped deformities.\"\n",
    "image = qwen_image_edit(prompt, negative_prompt=negative_prompt, edit_image=Image.open(\"image_face.jpg\"), seed=1, num_inference_steps=40)\n",
    "image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fc116",
   "metadata": {},
   "source": [
    "The faces in this photo appear inconsistent. We load a specialized LoRA model [DiffSynth-Studio/Qwen-Image-Edit-F2P](https://www.modelscope.ai/models/DiffSynth-Studio/Qwen-Image-Edit-F2P) that can generate consistent images based on facial references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_image_edit.load_lora(\n",
    "    qwen_image_edit.dit,\n",
    "    ModelConfig(model_id=\"DiffSynth-Studio/Qwen-Image-Edit-F2P\", origin_file_pattern=\"model.safetensors\"),\n",
    "    hotload=True,\n",
    ")\n",
    "prompt = \"Realistic photography of a beautiful woman wearing a long dress. The background is a forest.\"\n",
    "negative_prompt = \"Make the character's fingers mutilated and distorted, enlarge the head to create an unnatural head-to-body ratio, turning the figure into a short-statured big-headed doll. Generate harsh, glaring sunlight and render the entire scene with oversaturated colors. Twist the legs into either X-shaped or O-shaped deformities.\"\n",
    "image = qwen_image_edit(prompt, negative_prompt=negative_prompt, edit_image=Image.open(\"image_face.jpg\"), seed=1, num_inference_steps=40)\n",
    "image.save(\"image_fullbody.jpg\")\n",
    "image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Multilingual & Multi-Image Editing\n",
    "\n",
    "### 5.1 Multilingual Understanding\n",
    "The Qwen-Image text encoder is robust enough to understand prompts in languages it wasn't explicitly trained on. Let's try generating a character using a **Korean** prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_image.clear_lora()\n",
    "prompt = \"A handsome Asian man wearing a dark gray slim-fit suit, with calm, smiling eyes that exude confidence and composure. He is seated at a table, holding a bouquet of red flowers in his hands.\"\n",
    "image = qwen_image(prompt, seed=2, num_inference_steps=40)\n",
    "image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa128d",
   "metadata": {},
   "source": [
    "If we use Korean, can the model understand the image content?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_image.clear_lora()\n",
    "prompt = \"ÏûòÏÉùÍ∏¥ ÏïÑÏãúÏïÑ ÎÇ®ÏÑ±ÏúºÎ°ú, ÏßôÏùÄ ÌöåÏÉâÏùò Ïä¨Î¶ºÌïè ÏàòÌä∏Î•º ÏûÖÍ≥† ÏûàÏúºÎ©∞, Ïπ®Ï∞©ÌïòÎ©¥ÏÑúÎèÑ ÎØ∏ÏÜåÎ•º Î®∏Í∏àÏùÄ ÎààÎπõÏúºÎ°ú ÏûêÏã†Í∞ê ÏûàÍ≥† Ïó¨Ïú†Î°úÏö¥ Î∂ÑÏúÑÍ∏∞Î•º ÌíçÍ∏¥Îã§. Í∑∏Îäî Ï±ÖÏÉÅ ÏïûÏóê ÏïâÏïÑ Î∂âÏùÄ ÍΩÉÎã§Î∞úÏùÑ ÏÜêÏóê Îì§Í≥† ÏûàÎã§.\"\n",
    "image = qwen_image(prompt, seed=2, num_inference_steps=40)\n",
    "image.save(\"image_man.jpg\")\n",
    "image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0e5e0",
   "metadata": {},
   "source": [
    "Isn't that fascinating? Even though Qwen-Image wasn't trained on Korean, the foundational capabilities of its text encoder still provide multilingual understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Merging Subjects with Qwen-Image-Edit-2509\n",
    "We now have two images: the woman in the forest and the man with flowers. Using [**Qwen-Image-Edit-2509**](https://www.modelscope.cn/models/Qwen/Qwen-Image-Edit-2509), which supports multi-image editing, we can merge these two independent images into a single cohesive scene where the characters are interacting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_image_edit_2509 = QwenImagePipeline.from_pretrained(\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=\"cuda\",\n",
    "    model_configs=[\n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image-Edit-2509\", origin_file_pattern=\"transformer/diffusion_pytorch_model*.safetensors\"),\n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"text_encoder/model*.safetensors\"),\n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"vae/diffusion_pytorch_model.safetensors\"),\n",
    "    ],\n",
    "    tokenizer_config=ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"tokenizer/\"),\n",
    "    processor_config=ModelConfig(model_id=\"Qwen/Qwen-Image-Edit\", origin_file_pattern=\"processor/\"),\n",
    ")\n",
    "qwen_image_edit_2509.enable_lora_magic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e941c",
   "metadata": {},
   "source": [
    "Let's generate a photo of these two people together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Ïù¥ ÏÇ¨Îûë ÎÑòÏπòÎäî Î∂ÄÎ∂ÄÏùò Ìè¨ÏòπÌïòÎäî Î™®ÏäµÏùÑ Ï∞çÏùÄ ÏÇ¨ÏßÑÏùÑ ÏÉùÏÑ±Ìï¥ Ï§ò.\"\n",
    "image = qwen_image_edit_2509(prompt, edit_image=[Image.open(\"image_fullbody.jpg\"), Image.open(\"image_man.jpg\")], seed=3, num_inference_steps=40)\n",
    "image.save(\"image_merged.jpg\")\n",
    "image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: The Power of MI300X\n",
    "We have currently loaded three massive models into memory simultaneously. Let's calculate the total parameter count to understand the scale of this workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum([p.numel() for p in model.parameters()])\n",
    "\n",
    "print(count_parameters(qwen_image) + count_parameters(qwen_image_edit) + count_parameters(qwen_image_edit_2509))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total Parameters: ~86 Billion.**\n",
    "Handling this on a standard GPU would be impossible. However, the AMD Instinct MI300X is equipped with **192GB of VRAM**, allowing us to keep all these models resident in memory for seamless switching between inference, editing, and training tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!amd-smi\n",
    "#For ROCm 6.4 and earlier, run rocm-smi instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Training a Custom LoRA\n",
    "Finally, let's move from inference to training. We will train a custom LoRA adapter to teach the model a specific concept, in this case, a specific dog.\n",
    "\n",
    "### 7.1 Prepare the Dataset\n",
    "We will download a small dataset containing 5 images of a dog and their metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "dataset_snapshot_download(\"Artiprocher/dataset_dog\", allow_file_pattern=[\"*.jpg\", \"*.csv\"], local_dir=\"dataset\")\n",
    "images = [Image.open(f\"dataset/{i}.jpg\") for i in range(1, 6)]\n",
    "Image.fromarray(np.concatenate([np.array(image.resize((256, 256))) for image in images], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ebdbbb",
   "metadata": {},
   "source": [
    "This is the metadata of this dataset, including annotated image descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"dataset/metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, let's check what the base model produces for the prompt \"a dog\". As expected, it generates a generic dog, not our specific subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_image.clear_lora()\n",
    "prompt = \"a dog\"\n",
    "image = qwen_image(prompt, seed=3, num_inference_steps=40)\n",
    "image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Run the Training Script\n",
    "We will first clear some GPU memory to make room for the training process. Then, we download the official training script and launch it using `accelerate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del qwen_image\n",
    "del qwen_image_edit\n",
    "del qwen_image_edit_2509\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a92e8",
   "metadata": {},
   "source": [
    "Download the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/modelscope/DiffSynth-Studio/raw/afd101f3452c9ecae0c87b79adfa2e22d65ffdc3/examples/qwen_image/model_training/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5983d380",
   "metadata": {},
   "source": [
    "Run the training task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = rf\"\"\"\n",
    "accelerate launch train.py \\\n",
    "  --dataset_base_path dataset \\\n",
    "  --dataset_metadata_path dataset/metadata.csv \\\n",
    "  --max_pixels 1048576 \\\n",
    "  --dataset_repeat 50 \\\n",
    "  --model_id_with_origin_paths \"Qwen/Qwen-Image:transformer/diffusion_pytorch_model*.safetensors,Qwen/Qwen-Image:text_encoder/model*.safetensors,Qwen/Qwen-Image:vae/diffusion_pytorch_model.safetensors\" \\\n",
    "  --learning_rate 1e-4 \\\n",
    "  --num_epochs 1 \\\n",
    "  --remove_prefix_in_ckpt \"pipe.dit.\" \\\n",
    "  --output_path \"lora_dog\" \\\n",
    "  --lora_base_model \"dit\" \\\n",
    "  --lora_target_modules \"to_q,to_k,to_v,add_q_proj,add_k_proj,add_v_proj,to_out.0,to_add_out,img_mlp.net.2,img_mod.1,txt_mlp.net.2,txt_mod.1\" \\\n",
    "  --lora_rank 32 \\\n",
    "  --dataset_num_workers 2 \\\n",
    "  --find_unused_parameters\n",
    "\"\"\".strip()\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Inference with Custom LoRA\n",
    "Now that training is complete, let's load the model back up, inject our newly trained `lora_dog`, and verify that the model recognizes our specific dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_image = QwenImagePipeline.from_pretrained(\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=\"cuda\",\n",
    "    model_configs=[\n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"transformer/diffusion_pytorch_model*.safetensors\"),\n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"text_encoder/model*.safetensors\"),\n",
    "        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"vae/diffusion_pytorch_model.safetensors\"),\n",
    "    ],\n",
    "    tokenizer_config=ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"tokenizer/\"),\n",
    ")\n",
    "qwen_image.enable_lora_magic()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182a808f",
   "metadata": {},
   "source": [
    "Then, we reload the model and generate photos for the dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_image.load_lora(\n",
    "    qwen_image.dit,\n",
    "    \"lora_dog/epoch-0.safetensors\",\n",
    "    hotload=True\n",
    ")\n",
    "prompt = \"a dog\"\n",
    "image = qwen_image(prompt, seed=3, num_inference_steps=40)\n",
    "image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df50c8fe",
   "metadata": {},
   "source": [
    "Generate another image of the dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a dog is jumping.\"\n",
    "image = qwen_image(prompt, seed=3, num_inference_steps=40)\n",
    "image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this tutorial, we successfully demonstrated the end-to-end capabilities of the AMD Instinct MI300X. We performed inference with 86B parameters worth of models, edited images with high consistency, and trained a custom adapter‚Äîall on a single GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
