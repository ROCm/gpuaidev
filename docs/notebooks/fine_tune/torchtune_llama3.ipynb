{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73b0caa-096b-45fe-b26f-032128d4334f",
   "metadata": {},
   "source": [
    "# Fine-tune Llama-3.1 8B with torchtune\n",
    "\n",
    "This tutorial demonstrates how to fine-tune the Llama-3.1 8B large language model (LLM) on AMD ROCm GPUs by leveraging torchtune. Torchtune is an easy-to-use PyTorch library for authoring, post-training, and experimenting with LLMs. It features:\n",
    "\n",
    "- Hackable training recipes for SFT, knowledge distillation, RL and RLHF, and quantization-aware training.\n",
    "- Simple PyTorch implementations of popular LLMs like Llama, Gemma, Mistral, Phi, Qwen, and more.\n",
    "- OOTB best-in-class memory efficiency, performance improvements, and scaling, utilizing the latest PyTorch APIs.\n",
    "- YAML configs to easily configure training, evaluation, quantization, or inference recipes.\n",
    "\n",
    "For more information, see the [official torchtune GitHub page](https://github.com/pytorch/torchtune)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d94b31-35f8-4c8c-af0a-8a10aa5b4c62",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup. \n",
    "\n",
    "### Operating system\n",
    "\n",
    "* **Ubuntu 22.04**: Ensure your system is running Ubuntu version 22.04.\n",
    "\n",
    "### Hardware\n",
    "\n",
    "* **AMD Instinct™ GPUs**: This tutorial was tested on an AMD Instinct MI300X GPU. Ensure you are using an AMD Instinct GPU or compatible hardware with ROCm support and that your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "### Software\n",
    "\n",
    "* **ROCm 6.3**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html). After installation, confirm your setup using:\n",
    "\n",
    "    ``` bash\n",
    "    amd-smi\n",
    "    ```\n",
    "\n",
    "    This command lists your AMD GPUs with relevant details.\n",
    "    \n",
    "    **Note**: For ROCm 6.4 and earlier, use the `rocm-smi` command instead.\n",
    "\n",
    "* **Docker**: Ensure Docker is installed and configured correctly. Follow the Docker installation guide for your operating system.\n",
    "\n",
    "   **Note**: Ensure the Docker permissions are correctly configured. To configure permissions to allow non-root access, run the following commands:\n",
    "\n",
    "   ``` bash\n",
    "   sudo usermod -aG docker $USER\n",
    "   newgrp docker\n",
    "   ```\n",
    "\n",
    "   Verify Docker is working correctly:\n",
    "\n",
    "   ``` bash\n",
    "   docker run hello-world\n",
    "   ```\n",
    "\n",
    "### Hugging Face API access\n",
    "\n",
    "* Obtain an API token from [Hugging Face](https://huggingface.co) for downloading models.\n",
    "* Ensure the Hugging Face API token has the necessary permissions and approval to access the [Meta Llama checkpoints](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct).\n",
    "\n",
    "### Data preparation\n",
    "\n",
    "* This tutorial uses a sample dataset from Hugging Face, which is prepared during the setup steps.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4926a00e-7805-4de6-bb72-43db16ac09a2",
   "metadata": {},
   "source": [
    "## Prepare the training environment\n",
    "\n",
    "### 1. Pull the Docker image\n",
    "\n",
    "Ensure your system meets the [system requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "Pull the Docker image required for this tutorial:\n",
    "\n",
    "``` bash\n",
    "docker pull rocm/pytorch-training:latest\n",
    "```\n",
    "\n",
    "### 2. Launch the Docker container\n",
    "\n",
    "Launch the Docker container and map the necessary directories. Replace `/path/to/notebooks` with the full path to the directory on your host machine where these notebooks are stored.\n",
    "\n",
    "``` bash\n",
    "docker run -it --rm \\\n",
    "  --network=host \\\n",
    "  --device=/dev/kfd \\\n",
    "  --device=/dev/dri \\\n",
    "  --group-add=video \\\n",
    "  --ipc=host \\\n",
    "  --cap-add=SYS_PTRACE \\\n",
    "  --security-opt seccomp=unconfined \\\n",
    "  --shm-size 8G \\\n",
    "  -v $(pwd):/workspace \\\n",
    "  -w /workspace/notebooks \\\n",
    "  rocm/pytorch-training:latest\n",
    "```\n",
    "\n",
    "**Note**: This command mounts the current directory to the `/workspace` directory in the container. Ensure the notebook file is either copied to this directory before running the Docker command or uploaded into the Jupyter Notebook environment after it starts. Save the token or URL provided in the terminal output to access the notebook from your web browser. You can download this notebook from the [AI Developer Hub GitHub repository](https://github.com/ROCm/gpuaidev).\n",
    "\n",
    "### 3. Install and launch Jupyter\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "``` bash\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Start the Jupyter server:\n",
    "\n",
    "``` bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b3698c",
   "metadata": {},
   "source": [
    "### 4. Install the required libraries\n",
    "\n",
    "Install the libraries required for this tutorial. Run the following commands inside the Jupyter notebook running within the Docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a51315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch, torchvision, torchao nightlies\n",
    "!pip install --pre --upgrade torch torchvision torchao --index-url https://download.pytorch.org/whl/nightly/rocm6.3/\n",
    "!pip install --pre --upgrade torchtune --extra-index-url https://download.pytorch.org/whl/nightly/rocm6.3/ \n",
    "# This note book is verified under torch==2.7.0.dev20250302+rocm6.3, torchao==0.10.0.dev20250303+rocm6.3,  torchvision==0.22.0.dev20250302+rocm6.3, torchtune==0.6.0.dev20250302+rocm6.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f703213",
   "metadata": {},
   "source": [
    "Verify the installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cfc30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the installation and version of the required libraries\n",
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c923c",
   "metadata": {},
   "source": [
    "Here is the expected output:\n",
    "\n",
    "```\n",
    "pytorch-triton-rocm     3.2.0+git4b3bb1f8\n",
    "torch                   2.7.0.dev20250302+rocm6.3\n",
    "torchdata               0.11.0\n",
    "torchtune               0.6.0.dev20250302+rocm6.3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea665d8",
   "metadata": {},
   "source": [
    "### 5. Verify torchtune for ROCm 6.3\n",
    "\n",
    "To confirm that the package is installed correctly, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1775d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0aa46a",
   "metadata": {},
   "source": [
    "You should see the following output:\n",
    "\n",
    "```\n",
    "usage: tune [-h] {download,ls,cp,run,validate,cat} ...\n",
    "\n",
    "Welcome to the torchtune CLI!\n",
    "\n",
    "options:\n",
    "  -h, --help            show this help message and exit\n",
    "\n",
    "subcommands:\n",
    "  {download,ls,cp,run,validate,cat}\n",
    "    download            Download a model from the Hugging Face Hub or Kaggle\n",
    "                        Model Hub.\n",
    "    ls                  List all built-in recipes and configs\n",
    "    cp                  Copy a built-in recipe or config to a local path.\n",
    "    run                 Run a recipe. For distributed recipes, this supports\n",
    "                        all torchrun arguments.\n",
    "    validate            Validate a config and ensure that it is well-formed.\n",
    "    cat                 Pretty print a config, making it easy to know which\n",
    "                        parameters you can override with `tune run`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d0c48",
   "metadata": {},
   "source": [
    "\n",
    "**⚠️ Important: ensure the correct kernel is selected**\n",
    "\n",
    "If the verification process fails, ensure the correct Jupyter kernel is selected for your notebook.\n",
    "To change the kernel, follow these steps:\n",
    "\n",
    "1. Go to the **Kernel** menu.\n",
    "2. Select **Change Kernel**.\n",
    "3. Select `Python 3 (ipykernel)` from the list.\n",
    "\n",
    "**Important**: Failure to select the correct kernel can lead to unexpected issues when running the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad2378",
   "metadata": {},
   "source": [
    "### 6. Provide your Hugging Face token\n",
    "\n",
    "You'll require a Hugging Face API token to access Llama-3.1. Generate your token at [Hugging Face Tokens](https://huggingface.co/settings/tokens) and request access for [Llama-3.1 8B](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct). Tokens typically start with \"hf_\". \n",
    "\n",
    "Run the following interactive block in your Jupyter notebook to set up the token:\n",
    "\n",
    "**Note**: Uncheck the \"Add token as Git credential\" option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "# Prompt the user to log in\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f53e64b",
   "metadata": {},
   "source": [
    "Verify that your token was accepted correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a0725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb55cf-7f2d-45c6-9c5c-86a82ca4c9c6",
   "metadata": {},
   "source": [
    "## Fine-tuning the model\n",
    "\n",
    "This section covers the process of setting up and running fine-tuning for the Llama-3.1 model using torchtune. The following steps describe how to set up GPUs, import the required libraries, configure the model and training parameters, and run the fine-tuning process.\n",
    "\n",
    "**⚠️ Important: ensure the correct kernel is selected**\n",
    "\n",
    "Ensure the correct Jupyter kernel is selected for your notebook. To change the kernel, follow these steps:\n",
    "\n",
    "1. Go to the **Kernel** menu.\n",
    "2. Select **Change Kernel**.\n",
    "3. Select `Python 3 (ipykernel)` from the list.\n",
    "\n",
    "**Important**: Failure to select the correct kernel can lead to unexpected issues when running the notebook.\n",
    "\n",
    "### Set and verify the GPU availability\n",
    "\n",
    "Begin by specifying the GPUs available for fine-tuning. Verify that they are properly detected by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb94e7-c059-4883-97dc-c36546e65236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "gpus= [0, 1] # Rank 0 is for MI300x single device finetune, and Rank 0/1 for full \n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", ','.join(map(str, gpus)))\n",
    "# Ensure PyTorch detects the GPUs correctly\n",
    "print(f\"PyTorch detected number of available devices: {torch.cuda.device_count()}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb3982-f46e-47e5-b4f7-f9fbf873a2fc",
   "metadata": {},
   "source": [
    "### Download the Llama model\n",
    "\n",
    "Run the following command to download the weights to your local machine. This also downloads the tokenizer model and a responsible use guide.\n",
    "\n",
    "To download Llama-3.1, use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26886732-b369-495f-8b6b-decdf0564219",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune download meta-llama/Meta-Llama-3.1-8B-Instruct \\\n",
    "    --output-dir /tmp/Meta-Llama-3.1-8B-Instruct \\\n",
    "    --ignore-patterns \"original/consolidated.00.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d747b-4ef6-4969-9d15-f9834a5ee6bb",
   "metadata": {},
   "source": [
    "### Run the fine-tuning recipes\n",
    "\n",
    "Use these fine-tuning recipes for single GPU or distributed training.\n",
    "\n",
    "#### Single GPU training\n",
    "\n",
    "By default, the [alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca) dataset is used for fine-tuning, You can fine-tune Llama-3.1 8B with LoRA on a single GPU using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c916dcd-fc94-4214-895a-9720ad3ec3ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tune run lora_finetune_single_device --config llama3_1/8B_lora_single_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe90d81-e99e-46fd-bf96-5246210f75df",
   "metadata": {},
   "source": [
    "#### Distributed training\n",
    "For distributed training, the tune CLI integrates with torchrun.\n",
    "\n",
    "**Note**: you must have more than one GPU available to run the distributed training example. To run a full fine-tune of Llama-3.1 8B on two GPUs, use this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235355d2-634d-4444-8497-71058a1e473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune run --nproc_per_node 2 full_finetune_distributed --config llama3_1/8B_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce556ea-9525-46ab-bcf2-1fe2e8c319d2",
   "metadata": {},
   "source": [
    "### Customize your recipes for Llama\n",
    "\n",
    "There are two ways to modify configurations.\n",
    "\n",
    "#### Configuration overrides\n",
    "\n",
    "You can directly overwrite configuration fields from the command line. For example, you can set the batch size to `16` and disable activation checkpoints using the same command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d525b641-c645-4987-9ae6-173d6a75e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune run lora_finetune_single_device \\\n",
    "    --config llama3_1/8B_lora_single_device \\\n",
    "    batch_size=16 \\\n",
    "    enable_activation_checkpointing=False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35f6b7-4a2f-467c-b1fd-e53bf7dcd837",
   "metadata": {},
   "source": [
    "**Note**: If you encounter out-of-memory (OOM) errors, reduce the `batch_size` or enable gradient checkpointing. Use `amd-smi` to monitor VRAM usage during fine-tuning. For ROCm 6.4 and earlier, use the `rocm-smi` command for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9d33c-95b2-467c-8212-96a7810b5e3c",
   "metadata": {},
   "source": [
    "#### Update a local copy\n",
    "\n",
    "You can also copy the configuration to your local directory and modify the contents directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3eecd4-a968-42dd-89e2-f94b4633ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune cp llama3_1/8B_lora_single_device ./my_custom_config_llama3_1_8B_lora_single_device.yaml\n",
    "# Copied to ./my_custom_config_llama3_1_8B_lora_single_device.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab9baef-c9a4-4438-aa88-cbcdcec199b9",
   "metadata": {},
   "source": [
    "Then you can run your custom recipe by applying the `tune run` command to your local files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a176a-a893-46ee-8df9-b9bc4ddca1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune run lora_finetune_single_device --config ./my_custom_config_llama3_1_8B_lora_single_device.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52620657",
   "metadata": {},
   "source": [
    "Run the `tune --help` command to see all possible CLI commands and options. For more information on using and updating configurations, see the official torchtune [deep-dive](https://meta-pytorch.org/torchtune/main/deep_dives/configs.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38005e08-f1e8-4cde-a139-b18e74e41bb8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Custom datasets\n",
    "\n",
    "`torchtune` supports fine-tuning on a variety of different datasets, including instruct-style, chat-style, preference datasets, and more. To learn more about how to apply these components to fine-tune on your own custom dataset, see the provided links along with the torchtune API [docs](https://meta-pytorch.org/torchtune/main/api_ref_datasets.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f3834-0c06-4907-83cf-8f8ff9348f88",
   "metadata": {},
   "source": [
    "### Monitoring GPU memory\n",
    "\n",
    "To monitor GPU memory during training, run the following command in a terminal.\n",
    "\n",
    "**Note**: For ROCm 6.4 and earlier, use the `rocm-smi` command instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb36ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!amd-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91846614",
   "metadata": {},
   "source": [
    "This command displays memory usage and other GPU metrics to ensure your hardware resources are being optimally used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
