{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73b0caa-096b-45fe-b26f-032128d4334f",
   "metadata": {},
   "source": [
    "# Fine-Tuning Llama-3.1 with LoRA on AMD ROCm GPUs\n",
    "\n",
    "This tutorial demonstrates how to fine-tune the **Llama-3.1-8B** large language model using **Low-Rank Adaptation (LoRA)** on AMD ROCm GPUs. **Llama-3.1**, developed by Meta, is a widely used open-source large language model. For more information, visit [Meta's Llama page](https://ai.meta.com/llama/).\n",
    "\n",
    "Fine-tuning large language models can be computationally intensive due to the need to optimize all parameters. This approach, known as **full-parameter fine-tuning**, requires updating every weight in the model, leading to significant demand of memory and compute resources, often up to four times the size of the model itself.\n",
    "\n",
    "To address these challenges, we use **LoRA** (Low-Rank Adaptation), a parameter-efficient fine-tuning (PEFT) technique. As described by Hu et al. in [their 2021 paper](https://arxiv.org/abs/2106.09685), LoRA freezes the pre-trained model weights and introduces trainable rank-decomposition matrices into each layer of the Transformer architecture. This significantly reduces the number of trainable parameters for downstream tasks while maintaining performance, making it possible to fine-tune large models efficiently on resource-constrained hardware.\n",
    "\n",
    "> **Reference**: Hu et al., \"LoRA: Low-Rank Adaptation of Large Language Models,\" 2021.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d94b31-35f8-4c8c-af0a-8a10aa5b4c62",
   "metadata": {},
   "source": [
    "\n",
    "## **Prerequisites**\n",
    "\n",
    "### **1. Hardware Requirements**\n",
    "- AMD Instict GPUs (e.g., MI210, MI300X).  \n",
    "- Ensure your system meets the [System Requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html), including ROCm 6.0+ and Ubuntu 22.04.\n",
    "\n",
    "### **2. Docker**\n",
    "- Install Docker with GPU support.\n",
    "- Ensure your user has appropriate permissions to access the GPU.\n",
    "- Verify Docker permissions and GPU access:\n",
    "  ```bash\n",
    "  docker run --rm --device=/dev/kfd --device=/dev/dri rocm/pytorch:rocm6.1_ubuntu22.04_py3.10_pytorch_2.1.2 rocm-smi\n",
    "  ```\n",
    "\n",
    "### **3. Hugging Face API Access**\n",
    "- Obtain an API token from [Hugging Face](https://huggingface.co) for downloading models.\n",
    "- Ensure you have a Hugging Face API token with the necessary permissions and approval to access [Meta's LLaMA checkpoints](https://huggingface.co/meta-llama/Llama-3.1-8B).\n",
    "\n",
    "### **4. Data Preparation**\n",
    "- For this tutorial, we use a sample dataset from Hugging Face, which will be prepared during the setup steps.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4926a00e-7805-4de6-bb72-43db16ac09a2",
   "metadata": {},
   "source": [
    "## **Prepare Training Environment**\n",
    "\n",
    "### **1. Pull the Docker Image**\n",
    "\n",
    "Ensure your system meets the [System Requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "Pull the Docker image required for this tutorial:\n",
    "\n",
    "```bash\n",
    "docker pull rocm/pytorch:rocm6.1_ubuntu22.04_py3.10_pytorch_2.1.2\n",
    "```\n",
    "\n",
    "### **2. Launch the Docker Container**\n",
    "\n",
    "Launch the Docker container and map the necessary directories. Replace `/path/to/notebooks` with the full path to the directory on your host machine where these notebooks are stored.\n",
    "\n",
    "```bash\n",
    "docker run -it --rm \\\n",
    "  --network=host \\\n",
    "  --device=/dev/kfd \\\n",
    "  --device=/dev/dri \\\n",
    "  --group-add=video \\\n",
    "  --ipc=host \\\n",
    "  --cap-add=SYS_PTRACE \\\n",
    "  --security-opt seccomp=unconfined \\\n",
    "  --shm-size 8G \\\n",
    "  --hostname=ROCm-FT \\\n",
    "  -v /path/to/notebooks:/workspace/notebooks \\\n",
    "  -w /workspace/notebooks \\\n",
    "  rocm/pytorch:rocm6.1_ubuntu22.04_py3.10_pytorch_2.1.2\n",
    "```\n",
    "\n",
    "**Important**: Replace `/path/to/notebooks` with the absolute path to the directory on your host machine where your notebooks are stored. Ensure this directory is accessible to Docker and contains the necessary files for this tutorial.\n",
    "\n",
    "### **3. Install and Launch Jupyter**\n",
    "\n",
    "Inside the Docker container, install Jupyter using the following command:\n",
    "\n",
    "```bash\n",
    "pip install --upgrade pip setuptools wheel\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Start the Jupyter server:\n",
    "```bash\n",
    "jupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b3698c",
   "metadata": {},
   "source": [
    "### **4. Install Required Libraries**\n",
    "Install the libraries needed for this tutorial. Run the following commands inside the Jupyter notebook running within the Docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a51315",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries for fine-tuning, including parameter-efficient fine-tuning (peft) and transformers\n",
    "!pip install pandas peft==0.14.0 transformers==4.47.1 trl==0.13.0 accelerate==1.2.1 scipy tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f703213",
   "metadata": {},
   "source": [
    "Verify the installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cfc30b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Verify the installation and version of the required libraries\n",
    "!pip list | grep peft\n",
    "!pip list | grep transformer\n",
    "!pip list | grep accelerate\n",
    "!pip list | grep trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad2378",
   "metadata": {},
   "source": [
    "### **6. Provide Your Hugging Face Token**\n",
    "\n",
    "You will need a Hugging Face API token to access Llama-3.1-8B. Tokens typically start with \"hf_\". Generate your token at [Hugging Face Tokens](https://huggingface.co/settings/tokens) and request access for [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B).\n",
    "\n",
    "Run the following interactive block in your Jupyter notebook to set up the token:\n",
    "\n",
    "***Note***: Please uncheck the \"Add token as Git credential\" option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98d94d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "# Prompt the user to log in\n",
    "status = notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f53e64b",
   "metadata": {},
   "source": [
    "Verify that your token was captured correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a0725",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Validate the token\n",
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb55cf-7f2d-45c6-9c5c-86a82ca4c9c6",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model\n",
    "\n",
    "This section walks through the process of setting up and executing fine-tuning for the Llama-3.1 model using the LoRA technique. The following steps include setting up GPUs, importing the required libraries, configuring the model and training parameters, and running the fine-tuning process.\n",
    "\n",
    "\n",
    "**⚠️ Important: Ensure the Correct Kernel is Selected**  \n",
    "please ensure the correct Jupyter kernel is selected for your notebook.\n",
    "To do this:\n",
    "1. Go to the \"Kernel\" menu.\n",
    "2. Click \"Change Kernel.\"\n",
    "3. Select `Python 3 (ipykernel)` from the list.\n",
    "\n",
    "**Failure to select the correct kernel may lead to unexpected issues when running the notebook.**\n",
    "\n",
    "### Set and Verify GPU Availability\n",
    "\n",
    "Begin by specifying the GPUs available for fine-tuning and verifying that they are properly detected by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb94e7-c059-4883-97dc-c36546e65236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "gpus = [0, 1, 2, 3] # Specify the GPUs to be used for training\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", ','.join(map(str, gpus)))\n",
    "# Ensure PyTorch detects the GPUs correctly\n",
    "print(f\"PyTorch detected number of available devices: {torch.cuda.device_count()}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb3982-f46e-47e5-b4f7-f9fbf873a2fc",
   "metadata": {},
   "source": [
    "### Import the Required Packages\n",
    "\n",
    "Next, import the libraries necessary for fine-tuning, including utilities for dataset loading, model configuration, training setup, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26886732-b369-495f-8b6b-decdf0564219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets and transformers for handling the Llama-3.1 model\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "# Import utilities for LoRA fine-tuning and training configurations\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "print(\"Successfully imported required libraries for dataset handling, model configuration, and LoRA fine-tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d747b-4ef6-4969-9d15-f9834a5ee6bb",
   "metadata": {},
   "source": [
    "### Configuring the Model \n",
    "\n",
    "Load the base model, tokenizer, and set up the quantization configuration for efficient fine-tuning on ROCm-enabled GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c916dcd-fc94-4214-895a-9720ad3ec3ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_model_name = \"meta-llama/Llama-3.1-8B\"  # Hugging Face model repository name\n",
    "new_model_name = \"Llama-3.1-8B-lora\"  # Name for the fine-tuned model\n",
    "\n",
    "# Load and configure the tokenizer for padding and tokenization\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_name, \n",
    "    trust_remote_code=True, \n",
    "    use_fast=True\n",
    ")\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load the pre-trained Llama-3.1 model with device mapping for GPU\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Disable caching to optimize for fine-tuning\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe90d81-e99e-46fd-bf96-5246210f75df",
   "metadata": {},
   "source": [
    "### Load and Prepare the Dataset\n",
    "\n",
    "Fine-tune the base model for a question-and-answer task using a small dataset called [mlabonne/guanaco-llama2-1k](https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k/tree/main). This dataset is a subset (1,000 samples) of the [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset. This dataset is a human-generated, human-annotated, assistant-style conversation corpus that contains 161,443 messages in 35 different languages, annotated with 461,292 quality ratings. This results in over 10,000 fully annotated conversation trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235355d2-634d-4444-8497-71058a1e473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "data_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "# Load the fine-tuning dataset from Hugging Face\n",
    "training_data = load_dataset(data_name, split=\"train\")\n",
    "\n",
    "# Display dataset structure and a sample for verification\n",
    "print(training_data.shape)\n",
    "#11 is a QA sample in English\n",
    "print(training_data[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce556ea-9525-46ab-bcf2-1fe2e8c319d2",
   "metadata": {},
   "source": [
    "### Fine-Tuning Configuration\n",
    "\n",
    "Define the hyperparameters and configurations for the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d525b641-c645-4987-9ae6-173d6a75e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments, including output directory and optimization settings\n",
    "# Specify number of epochs, batch size, learning rate, and logging steps\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=\"./results_lora\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"adamw_torch\",\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    learning_rate=4e-5,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "print(\"Training parameters configured!.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35f6b7-4a2f-467c-b1fd-e53bf7dcd837",
   "metadata": {},
   "source": [
    "***NOTE***：If you encounter out-of-memory (OOM) errors, reduce per_device_train_batch_size or enable gradient checkpointing. Use rocm-smi to monitor VRAM usage during fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9d33c-95b2-467c-8212-96a7810b5e3c",
   "metadata": {},
   "source": [
    "### LoRA Configuration\n",
    "\n",
    "Low-Rank Adaptation (LoRA) introduces lightweight rank-decomposition matrices into the base model. By focusing only on updating these additional matrices, LoRA reduces the number of trainable parameters significantly, enabling efficient fine-tuning of large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3eecd4-a968-42dd-89e2-f94b4633ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "# Configure LoRA parameters for low-rank adaptation\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=8, # Alpha controls the scaling parameter\n",
    "    lora_dropout=0.1,\n",
    "    r=8, # r specifies the rank of the low-rank matrices\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(base_model, peft_parameters)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab9baef-c9a4-4438-aa88-cbcdcec199b9",
   "metadata": {},
   "source": [
    "This indicates that only a small portion of the total parameters are trainable during fine-tuning ensuring resource efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd32708",
   "metadata": {},
   "source": [
    "### Fine-Tuning with LoRA\n",
    "LoRA's lightweight approach allows fine-tuning while maintaining high efficiency in terms of computation and memory usage. We now define a training pipeline using the LoRA-integrated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a176a-a893-46ee-8df9-b9bc4ddca1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer with the fine-tuning dataset and configurations\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=training_data,\n",
    "    peft_config=peft_parameters,\n",
    "    args=train_params\n",
    ")\n",
    "\n",
    "# Execute the training process\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52620657",
   "metadata": {},
   "source": [
    "During training, the model outputs metrics such as training loss, step progress, and runtime performance, which can be monitored for insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38005e08-f1e8-4cde-a139-b18e74e41bb8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Save the Fine-Tuned Model\n",
    "\n",
    "After training is complete, save the model with the specified name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c0830b9-5f74-4d6f-bb33-355332c7e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model to the specified directory\n",
    "fine_tuning.model.save_pretrained(new_model_name)\n",
    "print(\"Successfully saved the model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f3834-0c06-4907-83cf-8f8ff9348f88",
   "metadata": {},
   "source": [
    "### Monitoring GPU Memory\n",
    "\n",
    "To monitor GPU memory during training, use the following command in a terminal:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a05e31c-b5b5-4e58-ab2a-5b8004a3bec3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!rocm-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91846614",
   "metadata": {},
   "source": [
    "This will display memory usage and other GPU metrics to ensure your hardware resources are used optimally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42cf9ef-b529-4053-8f8e-f7cc80f34c33",
   "metadata": {},
   "source": [
    "### Comparison: Fine-Tuning with and without LoRA\n",
    "\n",
    "To understand the benefits of LoRA, you can compare fine-tuning metrics (such as memory usage, training speed, and loss) between:\n",
    "\n",
    "Fine-tuning with LoRA (low-rank adaptation layers).\n",
    "Full fine-tuning (updating all model parameters).\n",
    "LoRA's resource-efficient approach is especially beneficial for training on hardware with limited memory or computational power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7312872-12a7-4472-8111-a2ff43ad05b5",
   "metadata": {},
   "source": [
    "### Testing the Fine-Tuned Model\n",
    "\n",
    "Load the fine-tuned model and run inference to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0451b4ef-24b8-419e-816f-33eaac534dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "from peft import LoraConfig, PeftModel\n",
    "peft_model = PeftModel.from_pretrained(base_model, new_model_name)\n",
    "peft_model = peft_model.merge_and_unload()\n",
    "\n",
    "# Configure the tokenizer for text generation\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\"\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=peft_model, \n",
    "    tokenizer=llama_tokenizer,\n",
    "    max_length=1024,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a9071a",
   "metadata": {},
   "source": [
    "Now let's run a query and view the response generated by our fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36335c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the fine-tuned model to generate responses for a query\n",
    "query = \"What do you think is the most important part of building an AI chatbot?\"\n",
    "output = pipeline(f\"<s>[INST] {query} [/INST]\")\n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
