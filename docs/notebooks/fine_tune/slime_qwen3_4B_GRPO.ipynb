{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73b0caa-096b-45fe-b26f-032128d4334f",
   "metadata": {},
   "source": "# RL Training with slime on AMD GPUs\n\nModern large language models don't stop improving after pretraining. To become useful, aligned, and robust in real-world tasks, they must learn from feedback — and that's where Reinforcement Learning (RL) comes in.\n\nIn this tutorial, we walk through a real, production-style RL training pipeline for the [Qwen3-4B](https://huggingface.co/Qwen/Qwen3-4B) large language model, running entirely on AMD GPUs with ROCm. The workflow is powered by **slime**, an SGLang-native post-training framework built specifically for **RL scaling at LLM scale**.\n\n---\n\n## Why slime?\n\nTraining LLMs with RL is challenging for two key reasons:\n\n1. **Rollout generation is expensive**  \n   You need a fast inference engine to sample large volumes of model responses.\n\n2. **Policy optimization is heavy**  \n   You need a highly optimized training stack that scales across GPUs.\n\n**[slime](https://github.com/THUDM/slime)** addresses both challenges by cleanly separating — and efficiently connecting — these two worlds:\n\n- **SGLang** handles high-throughput rollout generation\n- **Megatron-LM** handles distributed policy training\n\nTogether, they form a scalable, modular RL system that works naturally with modern LLM workloads.\n\n---\n\n## What is GRPO, and why use it?\n\nThis tutorial uses **GRPO (Group Relative Policy Optimization)** — a modern RL algorithm designed for scalable LLM training.\n\nTraditional RL methods often rely on a **single reference baseline** or a **critic model**, which can be:\n- Expensive to train\n- Hard to stabilize\n- Sensitive to reward noise\n\n**GRPO takes a different approach.**\n\nInstead of evaluating a response in isolation, GRPO:\n- Samples **multiple responses** for the same prompt\n- Groups them together\n- Computes **relative advantages** *within the group*\n\n### Why this matters for LLMs\n\nGRPO offers several practical advantages:\n- **No separate value network required**\n- **More stable training signals**\n- **Better scaling behavior for large batch rollouts**\n- **Naturally fits server-based rollout generation (SGLang)**\n\nThis makes GRPO especially well-suited for:\n- Instruction tuning\n- Reasoning improvement\n- Preference-based optimization\n- Large-scale RL on multi-GPU systems\n\n---\n\n## What you'll learn in this notebook\n\nBy the end of this tutorial, you will be able to:\n\n- **Set up a ROCm-enabled Docker environment** for slime on AMD GPUs\n- **Configure GRPO** for Qwen3-4B, including rollout and reward settings\n- **Run an end-to-end RL training loop**, combining:\n  - SGLang for generation\n  - Megatron-LM for optimization\n- **Understand the system-level design choices** behind scalable LLM RL training\n\nWhether you're experimenting with post-training research or building production-grade RL pipelines, this notebook is designed to give you both **working code** and **clear mental models**.\n\nLet's get started."
  },
  {
   "cell_type": "markdown",
   "id": "53d94b31-35f8-4c8c-af0a-8a10aa5b4c62",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial was developed and tested using the following setup. \n",
    "\n",
    "### Operating system\n",
    "\n",
    "* **Ubuntu 22.04**: Ensure your system is running Ubuntu 22.04.\n",
    "\n",
    "### Hardware\n",
    "\n",
    "* **AMD Instinct™ GPUs**: This tutorial was tested on a full node of AMD Instinct MI300X GPUs (eight MI300X GPUs). Ensure you are using AMD Instinct GPUs or compatible hardware with ROCm support and that your system meets the [official requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n",
    "\n",
    "### Software\n",
    "\n",
    "* **ROCm 7.0.0**: Install and verify ROCm by following the [ROCm install guide](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html). After installation, confirm your setup using:\n",
    "\n",
    "    ``` bash\n",
    "    amd-smi\n",
    "    ```\n",
    "\n",
    "    This command lists your AMD GPUs with relevant details.\n",
    "    \n",
    "    **Note**: For ROCm 6.4 and earlier, use the `rocm-smi` command instead.\n",
    "\n",
    "* **Docker**: Ensure Docker is installed and configured correctly. Follow the Docker installation guide for your operating system.\n",
    "\n",
    "   **Note**: Ensure the Docker permissions are correctly configured. To configure permissions to allow non-root access, run the following commands:\n",
    "\n",
    "   ``` bash\n",
    "   sudo usermod -aG docker $USER\n",
    "   newgrp docker\n",
    "   ```\n",
    "\n",
    "   Verify Docker is working correctly:\n",
    "\n",
    "   ``` bash\n",
    "   docker run hello-world\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c403c03-5913-4841-8558-707a0fc4166c",
   "metadata": {},
   "source": [
    "## System validation\n",
    "\n",
    "Before running AI workloads, it's important to ensure that your AMD hardware is configured correctly and performing optimally.\n",
    "\n",
    "Generally, application performance can benefit from disabling NUMA (Non-Uniform Memory Access) auto-balancing. However, this setting might be detrimental to performance with certain types of workloads.\n",
    "\n",
    "Run this command to verify the current NUMA settings:\n",
    "\n",
    "``` bash\n",
    "cat /proc/sys/kernel/numa_balancing\n",
    "``` \n",
    "\n",
    "An output of `0` indicates NUMA auto-balancing is disabled. If there is no output or the output is `1`, run the following command to disable NUMA auto-balancing.\n",
    "\n",
    "``` bash\n",
    "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'\n",
    "``` \n",
    "\n",
    "For more information, see [Disable NUMA auto-balancing](https://instinct.docs.amd.com/projects/amdgpu-docs/en/latest/system-optimization/mi300x.html#disable-numa-auto-balancing)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4926a00e-7805-4de6-bb72-43db16ac09a2",
   "metadata": {},
   "source": "## Set up the environment\n\nFollow these steps to prepare the training environment.\n\n### 1. Pull the Docker image\n\nEnsure your system meets the [System Requirements](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html).\n\nPull the Docker image required for this tutorial:\n\n``` bash\ndocker pull rlsys/slime:latest\n```\n\n### 2. Launch the Docker container\n\nLaunch the Docker container and map the necessary directories. \n\n``` bash\ndocker run -it \\\n  --device /dev/dri \\\n  --device /dev/kfd \\\n  -p 8265:8265 \\\n  --group-add video \\\n  --network host --ipc host \\\n  --cap-add SYS_PTRACE \\\n  --security-opt seccomp=unconfined \\\n  --privileged \\\n  -v $HOME/.ssh:/root/.ssh \\\n  -v $HOME:$HOME \\\n  -w /workspace/notebooks \\\n  --shm-size 128G \\\n  --name slime \\\n  --ulimit memlock=-1 \\\n  --ulimit stack=67108864 \\\n  rlsys/slime:latest \\\n  /bin/bash\n```\n\n**Note**: If you need to return to the `slime` container after exiting it, use these commands:\n\n``` bash\ndocker start slime\ndocker exec -it slime bash\n```\n\n**Note**: Ensure the notebook file is either copied to `/workspace` directory or uploaded into the Jupyter Notebook environment after it starts. Save the token or URL provided in the terminal output to access the notebook from your web browser. You can download this notebook from the [AI Developer Hub GitHub repository](https://github.com/ROCm/gpuaidev).\n\n\n### 3. Install and launch Jupyter\n\nInside the Docker container, install Jupyter using the following command:\n\n``` bash\npip install jupyter\n```\n\nStart the Jupyter server:\n\n``` bash\njupyter-lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n```\n\n**Note**: Ensure port `8888` is not already in use on your system before running the above command. If it is, you can specify a different port by replacing `--port=8888` with another port number, for example, `--port=8890`.\n\n### 4. Install the required libraries\nBefore we can start RL training, we need to install **slime** — the core framework that connects\n**SGLang-based rollout generation** with **Megatron-LM-based policy optimization**.\n\nSince slime is under active development, different commits may introduce behavior changes\nthat affect rollout semantics, reward computation, or training stability.  \nTo ensure this tutorial is **reproducible and stable**, we pin the installation to a known\nworking commit.\n\nRun the following commands inside the ROCm-enabled Docker container:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32cec49-b324-4a49-9912-d4a0d2c318e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/THUDM/slime.git\n",
    "%cd slime\n",
    "# Note --You can run the latest upstream version. If you want a stable version, please check out the following commit ID\n",
    "!git checkout 0934a0e\n",
    "# Install the package\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ae6a9d-8f7e-4ec7-af40-0d2c2da24011",
   "metadata": {},
   "source": [
    "Before moving on, let’s confirm that slime is correctly installed and visible to Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0cc42-11c5-4bae-9d22-3c4157b4dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the installation and version of the required libraries\n",
    "!pip list | grep slime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e527965-371d-4590-a506-5efbd2f0ef68",
   "metadata": {},
   "source": [
    "If slime appears in the output, your environment is ready for the next step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39c50fca-5747-42de-827f-94b0b3f4cdae",
   "metadata": {},
   "source": [
    "## Run GRPO Training\n",
    "\n",
    "This section walks through the end-to-end process of setting up and running\n",
    "**Group Relative Policy Optimization (GRPO)** training for Qwen3-4B.\n",
    "\n",
    "At a high level, GRPO training requires:\n",
    "- A **base pretrained model** (the policy we want to improve)\n",
    "- A **training dataset** used to generate rollouts and compute rewards\n",
    "- A **held-out evaluation dataset** to track generalization during training\n",
    "\n",
    "### 1. Download model and datasets\n",
    "\n",
    "We first download the base **Qwen3-4B** model, which serves as the initial policy\n",
    "for RL fine-tuning.\n",
    "\n",
    "For training, we use **`dapo-math-17k`**, a dataset designed to evaluate\n",
    "step-by-step mathematical reasoning — a setting where relative comparisons\n",
    "between multiple model outputs are especially effective.\n",
    "\n",
    "For evaluation, we use **`aime-2024`**, which provides a clean benchmark to\n",
    "monitor reasoning performance without leaking training data.\n",
    "\n",
    "Run the following commands to download all required artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e61b00-379c-417c-8077-f0d7e0c8e384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": "!hf download Qwen/Qwen3-4B --local-dir /root/Qwen3-4B"
  },
  {
   "cell_type": "markdown",
   "id": "q53gktnw3i",
   "source": "Download the base Qwen3-4B model checkpoint from Hugging Face:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f155c-de2a-43e9-b646-6b8d941308a3",
   "metadata": {},
   "outputs": [],
   "source": "!hf download --repo-type dataset zhuzilin/dapo-math-17k \\\n  --local-dir /root/dapo-math-17k"
  },
  {
   "cell_type": "markdown",
   "id": "i5yknn2bnwn",
   "source": "Download the training dataset (dapo-math-17k for mathematical reasoning tasks):",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d82abe2-2df1-4e31-b489-eb04f2c1e35a",
   "metadata": {},
   "outputs": [],
   "source": "!hf download --repo-type dataset zhuzilin/aime-2024 \\\n  --local-dir /root/aime-2024"
  },
  {
   "cell_type": "markdown",
   "id": "z4ciprv0kw",
   "source": "Download the evaluation dataset (AIME 2024 for benchmarking):",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "32df6044-4395-4f07-bec0-e6c17a2a4f35",
   "metadata": {},
   "source": [
    "### 2. Convert Checkpoint Format\n",
    "\n",
    "Before we can start GRPO training, we need to convert the pretrained **Hugging Face checkpoint** into the **Megatron-Core distributed format**.\n",
    "\n",
    "This conversion is required because **slime uses Megatron-LM for training**, which expects model weights to be laid out according to the target **parallelization strategy** (e.g., tensor parallelism and pipeline parallelism). Hugging Face checkpoints, by contrast, store weights in a framework-agnostic, single-process format.\n",
    "\n",
    "This is a **one-time preprocessing step** for a given model and parallel configuration. You do **not** need to repeat it for every training run, as long\n",
    "as the parallelism settings remain unchanged.\n",
    "\n",
    "Run the following commands to perform the conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232258b0-3158-4de0-858c-0a28cda308ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Navigate to the slime repository\n",
    "cd /workspace/notebooks/slime\n",
    "\n",
    "# Load model configuration arguments\n",
    "source scripts/models/qwen3-4B.sh\n",
    "\n",
    "# Locate megatron-core installation path\n",
    "MEGATRON_LM_PATH=$(pip list | grep megatron-core | awk '{print $NF}')\n",
    "\n",
    "# Run conversion tool\n",
    "PYTHONPATH=${MEGATRON_LM_PATH} python tools/convert_hf_to_torch_dist.py \\\n",
    "    ${MODEL_ARGS[@]} \\\n",
    "    --no-gradient-accumulation-fusion \\\n",
    "    --hf-checkpoint /root/Qwen3-4B \\\n",
    "    --save /root/Qwen3-4B_torch_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd465d75-b001-4ce0-95dd-c7a7fb1abb70",
   "metadata": {},
   "source": [
    "### 3. Launch GRPO Training\n",
    "\n",
    "This step prepares and launches the **GRPO training runtime**.\n",
    "\n",
    "Because slime training scripts are designed to run in standalone environments, they may include safeguards (such as `pkill -9 python`) that are unsafe inside a **Jupyter notebook**. Additionally, certain **offloading behaviors** can cause instability on AMD GPUs in interactive environments.\n",
    "\n",
    "To ensure a stable Jupyter-based workflow, this cell performs two actions:\n",
    "\n",
    "1. **Prevents the training script from terminating the Jupyter kernel**\n",
    "2. **Injects required `--no-offload` flags** for both training and rollout\n",
    "3. **Reduces the rollout count** to make early performance improvements easier to observe. (You can adjust --num-rollout based on your dataset size and training goals. A larger --num-rollout results in more rollouts per iteration, effectively increasing the training epoch and improving convergence at the cost of longer runtime.)\n",
    "\n",
    "Patch the training script (one-time setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5dca76-bf85-4a7a-ba98-8bdb90d272a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Navigate to the slime repository\n",
    "cd /workspace/notebooks/slime\n",
    "\n",
    "SCRIPT=scripts/run-qwen3-4B-amd.sh\n",
    "\n",
    "echo \"Patching $SCRIPT ...\"\n",
    "\n",
    "# 1. Comment out `pkill -9 python` only if it is not already commented\n",
    "if grep -qE '^[[:space:]]*pkill -9 python' \"$SCRIPT\"; then\n",
    "  echo \" - Commenting out pkill -9 python\"\n",
    "  sed -i 's/^[[:space:]]*pkill -9 python/# pkill -9 python/' \"$SCRIPT\"\n",
    "else\n",
    "  echo \" - pkill already commented or not present\"\n",
    "fi\n",
    "\n",
    "# 2. Inject no-offload flags only if they are not already present\n",
    "if ! grep -q -- '--no-offload-train' \"$SCRIPT\"; then\n",
    "  echo \" - Injecting --no-offload flags after --colocate\"\n",
    "  sed -i '/--colocate/a \\   --no-offload-train \\\\\\n   --no-offload-rollout \\\\' \"$SCRIPT\"\n",
    "else\n",
    "  echo \" - no-offload flags already present\"\n",
    "fi\n",
    "\n",
    "sed -i 's/--num-rollout[[:space:]]\\+[0-9]\\+/--num-rollout 200/' \"$SCRIPT\"\n",
    "\n",
    "echo \"Patch completed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124ad0cb-06b1-450a-bb37-7217cae401c0",
   "metadata": {},
   "source": [
    "Once the script is patched, start the GRPO training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79431aa-9878-4e67-8128-4921eb4ad1c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Navigate to the slime repository\n",
    "cd /workspace/notebooks/slime\n",
    "\n",
    "# Launch the training script with environment variables set\n",
    "SLIME_DIR=/root \\\n",
    "MODEL_DIR=/root \\\n",
    "DATA_DIR=/root \\\n",
    "bash scripts/run-qwen3-4B-amd.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae7e642-c08c-4d6c-9697-05bbde39077f",
   "metadata": {},
   "source": "### Understanding the training script\n\nThe `run-qwen3-4B-amd.sh` script contains all configuration for GRPO training. It's organized into several parameter groups that control different aspects of the training pipeline.\n\nBelow is a breakdown of the key components. Each section corresponds to a specific aspect of the training workflow."
  },
  {
   "cell_type": "markdown",
   "id": "xsdt4unv8w",
   "source": "#### Model configuration\n\n```bash\nSCRIPT_DIR=\"$(cd -- \"$(dirname -- \"${BASH_SOURCE[0]}\")\" &>/dev/null && pwd)\"\nsource \"${SCRIPT_DIR}/models/qwen3-4B.sh\"\n```\n\nThis loads model architecture settings from `scripts/models/qwen3-4B.sh`. These are Megatron-LM parameters that define the model structure.\n\n**⚠️ Important:** Ensure settings like `--rotary-base` match your target model. Different models may use different rotary base values. Override these after sourcing the config:\n\n```bash\nsource \"${SCRIPT_DIR}/models/qwen3-4B.sh\"\nMODEL_ARGS+=( --rotary-base 10000 )\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "fzqgvy6xunb",
   "source": "#### Checkpoint configuration\n\n```yaml\nCKPT_ARGS=(\n   # HF checkpoint required by SGLang; also used for tokenizer\n   --hf-checkpoint ${MODEL_DIR}/Qwen3-4B\n   # Reference model checkpoint\n   --ref-load ${MODEL_DIR}/Qwen3-4B_torch_dist\n   # Actor model load directory; if empty, loads from ref_load\n   --load ${MODEL_DIR}/Qwen3-4B_slime/\n   --save ${MODEL_DIR}/Qwen3-4B_slime/\n   --save-interval 20\n)\n```\n\nControls where models are loaded from and saved to during training.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "p01twbzctke",
   "source": "#### Rollout configuration\n\n```yaml\nROLLOUT_ARGS=(\n   # Dataset configuration\n   --prompt-data ${DATA_DIR}/dapo-math-17k/dapo-math-17k.jsonl\n   --input-key prompt\n   --label-key label\n   --apply-chat-template\n   --rollout-shuffle\n\n   # Reward model\n   --rm-type deepscaler\n   \n   # Rollout parameters\n   --num-rollout 200\n   --rollout-batch-size 32\n   --n-samples-per-prompt 8\n   --rollout-max-response-len 8192\n   --rollout-temperature 0.8\n   \n   # Training batch configuration\n   --global-batch-size 256\n   --balance-data\n)\n```\n\n**Key parameters:**\n- `--num-rollout`: Total number of rollouts for training\n- `--n-samples-per-prompt`: Responses sampled per prompt (used for group-relative advantages in GRPO)\n- `--rm-type`: Reward model type (slime supports multiple types and custom models via `--custom-rm-path`)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "09qcq6ceozsf",
   "source": "#### Evaluation configuration\n\nEvaluation inherits rollout settings but allows overriding specific parameters:\n\n```yaml\nEVAL_ARGS=(\n   --eval-interval 20\n   --eval-prompt-data aime ${DATA_DIR}/aime-2024/aime-2024.jsonl\n   --n-samples-per-eval-prompt 16\n   --eval-max-response-len 16384\n   --eval-top-p 0.7\n)\n```\n\n#### Performance and parallelism\n\n```yaml\nPERF_ARGS=(\n   --tensor-model-parallel-size 2\n   --sequence-parallel\n   --pipeline-model-parallel-size 1\n   --context-parallel-size 1\n   \n   --recompute-granularity full\n   --recompute-method uniform\n   --recompute-num-layers 1\n\n   --use-dynamic-batch-size\n   --max-tokens-per-gpu 9216\n)\n```\n\n**Key optimizations:**\n- `--use-dynamic-batch-size`: Packs samples of varying lengths into micro-batches up to token limit\n- `--max-tokens-per-gpu`: Hard limit of tokens per GPU\n\n**Note:** slime guarantees strict per-token loss calculation even with dynamic packing.\n\n#### GRPO algorithm parameters\n\n```yaml\nGRPO_ARGS=(\n   --advantage-estimator grpo\n   --use-kl-loss\n   --kl-loss-coef 0.00\n   --kl-loss-type low_var_kl\n   --entropy-coef 0.00\n   --eps-clip 0.2\n   --eps-clip-high 0.28\n)\n```\n\n#### Optimizer configuration\n\n```yaml\nOPTIMIZER_ARGS=(\n   --optimizer adam\n   --lr 1e-6\n   --lr-decay-style constant\n   --weight-decay 0.1\n   --adam-beta1 0.9\n   --adam-beta2 0.98\n)\n```\n\n#### SGLang configuration\n\n```yaml\nSGLANG_ARGS=(\n   --rollout-num-gpus-per-engine 2  # SGLang tensor parallelism\n   --sglang-mem-fraction-static 0.7\n)\n```\n\nArguments prefixed with `--sglang-` are forwarded directly to the SGLang engine.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "fef27556-2e80-408f-8ade-a725f3f3602e",
   "metadata": {},
   "source": [
    "### 4. Convert from Megatron Format to Huggingface Face Format to do inference after training\n",
    "\n",
    "After RL training with slime, the model checkpoints are saved in Megatron-LM distributed format, which is not directly usable for standard inference frameworks. To run inference with Hugging Face Transformers or SGLang, you need to convert these checkpoints back to Hugging Face (HF) format.\n",
    "\n",
    "convert the trained Megatron checkpoint (from a specific training iteration) back into Hugging Face format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf407e-ae5b-4e61-b27f-c7ad24b69e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Navigate to the slime repository\n",
    "cd /workspace/notebooks/slime\n",
    "\n",
    "# Load model configuration arguments\n",
    "source scripts/models/qwen3-4B.sh\n",
    "\n",
    "# Locate megatron-core installation path\n",
    "MEGATRON_LM_PATH=$(pip list | grep megatron-core | awk '{print $NF}')\n",
    "\n",
    "PYTHONPATH=${MEGATRON_LM_PATH} python tools/convert_hf_to_torch_dist.py \\\n",
    "    ${MODEL_ARGS[@]} \\\n",
    "    --no-gradient-accumulation-fusion \\\n",
    "    --hf-checkpoint /root/Qwen3-4B \\\n",
    "    --save /root/Qwen3-4B_torch_dist\n",
    "\n",
    "PYTHONPATH=${MEGATRON_LM_PATH} python tools/convert_torch_dist_to_hf.py \\\n",
    "  --input-dir /root/Qwen3-4B_slime/iter_0000199 \\\n",
    "  --output-dir /root/Qwen3-4B_slime_hf-iter_0000199 \\\n",
    "  --origin-hf-dir /root/Qwen3-4B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e35aab-8096-46a1-b132-daf8dc0ce1f7",
   "metadata": {},
   "source": [
    "After conversion, /root/Qwen3-4B_slime_hf_iter_0000199 is a standard Hugging Face model directory, ready for [transformers inference](https://huggingface.co/Qwen/Qwen3-4B), [SGLang serving](https://docs.sglang.io/basic_usage/send_request.html), and further evaluation or fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qo2enb70zp9",
   "source": "## Summary\n\nCongratulations! Through this GRPO training tutorial with slime, you learned how to train large language models using reinforcement learning on AMD GPUs.\n\n**Key takeaways:**\n\n- **Environment setup**: ROCm-enabled Docker containers with slime provide a complete RL training environment\n- **Checkpoint management**: Converting between Hugging Face and Megatron formats enables seamless integration across frameworks\n- **GRPO training**: Group-relative advantages provide stable RL training without requiring a separate value network\n- **Scalable architecture**: SGLang for rollout generation and Megatron-LM for policy optimization work together efficiently\n\n## Next steps\n\n1. **Experiment with different datasets**: Apply GRPO to other reasoning or instruction-following datasets\n2. **Tune hyperparameters**: Adjust learning rate, KL coefficients, or sampling strategies for your specific use case\n3. **Scale to larger models**: Use the same workflow with larger Qwen models or other LLM architectures\n4. **Evaluate trained models**: Test your fine-tuned models on downstream tasks to measure improvement\n\n## Additional resources\n\n- [slime GitHub Repository](https://github.com/THUDM/slime)\n- [SGLang Documentation](https://docs.sglang.io/)\n- [Megatron-LM Documentation](https://github.com/NVIDIA/Megatron-LM)\n- [ROCm Documentation](https://rocm.docs.amd.com/en/latest/index.html)\n- [AMD AI Developer Hub](https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e10a561-719c-40d0-a3f0-6aaab52909e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}