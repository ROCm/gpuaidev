{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "654be0fb-0d33-4b9f-a55c-95e5753d52b6",
   "metadata": {},
   "source": [
    "# Fine-tune Llama 3.2 11B Vision Instruct with HuggingFace + PyTorch on AMD MI300X/MI325X\n",
    "\n",
    "This tutorial demonstrates how to fine-tune **Llama 3.2 11B Vision Instruct** â€” a multimodal vision-language model â€” on the ChartQA dataset using **only HuggingFace Transformers, PEFT, and TRL**.\n",
    "\n",
    "Unlike the text-only Unsloth version, this notebook trains with **actual chart images** from ChartQA, allowing the model to learn visual chart understanding end-to-end.\n",
    "\n",
    "**Key Features:**\n",
    "- Llama 3.2 11B Vision: true multimodal model with vision encoder + cross-attention\n",
    "- Trains on real chart images (not just text descriptions)\n",
    "- LoRA fine-tuning targeting language model + cross-attention layers\n",
    "- ChartQA dataset with synthetic chain-of-thought reasoning\n",
    "- Optimized for AMD ROCm (MI300X)\n",
    "\n",
    "**References:**\n",
    "- [Llama 3.2 Vision Model Card](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct)\n",
    "- [TRL Multimodal SFT Guide](https://huggingface.co/docs/trl/main/en/training_vlm_sft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4371974-d131-4b8b-86cc-d56194d1e8e6",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Hardware\n",
    "- **AMD Instinct MI300X/MI325X**: This tutorial was designed for AMD Instinctâ„¢ MI300X GPUs with ROCm support.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6bc8a-52e1-4549-83be-d0b85ff7521a",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup](#environment-setup)\n",
    "2. [Data Preparation](#data-preparation)\n",
    "3. [Model Loading](#model-loading)\n",
    "4. [LoRA Configuration](#lora-config)\n",
    "5. [Training](#training)\n",
    "6. [Saving the Model](#saving-model)\n",
    "7. [Load the Adapters](#lora-inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62bb27-dcba-42de-85e9-e9a353a7c20d",
   "metadata": {},
   "source": [
    "## 1. Environment Setup <a id=\"environment-setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b4fe87-cc7a-4081-9fbd-5d917e00e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import types\n",
    "import torch\n",
    "\n",
    "if \"torchvision._meta_registrations\" not in sys.modules:\n",
    "    sys.modules[\"torchvision._meta_registrations\"] = types.ModuleType(\n",
    "        \"torchvision._meta_registrations\"\n",
    "    )\n",
    "    print(\"Applied torchvision _meta_registrations workaround for ROCm\")\n",
    "\n",
    "# Verify GPU setup\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA/ROCm available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9637f36d-8571-4729-83f4-c56d6ab4ae19",
   "metadata": {},
   "source": [
    "## 2. Data Preparation <a id=\"data-preparation\"></a>\n",
    "\n",
    "We use the ChartQA dataset with synthetic chain-of-thought reasoning. Since Llama 3.2 Vision is a true multimodal model, we include the **actual chart images** in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1543753b-3a54-4aa9-954f-5861b7ad5ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_hf import create_chart_qa_with_reasoning_dataset\n",
    "\n",
    "create_chart_qa_with_reasoning_dataset(\n",
    "    \"reasoning.parquet\",\n",
    "    \"cot_chartqa\",\n",
    "    override=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f26bf-25be-41df-b806-8c974bad30f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Load original ChartQA (has images!)\n",
    "original_chartqa = load_dataset(\"HuggingFaceM4/ChartQA\")\n",
    "\n",
    "# Load our CoT dataset\n",
    "reasoning_dataset = load_dataset(\"cot_chartqa\")\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"   Training samples: {len(reasoning_dataset['train'])}\")\n",
    "print(f\"   Original ChartQA columns: {original_chartqa['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ceb090-be60-4b5e-810c-72a0ff39e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs reasoning versions\n",
    "sample_idx = 27901\n",
    "\n",
    "print(f\"Sample Comparison (Index {sample_idx}):\")\n",
    "print(f\"\\nQuery: {original_chartqa['train'][sample_idx]['query']}\")\n",
    "print(f\"\\nOriginal Answer: {original_chartqa['train'][sample_idx]['label']}\")\n",
    "print(f\"\\nReasoning Version:\\n{reasoning_dataset['train'][sample_idx]['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967fbf5d-9c6c-4f49-8b9e-c4184c864033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Display the sample chart image\n",
    "sample = original_chartqa['train'][sample_idx]\n",
    "if 'image' in sample:\n",
    "    print(\"Sample Chart:\")\n",
    "    display(sample['image'])\n",
    "    sample['image'].save('example_chart.png')\n",
    "    print(\"Saved as example_chart.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4010519-5a31-4b11-ba78-f33f60bd72db",
   "metadata": {},
   "source": [
    "### Format Dataset for Vision-Language Training\n",
    "\n",
    "For Llama 3.2 Vision, we format data as **multimodal conversations** with `{\"type\": \"image\"}` and `{\"type\": \"text\"}` content items. The actual PIL images are included alongside the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe870c98-20e7-4d37-93c2-9525aa9eaf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from utils_hf import format_chartqa_for_vision_training\n",
    "\n",
    "MAX_SAMPLES = 1000  # Adjust based on your needs\n",
    "\n",
    "train_data = format_chartqa_for_vision_training(\n",
    "    original_chartqa[\"train\"],\n",
    "    reasoning_dataset[\"train\"],\n",
    "    max_samples=MAX_SAMPLES,\n",
    ")\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "\n",
    "print(f\"\\nDataset columns: {train_dataset.column_names}\")\n",
    "print(f\"Total training samples: {len(train_dataset)}\")\n",
    "\n",
    "# Preview a sample\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample messages structure:\")\n",
    "for msg in sample[\"messages\"]:\n",
    "    print(f\"  {msg['role']}: {[c['type'] for c in msg['content']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb3e8b1-aec1-4306-9f86-4ed08695e783",
   "metadata": {},
   "source": [
    "Note: We formatted only 1000 examples from the whole data as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fed85-245f-46c6-bb2d-473d779f3097",
   "metadata": {},
   "source": [
    "## 3. Model Loading <a id=\"model-loading\"></a>\n",
    "\n",
    "Load Llama 3.2 11B Vision Instruct using `MllamaForConditionalGeneration` and `AutoProcessor`. The processor handles both text tokenization and image preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3548e19a-4bd6-414a-a823-3aedfeab37bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MllamaForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "print(\"This is a ~22GB model in bf16, please be patient...\")\n",
    "\n",
    "# Load processor (handles both text tokenization and image preprocessing)\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Ensure padding is on the right for training\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Ensure pad token is set\n",
    "if processor.tokenizer.pad_token is None:\n",
    "    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "# Load model\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"   Model type: {type(model).__name__}\")\n",
    "print(f\"   Dtype: {model.dtype}\")\n",
    "\n",
    "# Show model structure overview\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"   Total parameters: {total_params / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87debbf4-6cfe-415f-ae88-4c11de6f3d03",
   "metadata": {},
   "source": [
    "## 4. LoRA Configuration <a id=\"lora-config\"></a>\n",
    "\n",
    "Add LoRA adapters using PEFT. For the vision model, we target the **language model** layers including cross-attention (which bridges vision and language). The vision encoder is kept frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d226b4de-6000-4a17-aded-4d069f5557de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "\n",
    "# Configure LoRA for the vision-language model\n",
    "# We target the language model layers (including cross-attention to vision)\n",
    "# The vision encoder is kept frozen\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # LoRA rank\n",
    "    lora_alpha=16,           # Scaling factor\n",
    "    lora_dropout=0.05,       # Small dropout for regularization\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        # Language model self-attention\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        # Language model MLP\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "print(\"Adding LoRA adapters...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTrainable: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
    "print(f\"Total:     {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e91539-8078-426b-8482-bf74d786d34a",
   "metadata": {},
   "source": [
    "## 5. Training <a id=\"training\"></a>\n",
    "\n",
    "For multimodal training with TRL's SFTTrainer, we need a custom `collate_fn` that:\n",
    "1. Applies the chat template to format the conversation text\n",
    "2. Processes images through the vision processor\n",
    "3. Creates proper labels (masking padding and image tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ed241-5191-4e77-887d-30eb8da92742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def collate_fn(examples):\n",
    "    \"\"\"\n",
    "    Custom collator for multimodal vision-language training.\n",
    "    \n",
    "    For each example:\n",
    "    1. Apply the Llama 3.2 Vision chat template to the messages\n",
    "    2. Process images + text through the processor\n",
    "    3. Create labels with proper masking\n",
    "    \"\"\"\n",
    "    # Apply chat template to get formatted text\n",
    "    texts = []\n",
    "    for example in examples:\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        texts.append(text.strip())\n",
    "    \n",
    "    # Collect images - each example has a list of images\n",
    "    images = []\n",
    "    for example in examples:\n",
    "        example_images = example.get(\"images\", [])\n",
    "        if example_images:\n",
    "            # Ensure all images are RGB PIL Images\n",
    "            imgs = [img.convert(\"RGB\") if isinstance(img, Image.Image) else img \n",
    "                    for img in example_images]\n",
    "            images.append(imgs)\n",
    "        else:\n",
    "            images.append(None)\n",
    "    \n",
    "    # Process through the Mllama processor (tokenizes text + processes images)\n",
    "    # The processor expects images as a list of lists (one list per batch item)\n",
    "    batch = processor(\n",
    "        images=images,\n",
    "        text=texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "    )\n",
    "    \n",
    "    # Create labels: clone input_ids and mask tokens we don't want to compute loss on\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    # Mask padding tokens\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    # Mask the image token (<|image|>) so we don't compute loss on it\n",
    "    image_token = processor.tokenizer.convert_tokens_to_ids(\"<|image|>\")\n",
    "    if image_token is not None and image_token != processor.tokenizer.unk_token_id:\n",
    "        labels[labels == image_token] = -100\n",
    "    \n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "# Quick test with one sample\n",
    "print(\"Testing collate_fn with a single sample...\")\n",
    "test_batch = collate_fn([train_data[0]])\n",
    "print(f\"   input_ids shape: {test_batch['input_ids'].shape}\")\n",
    "print(f\"   labels shape: {test_batch['labels'].shape}\")\n",
    "if 'pixel_values' in test_batch:\n",
    "    print(f\"   pixel_values shape: {test_batch['pixel_values'].shape}\")\n",
    "print(f\"   Keys: {list(test_batch.keys())}\")\n",
    "print(\"collate_fn works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da463cb-8572-4d41-845a-c9451758f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "print(\"Setting up trainer...\")\n",
    "\n",
    "# Training configuration optimized for AMD MI300X/MI325X\n",
    "training_args = SFTConfig(\n",
    "    # Batch settings\n",
    "    # Vision models use more memory per sample than text-only models\n",
    "    per_device_train_batch_size=1,   # Smaller batch due to image memory\n",
    "    gradient_accumulation_steps=8,    # Effective batch = 1 * 8 = 8\n",
    "    \n",
    "    # Training duration\n",
    "    num_train_epochs=1,\n",
    "    max_steps=10,\n",
    "    \n",
    "    # Learning rate (lower than text-only since vision model is larger)\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # Precision\n",
    "    bf16=True,\n",
    "    tf32=False,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    \n",
    "    # Checkpointing\n",
    "    output_dir=\"./checkpoints/chartqa_llama_vision\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    dataloader_pin_memory=False,   \n",
    "    dataloader_num_workers=0,       \n",
    "    \n",
    "    # IMPORTANT for multimodal training:\n",
    "    remove_unused_columns=False,    # Keep image columns\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  # We handle preprocessing in collate_fn\n",
    "    \n",
    "    # Misc\n",
    "    seed=42,\n",
    "    report_to=\"none\",\n",
    "    # report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "# Create trainer with custom collate_fn for multimodal data\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,       # Custom collator handles images + text\n",
    "    processing_class=processor,      # Pass processor instead of tokenizer\n",
    ")\n",
    "\n",
    "print(\"Trainer configured!\")\n",
    "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Max seq length: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80230a06-30e0-4aae-aeca-aac9a6c32e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb52d8b-29cf-4df4-b226-909664c1c0e2",
   "metadata": {},
   "source": [
    "The above cell (max_steps=10) will show the tiny fine-tuning which will finish very soon.\n",
    "\n",
    "<img src=\"./assets/Screenshot 2026-02-12 020007.png\" alt=\"20step-11b\" width=\"700\" height=\"450\">\n",
    "\n",
    "Note that full fine-tuning (5 epochs) will take about 25 hours to finish. During this time and if the WandBLogger was enabled we can follow the progress online.\n",
    "\n",
    "<img src=\"./assets/Screenshot 2026-02-12 010520.png\" alt=\"epoch10-11b\" width=\"700\" height=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c367c0bd-ec84-4c5e-b2de-1b5b82563319",
   "metadata": {},
   "source": [
    "## 6. Saving the Model <a id=\"saving-model\"></a>\n",
    "\n",
    "Save the fine-tuned model. For vision models, we save:\n",
    "1. **LoRA adapters** (lightweight, recommended)\n",
    "2. **Merged model** (full weights, optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269f89f-1bca-4fba-962b-c488cf6592a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters only (lightweight, ~50-100MB)\n",
    "lora_path = \"./models/chartqa_llama_vision_lora\"\n",
    "print(f\"Saving LoRA adapters to {lora_path}...\")\n",
    "\n",
    "model.save_pretrained(lora_path)\n",
    "processor.save_pretrained(lora_path)\n",
    "\n",
    "print(f\"LoRA adapters saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc18f9e-99b3-4a05-9984-3044a6e6400e",
   "metadata": {},
   "source": [
    "## 7. Loading LoRA Adapters & Inference Comparison <a id=\"lora-inference\"></a>\n",
    "\n",
    "Now let's demonstrate the full workflow of **loading saved LoRA adapters** from disk, merging them with the base model, and comparing inference outputs against the unmodified base model.\n",
    "\n",
    "This is the key step for deployment: you train once, save the lightweight LoRA adapters (~50MB vs ~22GB full model), and later load + merge them for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8184e-9a21-4dd4-b189-3787b989b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, free memory from the training model if it's still loaded\n",
    "import gc\n",
    "\n",
    "try:\n",
    "    del model, trainer\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"GPU memory freed. Available: {torch.cuda.mem_get_info()[0] / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682c6314-d2bb-49d4-9715-e123ede77b48",
   "metadata": {},
   "source": [
    "### 7.1 Load the Base Model (No Fine-Tuning)\n",
    "\n",
    "We load the original Llama 3.2 11B Vision Instruct model **without** any LoRA adapters to establish a baseline for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf907b16-4e60-4d59-89d3-5527a23681bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BASE model (no LoRA) for comparison\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "base_model_name = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "print(f\"Loading BASE model: {base_model_name} ...\")\n",
    "base_model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "base_model.eval()\n",
    "\n",
    "base_processor = AutoProcessor.from_pretrained(\n",
    "    base_model_name,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Base model loaded successfully!\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in base_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9163f1a2-657d-44e9-9945-704facb4ec77",
   "metadata": {},
   "source": [
    "### 7.2 Run Base Model Inference\n",
    "\n",
    "Let's run the base model on a set of chart questions to see how it performs **before** fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dec124-eaa4-4680-950b-c1f5f4fe0d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def run_inference(model, processor, question, image, max_new_tokens=512):\n",
    "    \"\"\"Run inference on a single question + image pair.\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": f\"Look at this chart and answer the following question.\\n\\nQuestion: {question}\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    input_text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = processor(\n",
    "        images=[image],\n",
    "        text=input_text,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    \n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    \n",
    "    response = processor.decode(outputs[0][input_len:], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "# Prepare test samples from the ChartQA dataset\n",
    "original_chartqa = load_dataset(\"HuggingFaceM4/ChartQA\")\n",
    "\n",
    "test_indices = [0, 100, 500, 27901]\n",
    "test_samples = []\n",
    "\n",
    "for idx in test_indices:\n",
    "    sample = original_chartqa[\"train\"][idx]\n",
    "    if sample.get(\"image\") is not None:\n",
    "        test_samples.append({\n",
    "            \"idx\": idx,\n",
    "            \"question\": sample[\"query\"],\n",
    "            \"ground_truth\": sample[\"label\"],\n",
    "            \"image\": sample[\"image\"].convert(\"RGB\"),\n",
    "        })\n",
    "\n",
    "print(f\"Prepared {len(test_samples)} test samples\")\n",
    "for s in test_samples:\n",
    "    print(f\"  Sample #{s['idx']}: {s['question'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21bf8b4-36fd-45e3-987a-ad30a498b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference with the BASE model\n",
    "print(\"Running BASE model inference...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "base_results = []\n",
    "for s in test_samples:\n",
    "    answer = run_inference(base_model, base_processor, s[\"question\"], s[\"image\"])\n",
    "    base_results.append(answer)\n",
    "    print(f\"\\nSample #{s['idx']}\")\n",
    "    print(f\"  Q: {s['question']}\")\n",
    "    print(f\"  Ground Truth: {s['ground_truth']}\")\n",
    "    print(f\"  Base Model:   {answer}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Base model inference complete ({len(base_results)} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51e566f-ff09-4504-9a87-3d98e4f76068",
   "metadata": {},
   "source": [
    "### 7.3 Load LoRA Adapters & Merge with Base Model\n",
    "\n",
    "Now we load the saved LoRA adapters from disk and merge them into the base model. This is the standard deployment workflow:\n",
    "\n",
    "1. **Load base model** (same as before)\n",
    "2. **Load LoRA adapters** from the saved checkpoint\n",
    "3. **Merge** adapters into the base model weights\n",
    "4. **Run inference** with the enhanced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e3b84-54c7-4d96-a4e4-35456aa09d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, os\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "HF_REPO_ID = \"viani-sus/llama-3.2-11b-vision-chartqa-lora\"\n",
    "BASE_MODEL_NAME = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# 1. Apply the LoRA adapters from the Hub on top of the base model\n",
    "print(f\"Applying LoRA adapters from Hub: {HF_REPO_ID} ...\")\n",
    "finetuned_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    HF_REPO_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "finetuned_model.eval()\n",
    "\n",
    "# 2. Load the processor\n",
    "finetuned_processor = AutoProcessor.from_pretrained(HF_REPO_ID, trust_remote_code=True)\n",
    "\n",
    "# Show adapter info\n",
    "trainable = sum(p.numel() for p in finetuned_model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in finetuned_model.parameters())\n",
    "print(f\"Fine-tuned model loaded!\")\n",
    "\n",
    "# Show the adapter file sizes from the cached download\n",
    "lora_path = snapshot_download(HF_REPO_ID, local_files_only=True)\n",
    "adapter_files = [f for f in os.listdir(lora_path) if f.endswith(('.safetensors', '.bin', '.json'))]\n",
    "total_size = sum(os.path.getsize(os.path.join(lora_path, f)) for f in adapter_files)\n",
    "print(f\"Adapter files: {len(adapter_files)} files, {total_size / 1e6:.1f} MB total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097479af-f6ce-4ead-b289-f6e1ae8c8d7a",
   "metadata": {},
   "source": [
    "### 7.4 Run Fine-Tuned Model Inference\n",
    "\n",
    "Now let's run the same questions through the fine-tuned model and compare the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b76ccae-904d-40a5-b699-fd8440604467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference with the FINE-TUNED model (LoRA merged)\n",
    "print(\"Running FINE-TUNED model inference...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "finetuned_results = []       # Full CoT responses\n",
    "finetuned_extracted = []     # Extracted final answers\n",
    "\n",
    "for s in test_samples:\n",
    "    full_response = run_inference(finetuned_model, finetuned_processor, s[\"question\"], s[\"image\"])\n",
    "    finetuned_results.append(full_response)\n",
    "    \n",
    "    print(f\"\\nSample #{s['idx']}\")\n",
    "    print(f\"  Q: {s['question']}\")\n",
    "    print(f\"  Ground Truth:      {s['ground_truth']}\")\n",
    "    print(f\"  Full CoT Response: {full_response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Fine-tuned model inference complete ({len(finetuned_results)} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7178d28-a6b7-40e0-be70-b0f152aa48c8",
   "metadata": {},
   "source": [
    "### 7.5 Side-by-Side Comparison\n",
    "\n",
    "Let's visualize the results side by side to clearly see the improvement from fine-tuning. The fine-tuned model should provide **chain-of-thought reasoning** before giving the final answer, while the base model typically gives short, often incorrect answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6debedc5-4c8e-4097-b3c7-619680872e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison with images\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON: Base Model vs Fine-Tuned Model (LoRA)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, s in enumerate(test_samples):\n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(f\"SAMPLE #{s['idx']}\")\n",
    "    print(f\"{'â”€' * 80}\")\n",
    "    \n",
    "    # Display the chart image\n",
    "    display(s[\"image\"].resize((400, 300)))\n",
    "    \n",
    "    print(f\"\\nQuestion:     {s['question']}\")\n",
    "    print(f\"Ground Truth: {s['ground_truth']}\")\n",
    "    print(f\"\\n--- Base Model (before fine-tuning) ---\")\n",
    "    print(f\"{base_results[i]}\")\n",
    "    print(f\"\\n--- Fine-Tuned Model (with LoRA) ---\")\n",
    "    print(f\"Full CoT: {finetuned_results[i]}\")\n",
    "\n",
    "gt_answers = [s[\"ground_truth\"] for s in test_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa125dfd-2f1b-482f-966a-d27b22b21c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del finetuned_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Inference comparison complete!\")\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"  1. The base model gives generic/short answers to chart questions, even if correct, hard to extract\")\n",
    "print(\"  2. The fine-tuned model provides chain-of-thought reasoning before the answer\")\n",
    "print(\"  3. LoRA adapters are lightweight (~50MB) vs full model weights (~22GB)\")\n",
    "print(\"  4. Loading and merging LoRA adapters takes seconds, not minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e182e6-f3a4-4c98-ac89-470c0381e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ‰ Tutorial complete! Thank You!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
